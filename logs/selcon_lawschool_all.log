Hyperparms:	fraction:0.01	select every:35	num epochs:50	delta:0.01
[03/22 14:01:41] train_sl INFO: DotMap(setting='SL', dataset=DotMap(name='LawSchool_selcon', datadir='../data', feature='dss', type='pre-defined'), dataloader=DotMap(shuffle=True, batch_size=100, pin_memory=False), model=DotMap(architecture='RegressionNet', type='pre-defined', input_dim=10, numclasses=10), ckpt=DotMap(is_load=False, is_save=True, dir='results/', save_every=20), loss=DotMap(type='MeanSquaredLoss', use_sigmoid=False), optimizer=DotMap(type='adam', lr=0.01), scheduler=DotMap(type='StepLR', step_size=1, gamma=0.1), dss_args=DotMap(type='SELCON', fraction=0.01, select_every=35, kappa=0, delta=0.01, linear_layer=False, lam=1e-05, batch_sampler='sequential', selection_type='Supervised'), train_args=DotMap(num_epochs=50, device='cuda', print_every=50, results_dir='results/', print_args=['val_loss', 'tst_loss', 'trn_loss', 'time'], return_args=[]))
[03/22 14:01:46] train_sl INFO: SELCON: starting pre compute
[03/22 14:02:50] train_sl INFO: SELCON: Finishing F phi
[03/22 14:02:52] train_sl INFO: SELCON: Finishing element wise F
[03/22 14:02:52] train_sl INFO: Model checkpoint saved at epoch: 20
[03/22 14:02:52] train_sl INFO: Epoch: 36, SELCON dataloader subset selection finished, takes 0.0244. 
[03/22 14:02:52] train_sl INFO: Model checkpoint saved at epoch: 40
[03/22 14:02:53] train_sl INFO: Epoch: 50 , Validation Loss: 2.3835276782512667 , Test Loss: 2.418806087970734 , Training Loss: 2.1428809572870913 , Timing: 0.0034508705139160156
[03/22 14:02:53] train_sl INFO: SELCON Selection Run---------------------------------
[03/22 14:02:53] train_sl INFO: Final SubsetTrn: 2.796211
[03/22 14:02:53] train_sl INFO: Validation Loss: 2.38
[03/22 14:02:53] train_sl INFO: Test Data Loss: 2.418806
[03/22 14:02:53] train_sl INFO: ---------------------------------------------------------------------
[03/22 14:02:53] train_sl INFO: SELCON
[03/22 14:02:53] train_sl INFO: ---------------------------------------------------------------------
[03/22 14:02:53] train_sl INFO: [0.0040130615234375, 0.0034890174865722656, 0.005963325500488281, 0.0031626224517822266, 0.0032095909118652344, 0.0031845569610595703, 0.0032243728637695312, 0.003160715103149414, 0.0031540393829345703, 0.003214597702026367, 0.0031538009643554688, 0.003158092498779297, 0.0031867027282714844, 0.00318145751953125, 0.0032019615173339844, 0.0032029151916503906, 0.0032258033752441406, 0.0031633377075195312, 0.0031714439392089844, 0.0032024383544921875, 0.003269195556640625, 0.0034437179565429688, 0.003420114517211914, 0.003473520278930664, 0.0034132003784179688, 0.0034406185150146484, 0.0034110546112060547, 0.0033845901489257812, 0.003441333770751953, 0.003399372100830078, 0.0034987926483154297, 0.0036504268646240234, 0.003620147705078125, 0.003685474395751953, 0.0038428306579589844, 0.028144359588623047, 0.0034096240997314453, 0.0032470226287841797, 0.003409147262573242, 0.0034334659576416016, 0.0034210681915283203, 0.00337982177734375, 0.0034415721893310547, 0.003467559814453125, 0.003467559814453125, 0.0034215450286865234, 0.0034253597259521484, 0.0033464431762695312, 0.003454446792602539, 0.0034508705139160156]
[03/22 14:02:53] train_sl INFO: Total time taken by SELCON = 0.1959 
Hyperparms:	fraction:0.01	select every:35	num epochs:50	delta:0.04
[03/22 14:02:53] train_sl INFO: DotMap(setting='SL', dataset=DotMap(name='LawSchool_selcon', datadir='../data', feature='dss', type='pre-defined'), dataloader=DotMap(shuffle=True, batch_size=100, pin_memory=False), model=DotMap(architecture='RegressionNet', type='pre-defined', input_dim=10, numclasses=10), ckpt=DotMap(is_load=False, is_save=True, dir='results/', save_every=20), loss=DotMap(type='MeanSquaredLoss', use_sigmoid=False), optimizer=DotMap(type='adam', lr=0.01), scheduler=DotMap(type='StepLR', step_size=1, gamma=0.1), dss_args=DotMap(type='SELCON', fraction=0.01, select_every=35, kappa=0, delta=0.04, linear_layer=False, lam=1e-05, batch_sampler='sequential', selection_type='Supervised', collate_fn=None, model=RegressionNet(
  (linear): Linear(in_features=10, out_features=1, bias=True)
), lr=0.01, loss=MSELoss(), device='cuda', optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.01
    weight_decay: 0
), criterion=MSELoss(), num_classes=10, batch_size=100, num_epochs=50), train_args=DotMap(num_epochs=50, device='cuda', print_every=50, results_dir='results/', print_args=['val_loss', 'tst_loss', 'trn_loss', 'time'], return_args=[]), dss_arg=DotMap(batch_sampler=DotMap()), is_reg=DotMap())
[03/22 14:02:53] train_sl INFO: SELCON: starting pre compute
[03/22 14:02:53] train_sl INFO: SELCON: Finishing F phi
[03/22 14:02:55] train_sl INFO: SELCON: Finishing element wise F
[03/22 14:02:55] train_sl INFO: Model checkpoint saved at epoch: 20
[03/22 14:02:55] train_sl INFO: Epoch: 36, SELCON dataloader subset selection finished, takes 0.0242. 
[03/22 14:02:55] train_sl INFO: Model checkpoint saved at epoch: 40
[03/22 14:02:55] train_sl INFO: Epoch: 50 , Validation Loss: 1.0451133579015732 , Test Loss: 1.0439921349287034 , Training Loss: 0.866603690557755 , Timing: 0.003402233123779297
[03/22 14:02:55] train_sl INFO: SELCON Selection Run---------------------------------
[03/22 14:02:55] train_sl INFO: Final SubsetTrn: 0.865784
[03/22 14:02:55] train_sl INFO: Validation Loss: 1.05
[03/22 14:02:55] train_sl INFO: Test Data Loss: 1.043992
[03/22 14:02:55] train_sl INFO: ---------------------------------------------------------------------
[03/22 14:02:55] train_sl INFO: SELCON
[03/22 14:02:55] train_sl INFO: ---------------------------------------------------------------------
[03/22 14:02:55] train_sl INFO: [0.004129648208618164, 0.0038788318634033203, 0.0038716793060302734, 0.0038671493530273438, 0.0038633346557617188, 0.0038733482360839844, 0.003865480422973633, 0.0037343502044677734, 0.0037147998809814453, 0.0036797523498535156, 0.003685474395751953, 0.003711700439453125, 0.003514528274536133, 0.0034859180450439453, 0.0036776065826416016, 0.0036852359771728516, 0.0038232803344726562, 0.003824949264526367, 0.003740549087524414, 0.003880023956298828, 0.003875255584716797, 0.004018068313598633, 0.0037178993225097656, 0.003837108612060547, 0.0038862228393554688, 0.003823995590209961, 0.004112720489501953, 0.004000186920166016, 0.0039637088775634766, 0.003814697265625, 0.004020214080810547, 0.0038530826568603516, 0.003818035125732422, 0.0037882328033447266, 0.0038475990295410156, 0.027878999710083008, 0.003428220748901367, 0.0031807422637939453, 0.003365755081176758, 0.0032296180725097656, 0.003423452377319336, 0.0033826828002929688, 0.003412961959838867, 0.0037517547607421875, 0.003434419631958008, 0.0034189224243164062, 0.0034019947052001953, 0.0034155845642089844, 0.0034151077270507812, 0.003402233123779297]
[03/22 14:02:55] train_sl INFO: Total time taken by SELCON = 0.2094 
Hyperparms:	fraction:0.01	select every:35	num epochs:50	delta:0.1
[03/22 14:02:55] train_sl INFO: DotMap(setting='SL', dataset=DotMap(name='LawSchool_selcon', datadir='../data', feature='dss', type='pre-defined'), dataloader=DotMap(shuffle=True, batch_size=100, pin_memory=False), model=DotMap(architecture='RegressionNet', type='pre-defined', input_dim=10, numclasses=10), ckpt=DotMap(is_load=False, is_save=True, dir='results/', save_every=20), loss=DotMap(type='MeanSquaredLoss', use_sigmoid=False), optimizer=DotMap(type='adam', lr=0.01), scheduler=DotMap(type='StepLR', step_size=1, gamma=0.1), dss_args=DotMap(type='SELCON', fraction=0.01, select_every=35, kappa=0, delta=0.1, linear_layer=False, lam=1e-05, batch_sampler='sequential', selection_type='Supervised', collate_fn=None, model=RegressionNet(
  (linear): Linear(in_features=10, out_features=1, bias=True)
), lr=0.01, loss=MSELoss(), device='cuda', optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.01
    weight_decay: 0
), criterion=MSELoss(), num_classes=10, batch_size=100, num_epochs=50), train_args=DotMap(num_epochs=50, device='cuda', print_every=50, results_dir='results/', print_args=['val_loss', 'tst_loss', 'trn_loss', 'time'], return_args=[]), dss_arg=DotMap(batch_sampler=DotMap()), is_reg=DotMap())
[03/22 14:02:55] train_sl INFO: SELCON: starting pre compute
[03/22 14:02:55] train_sl INFO: SELCON: Finishing F phi
[03/22 14:02:57] train_sl INFO: SELCON: Finishing element wise F
[03/22 14:02:57] train_sl INFO: Model checkpoint saved at epoch: 20
[03/22 14:02:58] train_sl INFO: Epoch: 36, SELCON dataloader subset selection finished, takes 0.0246. 
[03/22 14:02:58] train_sl INFO: Model checkpoint saved at epoch: 40
[03/22 14:02:58] train_sl INFO: Epoch: 50 , Validation Loss: 3.0894286036491394 , Test Loss: 3.0144994616508485 , Training Loss: 2.647395357489586 , Timing: 0.0032927989959716797
[03/22 14:02:58] train_sl INFO: SELCON Selection Run---------------------------------
[03/22 14:02:58] train_sl INFO: Final SubsetTrn: 0.162123
[03/22 14:02:58] train_sl INFO: Validation Loss: 3.09
[03/22 14:02:58] train_sl INFO: Test Data Loss: 3.014499
[03/22 14:02:58] train_sl INFO: ---------------------------------------------------------------------
[03/22 14:02:58] train_sl INFO: SELCON
[03/22 14:02:58] train_sl INFO: ---------------------------------------------------------------------
[03/22 14:02:58] train_sl INFO: [0.00404047966003418, 0.003854513168334961, 0.0038225650787353516, 0.0037949085235595703, 0.0059435367584228516, 0.0038182735443115234, 0.0034868717193603516, 0.0034418106079101562, 0.0034339427947998047, 0.0034799575805664062, 0.003388643264770508, 0.0034275054931640625, 0.0034372806549072266, 0.0036737918853759766, 0.003935813903808594, 0.003790616989135742, 0.0036897659301757812, 0.003726959228515625, 0.003596782684326172, 0.003331422805786133, 0.003155946731567383, 0.003175497055053711, 0.003038644790649414, 0.002991914749145508, 0.0029816627502441406, 0.003145933151245117, 0.0033562183380126953, 0.0032782554626464844, 0.0034782886505126953, 0.003557920455932617, 0.006974220275878906, 0.009379863739013672, 0.00615382194519043, 0.004617214202880859, 0.0045702457427978516, 0.02785181999206543, 0.003116607666015625, 0.0031685829162597656, 0.0032138824462890625, 0.0031862258911132812, 0.004437923431396484, 0.0033731460571289062, 0.0034286975860595703, 0.003210783004760742, 0.0032176971435546875, 0.0034253597259521484, 0.003412008285522461, 0.003232240676879883, 0.0032372474670410156, 0.0032927989959716797]
[03/22 14:02:58] train_sl INFO: Total time taken by SELCON = 0.2138 
Hyperparms:	fraction:0.01	select every:35	num epochs:100	delta:0.01
[03/22 14:02:58] train_sl INFO: DotMap(setting='SL', dataset=DotMap(name='LawSchool_selcon', datadir='../data', feature='dss', type='pre-defined'), dataloader=DotMap(shuffle=True, batch_size=100, pin_memory=False), model=DotMap(architecture='RegressionNet', type='pre-defined', input_dim=10, numclasses=10), ckpt=DotMap(is_load=False, is_save=True, dir='results/', save_every=20), loss=DotMap(type='MeanSquaredLoss', use_sigmoid=False), optimizer=DotMap(type='adam', lr=0.01), scheduler=DotMap(type='StepLR', step_size=1, gamma=0.1), dss_args=DotMap(type='SELCON', fraction=0.01, select_every=35, kappa=0, delta=0.01, linear_layer=False, lam=1e-05, batch_sampler='sequential', selection_type='Supervised', collate_fn=None, model=RegressionNet(
  (linear): Linear(in_features=10, out_features=1, bias=True)
), lr=0.01, loss=MSELoss(), device='cuda', optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.01
    weight_decay: 0
), criterion=MSELoss(), num_classes=10, batch_size=100, num_epochs=50), train_args=DotMap(num_epochs=100, device='cuda', print_every=50, results_dir='results/', print_args=['val_loss', 'tst_loss', 'trn_loss', 'time'], return_args=[]), dss_arg=DotMap(batch_sampler=DotMap()), is_reg=DotMap())
[03/22 14:02:58] train_sl INFO: SELCON: starting pre compute
[03/22 14:02:58] train_sl INFO: SELCON: Finishing F phi
[03/22 14:03:00] train_sl INFO: SELCON: Finishing element wise F
[03/22 14:03:00] train_sl INFO: Model checkpoint saved at epoch: 20
[03/22 14:03:00] train_sl INFO: Epoch: 36, SELCON dataloader subset selection finished, takes 0.0245. 
[03/22 14:03:00] train_sl INFO: Model checkpoint saved at epoch: 40
[03/22 14:03:00] train_sl INFO: Epoch: 50 , Validation Loss: 1.3325148165225982 , Test Loss: 1.4194673717021942 , Training Loss: 0.977211173910361 , Timing: 0.0036547183990478516
[03/22 14:03:00] train_sl INFO: Model checkpoint saved at epoch: 60
[03/22 14:03:01] train_sl INFO: Epoch: 71, SELCON dataloader subset selection finished, takes 0.0244. 
[03/22 14:03:01] train_sl INFO: Model checkpoint saved at epoch: 80
[03/22 14:03:01] train_sl INFO: Epoch: 100 , Validation Loss: 1.3300385057926178 , Test Loss: 1.4118328839540482 , Training Loss: 0.9995313845574856 , Timing: 0.0034263134002685547
[03/22 14:03:01] train_sl INFO: Model checkpoint saved at epoch: 100
[03/22 14:03:01] train_sl INFO: SELCON Selection Run---------------------------------
[03/22 14:03:01] train_sl INFO: Final SubsetTrn: 0.004874
[03/22 14:03:01] train_sl INFO: Validation Loss: 1.33
[03/22 14:03:01] train_sl INFO: Test Data Loss: 1.411833
[03/22 14:03:01] train_sl INFO: ---------------------------------------------------------------------
[03/22 14:03:01] train_sl INFO: SELCON
[03/22 14:03:01] train_sl INFO: ---------------------------------------------------------------------
[03/22 14:03:01] train_sl INFO: [0.003885984420776367, 0.003674745559692383, 0.003655672073364258, 0.0035538673400878906, 0.003709554672241211, 0.003686189651489258, 0.0038394927978515625, 0.003702878952026367, 0.0036919116973876953, 0.0038444995880126953, 0.003905057907104492, 0.003848552703857422, 0.0038580894470214844, 0.003834247589111328, 0.0038149356842041016, 0.0038230419158935547, 0.0038433074951171875, 0.003559589385986328, 0.004221916198730469, 0.0037915706634521484, 0.0040323734283447266, 0.003862619400024414, 0.0039048194885253906, 0.003476858139038086, 0.003452301025390625, 0.0034508705139160156, 0.00366973876953125, 0.0036869049072265625, 0.00350189208984375, 0.0036444664001464844, 0.0034530162811279297, 0.0038306713104248047, 0.0038213729858398438, 0.0038573741912841797, 0.0038459300994873047, 0.02827906608581543, 0.0034155845642089844, 0.003406047821044922, 0.0034284591674804688, 0.003403902053833008, 0.0034422874450683594, 0.003401517868041992, 0.003237485885620117, 0.003233671188354492, 0.0034189224243164062, 0.003457307815551758, 0.003702878952026367, 0.0036597251892089844, 0.0034575462341308594, 0.0036547183990478516, 0.0038154125213623047, 0.003464937210083008, 0.003450155258178711, 0.0036919116973876953, 0.0037450790405273438, 0.003401041030883789, 0.0034635066986083984, 0.003428936004638672, 0.0033979415893554688, 0.003408193588256836, 0.003407001495361328, 0.003450155258178711, 0.0034189224243164062, 0.003467082977294922, 0.0032176971435546875, 0.003184080123901367, 0.0031998157501220703, 0.003217935562133789, 0.0032248497009277344, 0.0034058094024658203, 0.02809286117553711, 0.003408193588256836, 0.0034513473510742188, 0.003427743911743164, 0.0034172534942626953, 0.003414154052734375, 0.0034198760986328125, 0.0034284591674804688, 0.0034308433532714844, 0.003398418426513672, 0.003431558609008789, 0.003401517868041992, 0.003182649612426758, 0.0032112598419189453, 0.003213167190551758, 0.003210783004760742, 0.003456592559814453, 0.003438234329223633, 0.003225564956665039, 0.0034105777740478516, 0.003440380096435547, 0.003442049026489258, 0.0031960010528564453, 0.0031921863555908203, 0.003411531448364258, 0.0032346248626708984, 0.0032057762145996094, 0.0034122467041015625, 0.0034360885620117188, 0.0034263134002685547]
[03/22 14:03:01] train_sl INFO: Total time taken by SELCON = 0.4018 
Hyperparms:	fraction:0.01	select every:35	num epochs:100	delta:0.04
[03/22 14:03:01] train_sl INFO: DotMap(setting='SL', dataset=DotMap(name='LawSchool_selcon', datadir='../data', feature='dss', type='pre-defined'), dataloader=DotMap(shuffle=True, batch_size=100, pin_memory=False), model=DotMap(architecture='RegressionNet', type='pre-defined', input_dim=10, numclasses=10), ckpt=DotMap(is_load=False, is_save=True, dir='results/', save_every=20), loss=DotMap(type='MeanSquaredLoss', use_sigmoid=False), optimizer=DotMap(type='adam', lr=0.01), scheduler=DotMap(type='StepLR', step_size=1, gamma=0.1), dss_args=DotMap(type='SELCON', fraction=0.01, select_every=35, kappa=0, delta=0.04, linear_layer=False, lam=1e-05, batch_sampler='sequential', selection_type='Supervised', collate_fn=None, model=RegressionNet(
  (linear): Linear(in_features=10, out_features=1, bias=True)
), lr=0.01, loss=MSELoss(), device='cuda', optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.01
    weight_decay: 0
), criterion=MSELoss(), num_classes=10, batch_size=100, num_epochs=100), train_args=DotMap(num_epochs=100, device='cuda', print_every=50, results_dir='results/', print_args=['val_loss', 'tst_loss', 'trn_loss', 'time'], return_args=[]), dss_arg=DotMap(batch_sampler=DotMap()), is_reg=DotMap())
[03/22 14:03:01] train_sl INFO: SELCON: starting pre compute
[03/22 14:03:03] train_sl INFO: SELCON: Finishing F phi
[03/22 14:03:06] train_sl INFO: SELCON: Finishing element wise F
[03/22 14:03:06] train_sl INFO: Model checkpoint saved at epoch: 20
[03/22 14:03:06] train_sl INFO: Epoch: 36, SELCON dataloader subset selection finished, takes 0.0241. 
[03/22 14:03:06] train_sl INFO: Model checkpoint saved at epoch: 40
[03/22 14:03:06] train_sl INFO: Epoch: 50 , Validation Loss: 0.14493300151079894 , Test Loss: 0.14405591934919357 , Training Loss: 0.16589588012832862 , Timing: 0.0031790733337402344
[03/22 14:03:06] train_sl INFO: Model checkpoint saved at epoch: 60
[03/22 14:03:06] train_sl INFO: Epoch: 71, SELCON dataloader subset selection finished, takes 0.0240. 
[03/22 14:03:06] train_sl INFO: Model checkpoint saved at epoch: 80
[03/22 14:03:06] train_sl INFO: Epoch: 100 , Validation Loss: 0.08625554703176022 , Test Loss: 0.08722241260111332 , Training Loss: 0.09929929713073832 , Timing: 0.0032193660736083984
[03/22 14:03:06] train_sl INFO: Model checkpoint saved at epoch: 100
[03/22 14:03:06] train_sl INFO: SELCON Selection Run---------------------------------
[03/22 14:03:06] train_sl INFO: Final SubsetTrn: 0.029086
[03/22 14:03:06] train_sl INFO: Validation Loss: 0.09
[03/22 14:03:06] train_sl INFO: Test Data Loss: 0.087222
[03/22 14:03:06] train_sl INFO: ---------------------------------------------------------------------
[03/22 14:03:06] train_sl INFO: SELCON
[03/22 14:03:06] train_sl INFO: ---------------------------------------------------------------------
[03/22 14:03:06] train_sl INFO: [0.004044055938720703, 0.003851652145385742, 0.004338741302490234, 0.0038881301879882812, 0.003828287124633789, 0.003777027130126953, 0.003826618194580078, 0.0038826465606689453, 0.003762960433959961, 0.0037953853607177734, 0.003778696060180664, 0.0037736892700195312, 0.003451824188232422, 0.0036177635192871094, 0.0038568973541259766, 0.0038344860076904297, 0.0039539337158203125, 0.0038208961486816406, 0.003828763961791992, 0.003841876983642578, 0.003990888595581055, 0.0044138431549072266, 0.003814697265625, 0.0038356781005859375, 0.003817319869995117, 0.0036804676055908203, 0.003810882568359375, 0.003856182098388672, 0.003826618194580078, 0.0038156509399414062, 0.004429817199707031, 0.0038509368896484375, 0.0038483142852783203, 0.0037429332733154297, 0.003812074661254883, 0.027789592742919922, 0.003421783447265625, 0.0034685134887695312, 0.00341796875, 0.0034291744232177734, 0.0034236907958984375, 0.0033860206604003906, 0.0034339427947998047, 0.0033872127532958984, 0.0034258365631103516, 0.0034558773040771484, 0.003216266632080078, 0.0031778812408447266, 0.0032265186309814453, 0.0031790733337402344, 0.0036280155181884766, 0.0035715103149414062, 0.003261089324951172, 0.0034389495849609375, 0.003432035446166992, 0.0034372806549072266, 0.003440380096435547, 0.003500223159790039, 0.0033867359161376953, 0.003258943557739258, 0.0032491683959960938, 0.003224611282348633, 0.0034406185150146484, 0.0034296512603759766, 0.003412008285522461, 0.0034177303314208984, 0.0034008026123046875, 0.0033783912658691406, 0.003430604934692383, 0.0033860206604003906, 0.028354406356811523, 0.00341033935546875, 0.003380298614501953, 0.003377199172973633, 0.0033936500549316406, 0.003360271453857422, 0.0031790733337402344, 0.0034661293029785156, 0.0034639835357666016, 0.003321409225463867, 0.003432035446166992, 0.0034427642822265625, 0.003438234329223633, 0.003440380096435547, 0.0032532215118408203, 0.0034568309783935547, 0.0034584999084472656, 0.0034449100494384766, 0.0034503936767578125, 0.003445863723754883, 0.0032427310943603516, 0.0034363269805908203, 0.003461122512817383, 0.0034422874450683594, 0.0034492015838623047, 0.003440380096435547, 0.003461122512817383, 0.0034332275390625, 0.003434419631958008, 0.0032193660736083984]
[03/22 14:03:06] train_sl INFO: Total time taken by SELCON = 0.4053 
Hyperparms:	fraction:0.01	select every:35	num epochs:100	delta:0.1
[03/22 14:03:06] train_sl INFO: DotMap(setting='SL', dataset=DotMap(name='LawSchool_selcon', datadir='../data', feature='dss', type='pre-defined'), dataloader=DotMap(shuffle=True, batch_size=100, pin_memory=False), model=DotMap(architecture='RegressionNet', type='pre-defined', input_dim=10, numclasses=10), ckpt=DotMap(is_load=False, is_save=True, dir='results/', save_every=20), loss=DotMap(type='MeanSquaredLoss', use_sigmoid=False), optimizer=DotMap(type='adam', lr=0.01), scheduler=DotMap(type='StepLR', step_size=1, gamma=0.1), dss_args=DotMap(type='SELCON', fraction=0.01, select_every=35, kappa=0, delta=0.1, linear_layer=False, lam=1e-05, batch_sampler='sequential', selection_type='Supervised', collate_fn=None, model=RegressionNet(
  (linear): Linear(in_features=10, out_features=1, bias=True)
), lr=0.01, loss=MSELoss(), device='cuda', optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.01
    weight_decay: 0
), criterion=MSELoss(), num_classes=10, batch_size=100, num_epochs=100), train_args=DotMap(num_epochs=100, device='cuda', print_every=50, results_dir='results/', print_args=['val_loss', 'tst_loss', 'trn_loss', 'time'], return_args=[]), dss_arg=DotMap(batch_sampler=DotMap()), is_reg=DotMap())
[03/22 14:03:07] train_sl INFO: SELCON: starting pre compute
[03/22 14:03:07] train_sl INFO: SELCON: Finishing F phi
[03/22 14:03:09] train_sl INFO: SELCON: Finishing element wise F
[03/22 14:03:09] train_sl INFO: Model checkpoint saved at epoch: 20
[03/22 14:03:09] train_sl INFO: Epoch: 36, SELCON dataloader subset selection finished, takes 0.0303. 
[03/22 14:03:09] train_sl INFO: Model checkpoint saved at epoch: 40
[03/22 14:03:09] train_sl INFO: Epoch: 50 , Validation Loss: 0.05724025443196297 , Test Loss: 0.05740934461355209 , Training Loss: 0.057897488371684 , Timing: 0.003409147262573242
[03/22 14:03:09] train_sl INFO: Model checkpoint saved at epoch: 60
[03/22 14:03:09] train_sl INFO: Epoch: 71, SELCON dataloader subset selection finished, takes 0.0245. 
[03/22 14:03:09] train_sl INFO: Model checkpoint saved at epoch: 80
[03/22 14:03:09] train_sl INFO: Epoch: 100 , Validation Loss: 0.011619888944551349 , Test Loss: 0.011851965636014938 , Training Loss: 0.011422468687389763 , Timing: 0.003269195556640625
[03/22 14:03:09] train_sl INFO: Model checkpoint saved at epoch: 100
[03/22 14:03:09] train_sl INFO: SELCON Selection Run---------------------------------
[03/22 14:03:09] train_sl INFO: Final SubsetTrn: 0.013002
[03/22 14:03:09] train_sl INFO: Validation Loss: 0.01
[03/22 14:03:09] train_sl INFO: Test Data Loss: 0.011852
[03/22 14:03:09] train_sl INFO: ---------------------------------------------------------------------
[03/22 14:03:09] train_sl INFO: SELCON
[03/22 14:03:09] train_sl INFO: ---------------------------------------------------------------------
[03/22 14:03:09] train_sl INFO: [0.00408172607421875, 0.0036993026733398438, 0.004422664642333984, 0.0036134719848632812, 0.0036628246307373047, 0.003818035125732422, 0.003625631332397461, 0.0038499832153320312, 0.0037708282470703125, 0.003812074661254883, 0.0036661624908447266, 0.003796100616455078, 0.0037872791290283203, 0.003778219223022461, 0.0037736892700195312, 0.0038208961486816406, 0.003765106201171875, 0.003764629364013672, 0.0038170814514160156, 0.0037641525268554688, 0.003821849822998047, 0.003830432891845703, 0.0037856101989746094, 0.0034804344177246094, 0.0033833980560302734, 0.003816843032836914, 0.0037653446197509766, 0.003542184829711914, 0.003650188446044922, 0.0034046173095703125, 0.0035181045532226562, 0.003645658493041992, 0.0038123130798339844, 0.006120204925537109, 0.007875204086303711, 0.033751487731933594, 0.0034322738647460938, 0.0034351348876953125, 0.0033948421478271484, 0.003366231918334961, 0.0034027099609375, 0.003201723098754883, 0.0034363269805908203, 0.0033659934997558594, 0.0034399032592773438, 0.0034186840057373047, 0.0033779144287109375, 0.0034189224243164062, 0.003221750259399414, 0.003409147262573242, 0.003488779067993164, 0.003366708755493164, 0.0033762454986572266, 0.0033674240112304688, 0.0033686161041259766, 0.0036695003509521484, 0.003360748291015625, 0.003391742706298828, 0.003239870071411133, 0.0034482479095458984, 0.003451824188232422, 0.003422260284423828, 0.0034265518188476562, 0.0032186508178710938, 0.0032591819763183594, 0.0032281875610351562, 0.003247499465942383, 0.003267049789428711, 0.0034742355346679688, 0.0034613609313964844, 0.028222322463989258, 0.003481626510620117, 0.003470182418823242, 0.0034284591674804688, 0.003313779830932617, 0.0032324790954589844, 0.0034627914428710938, 0.0034706592559814453, 0.003451824188232422, 0.003423929214477539, 0.0034372806549072266, 0.0034346580505371094, 0.0034265518188476562, 0.0034220218658447266, 0.0034346580505371094, 0.0034329891204833984, 0.0034377574920654297, 0.0034215450286865234, 0.0034172534942626953, 0.003420114517211914, 0.003412008285522461, 0.003417491912841797, 0.0032210350036621094, 0.003209829330444336, 0.0032165050506591797, 0.0031986236572265625, 0.0031938552856445312, 0.0032045841217041016, 0.003209829330444336, 0.003269195556640625]
[03/22 14:03:09] train_sl INFO: Total time taken by SELCON = 0.4119 
Hyperparms:	fraction:0.01	select every:35	num epochs:200	delta:0.01
[03/22 14:03:09] train_sl INFO: DotMap(setting='SL', dataset=DotMap(name='LawSchool_selcon', datadir='../data', feature='dss', type='pre-defined'), dataloader=DotMap(shuffle=True, batch_size=100, pin_memory=False), model=DotMap(architecture='RegressionNet', type='pre-defined', input_dim=10, numclasses=10), ckpt=DotMap(is_load=False, is_save=True, dir='results/', save_every=20), loss=DotMap(type='MeanSquaredLoss', use_sigmoid=False), optimizer=DotMap(type='adam', lr=0.01), scheduler=DotMap(type='StepLR', step_size=1, gamma=0.1), dss_args=DotMap(type='SELCON', fraction=0.01, select_every=35, kappa=0, delta=0.01, linear_layer=False, lam=1e-05, batch_sampler='sequential', selection_type='Supervised', collate_fn=None, model=RegressionNet(
  (linear): Linear(in_features=10, out_features=1, bias=True)
), lr=0.01, loss=MSELoss(), device='cuda', optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.01
    weight_decay: 0
), criterion=MSELoss(), num_classes=10, batch_size=100, num_epochs=100), train_args=DotMap(num_epochs=200, device='cuda', print_every=50, results_dir='results/', print_args=['val_loss', 'tst_loss', 'trn_loss', 'time'], return_args=[]), dss_arg=DotMap(batch_sampler=DotMap()), is_reg=DotMap())
[03/22 14:03:10] train_sl INFO: SELCON: starting pre compute
[03/22 14:05:34] train_sl INFO: SELCON: Finishing F phi
[03/22 14:05:36] train_sl INFO: SELCON: Finishing element wise F
[03/22 14:05:36] train_sl INFO: Model checkpoint saved at epoch: 20
[03/22 14:05:36] train_sl INFO: Epoch: 36, SELCON dataloader subset selection finished, takes 0.0225. 
[03/22 14:05:36] train_sl INFO: Model checkpoint saved at epoch: 40
[03/22 14:05:37] train_sl INFO: Epoch: 50 , Validation Loss: 1.0427993953227996 , Test Loss: 1.047402524203062 , Training Loss: 0.541115587290663 , Timing: 0.003118276596069336
[03/22 14:05:37] train_sl INFO: Model checkpoint saved at epoch: 60
[03/22 14:05:37] train_sl INFO: Epoch: 71, SELCON dataloader subset selection finished, takes 0.0247. 
[03/22 14:05:37] train_sl INFO: Model checkpoint saved at epoch: 80
[03/22 14:05:37] train_sl INFO: Epoch: 100 , Validation Loss: 0.6563402280211449 , Test Loss: 0.6583782389760018 , Training Loss: 0.39767477102577686 , Timing: 0.003203153610229492
[03/22 14:05:37] train_sl INFO: Model checkpoint saved at epoch: 100
[03/22 14:05:37] train_sl INFO: Epoch: 106, SELCON dataloader subset selection finished, takes 0.0246. 
[03/22 14:05:37] train_sl INFO: Model checkpoint saved at epoch: 120
[03/22 14:05:37] train_sl INFO: Model checkpoint saved at epoch: 140
[03/22 14:05:37] train_sl INFO: Epoch: 141, SELCON dataloader subset selection finished, takes 0.0240. 
[03/22 14:05:37] train_sl INFO: Epoch: 150 , Validation Loss: 0.4723953254520893 , Test Loss: 0.474724917858839 , Training Loss: 0.3045460112536183 , Timing: 0.003409862518310547
[03/22 14:05:37] train_sl INFO: Model checkpoint saved at epoch: 160
[03/22 14:05:37] train_sl INFO: Epoch: 176, SELCON dataloader subset selection finished, takes 0.0244. 
[03/22 14:05:37] train_sl INFO: Model checkpoint saved at epoch: 180
[03/22 14:05:38] train_sl INFO: Epoch: 200 , Validation Loss: 0.3607924371957779 , Test Loss: 0.36289609894156455 , Training Loss: 0.24748440616979048 , Timing: 0.003278970718383789
[03/22 14:05:38] train_sl INFO: Model checkpoint saved at epoch: 200
[03/22 14:05:38] train_sl INFO: SELCON Selection Run---------------------------------
[03/22 14:05:38] train_sl INFO: Final SubsetTrn: 0.246176
[03/22 14:05:38] train_sl INFO: Validation Loss: 0.36
[03/22 14:05:38] train_sl INFO: Test Data Loss: 0.362896
[03/22 14:05:38] train_sl INFO: ---------------------------------------------------------------------
[03/22 14:05:38] train_sl INFO: SELCON
[03/22 14:05:38] train_sl INFO: ---------------------------------------------------------------------
[03/22 14:05:38] train_sl INFO: [0.003863811492919922, 0.003725290298461914, 0.003714323043823242, 0.004069089889526367, 0.003737926483154297, 0.0038928985595703125, 0.0038802623748779297, 0.003711700439453125, 0.003863096237182617, 0.0038352012634277344, 0.003905057907104492, 0.0038509368896484375, 0.004462718963623047, 0.004947185516357422, 0.0038843154907226562, 0.004102468490600586, 0.003882884979248047, 0.008031845092773438, 0.003413677215576172, 0.003303050994873047, 0.0034508705139160156, 0.0034630298614501953, 0.003478527069091797, 0.004057884216308594, 0.003435373306274414, 0.0036284923553466797, 0.0036499500274658203, 0.0037941932678222656, 0.0036835670471191406, 0.0033202171325683594, 0.00319671630859375, 0.003255605697631836, 0.0034890174865722656, 0.0035033226013183594, 0.0034525394439697266, 0.026072025299072266, 0.0030722618103027344, 0.003088235855102539, 0.003112316131591797, 0.003113269805908203, 0.0032739639282226562, 0.003085613250732422, 0.0030663013458251953, 0.00310516357421875, 0.003098011016845703, 0.003099203109741211, 0.0030732154846191406, 0.003278493881225586, 0.0031075477600097656, 0.003118276596069336, 0.003393411636352539, 0.0035016536712646484, 0.0035200119018554688, 0.003450632095336914, 0.0035119056701660156, 0.003343343734741211, 0.00327301025390625, 0.0032873153686523438, 0.003245830535888672, 0.0032846927642822266, 0.003533601760864258, 0.003330230712890625, 0.0032596588134765625, 0.003273487091064453, 0.0032482147216796875, 0.0032379627227783203, 0.0033271312713623047, 0.0032312870025634766, 0.00321197509765625, 0.003277301788330078, 0.02840423583984375, 0.0034360885620117188, 0.0032737255096435547, 0.003229379653930664, 0.003313302993774414, 0.003427267074584961, 0.003406047821044922, 0.003457307815551758, 0.003432035446166992, 0.003218412399291992, 0.0034286975860595703, 0.003452301025390625, 0.0034618377685546875, 0.0034461021423339844, 0.003437519073486328, 0.0033147335052490234, 0.00344085693359375, 0.0034151077270507812, 0.003286600112915039, 0.003515958786010742, 0.0034737586975097656, 0.003233671188354492, 0.0032143592834472656, 0.003210306167602539, 0.0032548904418945312, 0.0032186508178710938, 0.003223419189453125, 0.003223896026611328, 0.0032525062561035156, 0.003203153610229492, 0.0034699440002441406, 0.003426074981689453, 0.0034554004669189453, 0.0034303665161132812, 0.003416299819946289, 0.028279781341552734, 0.003488779067993164, 0.0034117698669433594, 0.0034499168395996094, 0.00345611572265625, 0.0033369064331054688, 0.0031898021697998047, 0.003218412399291992, 0.003201007843017578, 0.0031991004943847656, 0.0032262802124023438, 0.0032074451446533203, 0.0032541751861572266, 0.0032095909118652344, 0.0033152103424072266, 0.003232717514038086, 0.003246784210205078, 0.003233194351196289, 0.003228425979614258, 0.003191232681274414, 0.003270387649536133, 0.0031960010528564453, 0.0032622814178466797, 0.003261089324951172, 0.003227710723876953, 0.0032095909118652344, 0.0032024383544921875, 0.0032196044921875, 0.003298044204711914, 0.003454923629760742, 0.0034322738647460938, 0.003445148468017578, 0.0034482479095458984, 0.0034112930297851562, 0.003383159637451172, 0.028337955474853516, 0.003525257110595703, 0.0034301280975341797, 0.0034513473510742188, 0.0032510757446289062, 0.003391742706298828, 0.003403186798095703, 0.0034360885620117188, 0.0032546520233154297, 0.003409862518310547, 0.003463268280029297, 0.003401041030883789, 0.0036525726318359375, 0.0033054351806640625, 0.0034215450286865234, 0.0034270286560058594, 0.0032472610473632812, 0.0033998489379882812, 0.0034193992614746094, 0.0033979415893554688, 0.003432750701904297, 0.0033974647521972656, 0.0034418106079101562, 0.0032300949096679688, 0.0034677982330322266, 0.0032134056091308594, 0.003446340560913086, 0.0034575462341308594, 0.0034034252166748047, 0.0033926963806152344, 0.003397703170776367, 0.0036802291870117188, 0.0033168792724609375, 0.003272533416748047, 0.003440380096435547, 0.028102636337280273, 0.0032629966735839844, 0.0034589767456054688, 0.0034041404724121094, 0.0033922195434570312, 0.004066944122314453, 0.003398895263671875, 0.003427743911743164, 0.0032711029052734375, 0.003448009490966797, 0.0034551620483398438, 0.003469705581665039, 0.0034797191619873047, 0.0034537315368652344, 0.003450155258178711, 0.003260374069213867, 0.003467082977294922, 0.00347900390625, 0.003513336181640625, 0.003469228744506836, 0.003478527069091797, 0.0032625198364257812, 0.003231525421142578, 0.003285646438598633, 0.003278970718383789]
[03/22 14:05:38] train_sl INFO: Total time taken by SELCON = 0.8085 
Hyperparms:	fraction:0.01	select every:35	num epochs:200	delta:0.04
[03/22 14:05:38] train_sl INFO: DotMap(setting='SL', dataset=DotMap(name='LawSchool_selcon', datadir='../data', feature='dss', type='pre-defined'), dataloader=DotMap(shuffle=True, batch_size=100, pin_memory=False), model=DotMap(architecture='RegressionNet', type='pre-defined', input_dim=10, numclasses=10), ckpt=DotMap(is_load=False, is_save=True, dir='results/', save_every=20), loss=DotMap(type='MeanSquaredLoss', use_sigmoid=False), optimizer=DotMap(type='adam', lr=0.01), scheduler=DotMap(type='StepLR', step_size=1, gamma=0.1), dss_args=DotMap(type='SELCON', fraction=0.01, select_every=35, kappa=0, delta=0.04, linear_layer=False, lam=1e-05, batch_sampler='sequential', selection_type='Supervised', collate_fn=None, model=RegressionNet(
  (linear): Linear(in_features=10, out_features=1, bias=True)
), lr=0.01, loss=MSELoss(), device='cuda', optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.01
    weight_decay: 0
), criterion=MSELoss(), num_classes=10, batch_size=100, num_epochs=200), train_args=DotMap(num_epochs=200, device='cuda', print_every=50, results_dir='results/', print_args=['val_loss', 'tst_loss', 'trn_loss', 'time'], return_args=[]), dss_arg=DotMap(batch_sampler=DotMap()), is_reg=DotMap())
[03/22 14:05:38] train_sl INFO: SELCON: starting pre compute
[03/22 14:05:38] train_sl INFO: SELCON: Finishing F phi
[03/22 14:05:40] train_sl INFO: SELCON: Finishing element wise F
[03/22 14:05:40] train_sl INFO: Model checkpoint saved at epoch: 20
[03/22 14:05:40] train_sl INFO: Epoch: 36, SELCON dataloader subset selection finished, takes 0.0402. 
[03/22 14:05:40] train_sl INFO: Model checkpoint saved at epoch: 40
[03/22 14:05:41] train_sl INFO: Epoch: 50 , Validation Loss: 2.1609042644500733 , Test Loss: 2.2630667984485626 , Training Loss: 1.9502700802225332 , Timing: 0.0033500194549560547
[03/22 14:05:41] train_sl INFO: Model checkpoint saved at epoch: 60
[03/22 14:05:41] train_sl INFO: Epoch: 71, SELCON dataloader subset selection finished, takes 0.0246. 
[03/22 14:05:41] train_sl INFO: Model checkpoint saved at epoch: 80
[03/22 14:05:41] train_sl INFO: Epoch: 100 , Validation Loss: 3.25951127409935 , Test Loss: 3.441423201560974 , Training Loss: 2.840021721445597 , Timing: 0.004687070846557617
[03/22 14:05:41] train_sl INFO: Model checkpoint saved at epoch: 100
[03/22 14:05:41] train_sl INFO: Epoch: 106, SELCON dataloader subset selection finished, takes 0.0245. 
[03/22 14:05:41] train_sl INFO: Model checkpoint saved at epoch: 120
[03/22 14:05:41] train_sl INFO: Model checkpoint saved at epoch: 140
[03/22 14:05:41] train_sl INFO: Epoch: 141, SELCON dataloader subset selection finished, takes 0.0239. 
[03/22 14:05:41] train_sl INFO: Epoch: 150 , Validation Loss: 3.272114998102188 , Test Loss: 3.4572789907455443 , Training Loss: 2.834023259007014 , Timing: 0.003395557403564453
[03/22 14:05:41] train_sl INFO: Model checkpoint saved at epoch: 160
[03/22 14:05:41] train_sl INFO: Epoch: 176, SELCON dataloader subset selection finished, takes 0.0238. 
[03/22 14:05:41] train_sl INFO: Model checkpoint saved at epoch: 180
[03/22 14:05:42] train_sl INFO: Epoch: 200 , Validation Loss: 3.4860835909843444 , Test Loss: 3.6828843533992766 , Training Loss: 3.0174936675108395 , Timing: 0.0034415721893310547
[03/22 14:05:42] train_sl INFO: Model checkpoint saved at epoch: 200
[03/22 14:05:42] train_sl INFO: SELCON Selection Run---------------------------------
[03/22 14:05:42] train_sl INFO: Final SubsetTrn: 0.004615
[03/22 14:05:42] train_sl INFO: Validation Loss: 3.49
[03/22 14:05:42] train_sl INFO: Test Data Loss: 3.682884
[03/22 14:05:42] train_sl INFO: ---------------------------------------------------------------------
[03/22 14:05:42] train_sl INFO: SELCON
[03/22 14:05:42] train_sl INFO: ---------------------------------------------------------------------
[03/22 14:05:42] train_sl INFO: [0.004245758056640625, 0.0033729076385498047, 0.0032193660736083984, 0.003455638885498047, 0.00409245491027832, 0.003681659698486328, 0.00394749641418457, 0.0041446685791015625, 0.003949403762817383, 0.0039288997650146484, 0.003960609436035156, 0.0038115978240966797, 0.003702878952026367, 0.0036890506744384766, 0.0038487911224365234, 0.003900766372680664, 0.0037627220153808594, 0.0039000511169433594, 0.00395965576171875, 0.003860950469970703, 0.0038301944732666016, 0.0036928653717041016, 0.003953218460083008, 0.0037326812744140625, 0.0037055015563964844, 0.003573894500732422, 0.0038743019104003906, 0.003881692886352539, 0.003905773162841797, 0.0038788318634033203, 0.0037403106689453125, 0.0036699771881103516, 0.004716634750366211, 0.004443168640136719, 0.0058917999267578125, 0.04384803771972656, 0.003242969512939453, 0.007286787033081055, 0.00859212875366211, 0.00687098503112793, 0.00588536262512207, 0.0054547786712646484, 0.004704952239990234, 0.004719734191894531, 0.004300594329833984, 0.004397153854370117, 0.004109621047973633, 0.00397181510925293, 0.003693103790283203, 0.0033500194549560547, 0.0033943653106689453, 0.003262042999267578, 0.003246307373046875, 0.0034685134887695312, 0.0034215450286865234, 0.0034379959106445312, 0.003481149673461914, 0.0032689571380615234, 0.003283977508544922, 0.0032269954681396484, 0.0034241676330566406, 0.003443002700805664, 0.003437519073486328, 0.003435850143432617, 0.0034379959106445312, 0.00342559814453125, 0.003509044647216797, 0.003401517868041992, 0.0034062862396240234, 0.0034165382385253906, 0.02844548225402832, 0.003467082977294922, 0.0034341812133789062, 0.0032241344451904297, 0.003406047821044922, 0.003407001495361328, 0.0032231807708740234, 0.003407716751098633, 0.0032193660736083984, 0.003420114517211914, 0.003470182418823242, 0.003335237503051758, 0.003263711929321289, 0.0032401084899902344, 0.003213167190551758, 0.0032498836517333984, 0.0032503604888916016, 0.0033185482025146484, 0.0032303333282470703, 0.003209829330444336, 0.003405332565307617, 0.0032210350036621094, 0.0032050609588623047, 0.003214120864868164, 0.003203153610229492, 0.003187417984008789, 0.003270864486694336, 0.0038115978240966797, 0.004853010177612305, 0.004687070846557617, 0.0038907527923583984, 0.003469228744506836, 0.003442049026489258, 0.003213644027709961, 0.0034110546112060547, 0.02826666831970215, 0.0032186508178710938, 0.0033845901489257812, 0.0038712024688720703, 0.003213167190551758, 0.0034029483795166016, 0.003373861312866211, 0.0032262802124023438, 0.0033783912658691406, 0.0034012794494628906, 0.0033876895904541016, 0.003378629684448242, 0.003407716751098633, 0.0033779144287109375, 0.0034117698669433594, 0.0034189224243164062, 0.003422975540161133, 0.003240823745727539, 0.0031833648681640625, 0.003180265426635742, 0.0031561851501464844, 0.0031995773315429688, 0.003171205520629883, 0.0032434463500976562, 0.0035538673400878906, 0.003397703170776367, 0.003364086151123047, 0.0033478736877441406, 0.0033521652221679688, 0.003399372100830078, 0.003204822540283203, 0.0031578540802001953, 0.0033721923828125, 0.0033712387084960938, 0.003226757049560547, 0.027522802352905273, 0.0033960342407226562, 0.003431558609008789, 0.0034093856811523438, 0.003291606903076172, 0.0033712387084960938, 0.003371715545654297, 0.003375530242919922, 0.003411531448364258, 0.003395557403564453, 0.0033729076385498047, 0.0033609867095947266, 0.0033745765686035156, 0.003393888473510742, 0.00403594970703125, 0.0033986568450927734, 0.0033414363861083984, 0.003438234329223633, 0.003350973129272461, 0.003366708755493164, 0.0034363269805908203, 0.0033783912658691406, 0.003431081771850586, 0.003351449966430664, 0.0033736228942871094, 0.003384113311767578, 0.0033843517303466797, 0.0034027099609375, 0.0033910274505615234, 0.003428936004638672, 0.0033953189849853516, 0.0033800601959228516, 0.0033838748931884766, 0.0034148693084716797, 0.003355741500854492, 0.027380943298339844, 0.003369569778442383, 0.0036771297454833984, 0.003233671188354492, 0.0034072399139404297, 0.0034227371215820312, 0.0034220218658447266, 0.003393888473510742, 0.0034008026123046875, 0.0034236907958984375, 0.003430604934692383, 0.0034101009368896484, 0.003202199935913086, 0.003194570541381836, 0.003186941146850586, 0.0032036304473876953, 0.0032105445861816406, 0.0032083988189697266, 0.0032193660736083984, 0.0034160614013671875, 0.0032532215118408203, 0.0032050609588623047, 0.0034317970275878906, 0.003411531448364258, 0.0034415721893310547]
[03/22 14:05:42] train_sl INFO: Total time taken by SELCON = 0.8566 
Hyperparms:	fraction:0.01	select every:35	num epochs:200	delta:0.1
[03/22 14:05:42] train_sl INFO: DotMap(setting='SL', dataset=DotMap(name='LawSchool_selcon', datadir='../data', feature='dss', type='pre-defined'), dataloader=DotMap(shuffle=True, batch_size=100, pin_memory=False), model=DotMap(architecture='RegressionNet', type='pre-defined', input_dim=10, numclasses=10), ckpt=DotMap(is_load=False, is_save=True, dir='results/', save_every=20), loss=DotMap(type='MeanSquaredLoss', use_sigmoid=False), optimizer=DotMap(type='adam', lr=0.01), scheduler=DotMap(type='StepLR', step_size=1, gamma=0.1), dss_args=DotMap(type='SELCON', fraction=0.01, select_every=35, kappa=0, delta=0.1, linear_layer=False, lam=1e-05, batch_sampler='sequential', selection_type='Supervised', collate_fn=None, model=RegressionNet(
  (linear): Linear(in_features=10, out_features=1, bias=True)
), lr=0.01, loss=MSELoss(), device='cuda', optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.01
    weight_decay: 0
), criterion=MSELoss(), num_classes=10, batch_size=100, num_epochs=200), train_args=DotMap(num_epochs=200, device='cuda', print_every=50, results_dir='results/', print_args=['val_loss', 'tst_loss', 'trn_loss', 'time'], return_args=[]), dss_arg=DotMap(batch_sampler=DotMap()), is_reg=DotMap())
[03/22 14:05:42] train_sl INFO: SELCON: starting pre compute
[03/22 14:05:42] train_sl INFO: SELCON: Finishing F phi
[03/22 14:05:44] train_sl INFO: SELCON: Finishing element wise F
[03/22 14:05:44] train_sl INFO: Model checkpoint saved at epoch: 20
[03/22 14:05:44] train_sl INFO: Epoch: 36, SELCON dataloader subset selection finished, takes 0.0240. 
[03/22 14:05:44] train_sl INFO: Model checkpoint saved at epoch: 40
[03/22 14:05:44] train_sl INFO: Epoch: 50 , Validation Loss: 0.8830234795808792 , Test Loss: 0.8815176889300347 , Training Loss: 0.8319087942632345 , Timing: 0.003406524658203125
[03/22 14:05:44] train_sl INFO: Model checkpoint saved at epoch: 60
[03/22 14:05:44] train_sl INFO: Epoch: 71, SELCON dataloader subset selection finished, takes 0.0245. 
[03/22 14:05:44] train_sl INFO: Model checkpoint saved at epoch: 80
[03/22 14:05:45] train_sl INFO: Epoch: 100 , Validation Loss: 0.9625239670276642 , Test Loss: 0.9658966690301896 , Training Loss: 0.9591840284948165 , Timing: 0.0032019615173339844
[03/22 14:05:45] train_sl INFO: Model checkpoint saved at epoch: 100
[03/22 14:05:45] train_sl INFO: Epoch: 106, SELCON dataloader subset selection finished, takes 0.0245. 
[03/22 14:05:45] train_sl INFO: Model checkpoint saved at epoch: 120
[03/22 14:05:45] train_sl INFO: Model checkpoint saved at epoch: 140
[03/22 14:05:45] train_sl INFO: Epoch: 141, SELCON dataloader subset selection finished, takes 0.0237. 
[03/22 14:05:45] train_sl INFO: Epoch: 150 , Validation Loss: 0.9678406536579132 , Test Loss: 0.9713254615664482 , Training Loss: 0.9670176841318607 , Timing: 0.0034284591674804688
[03/22 14:05:45] train_sl INFO: Model checkpoint saved at epoch: 160
[03/22 14:05:45] train_sl INFO: Epoch: 176, SELCON dataloader subset selection finished, takes 0.0244. 
[03/22 14:05:45] train_sl INFO: Model checkpoint saved at epoch: 180
[03/22 14:05:45] train_sl INFO: Epoch: 200 , Validation Loss: 0.9885194778442383 , Test Loss: 0.9930826753377915 , Training Loss: 1.0016291118585146 , Timing: 0.0034132003784179688
[03/22 14:05:45] train_sl INFO: Model checkpoint saved at epoch: 200
[03/22 14:05:45] train_sl INFO: SELCON Selection Run---------------------------------
[03/22 14:05:45] train_sl INFO: Final SubsetTrn: 0.003251
[03/22 14:05:45] train_sl INFO: Validation Loss: 0.99
[03/22 14:05:45] train_sl INFO: Test Data Loss: 0.993083
[03/22 14:05:45] train_sl INFO: ---------------------------------------------------------------------
[03/22 14:05:45] train_sl INFO: SELCON
[03/22 14:05:45] train_sl INFO: ---------------------------------------------------------------------
[03/22 14:05:45] train_sl INFO: [0.003990888595581055, 0.0038013458251953125, 0.0039386749267578125, 0.0037994384765625, 0.0037736892700195312, 0.003649473190307617, 0.0038030147552490234, 0.0037899017333984375, 0.0038950443267822266, 0.003749370574951172, 0.003759145736694336, 0.003776073455810547, 0.0037496089935302734, 0.0039010047912597656, 0.003781557083129883, 0.0037865638732910156, 0.0036802291870117188, 0.0037734508514404297, 0.003793954849243164, 0.0038406848907470703, 0.003809213638305664, 0.003800630569458008, 0.0037577152252197266, 0.004456281661987305, 0.0037658214569091797, 0.0034139156341552734, 0.0034203529357910156, 0.0037822723388671875, 0.003805398941040039, 0.0036203861236572266, 0.0037996768951416016, 0.0036308765411376953, 0.0037512779235839844, 0.003641366958618164, 0.0037796497344970703, 0.027660608291625977, 0.0033795833587646484, 0.0034492015838623047, 0.0033719539642333984, 0.003387928009033203, 0.003428220748901367, 0.003391265869140625, 0.0034117698669433594, 0.003710508346557617, 0.003389596939086914, 0.0033898353576660156, 0.0033845901489257812, 0.0033843517303466797, 0.0033979415893554688, 0.003406524658203125, 0.0033795833587646484, 0.0033414363861083984, 0.0033447742462158203, 0.003393888473510742, 0.0033884048461914062, 0.003420591354370117, 0.003451824188232422, 0.003346681594848633, 0.003393888473510742, 0.0033681392669677734, 0.003442049026489258, 0.003201723098754883, 0.0034203529357910156, 0.0034220218658447266, 0.0034079551696777344, 0.003215312957763672, 0.0032165050506591797, 0.0032072067260742188, 0.0032296180725097656, 0.003215789794921875, 0.028215408325195312, 0.0034089088439941406, 0.0034072399139404297, 0.003403902053833008, 0.003392457962036133, 0.0033969879150390625, 0.0034105777740478516, 0.003193378448486328, 0.0034024715423583984, 0.003398418426513672, 0.0034766197204589844, 0.0034284591674804688, 0.003420591354370117, 0.0033922195434570312, 0.003209829330444336, 0.0032312870025634766, 0.003410816192626953, 0.003419637680053711, 0.00341796875, 0.003457307815551758, 0.0033843517303466797, 0.0033152103424072266, 0.003222942352294922, 0.0034008026123046875, 0.003425121307373047, 0.0034027099609375, 0.0034275054931640625, 0.0034012794494628906, 0.003401041030883789, 0.0032019615173339844, 0.003892660140991211, 0.0034520626068115234, 0.0034432411193847656, 0.003453969955444336, 0.003425121307373047, 0.028327226638793945, 0.003458261489868164, 0.0034334659576416016, 0.0032196044921875, 0.003407716751098633, 0.0032134056091308594, 0.0034008026123046875, 0.0034062862396240234, 0.0034055709838867188, 0.0034046173095703125, 0.003410816192626953, 0.0034062862396240234, 0.004133462905883789, 0.0034279823303222656, 0.0034236907958984375, 0.003420591354370117, 0.0033740997314453125, 0.003191709518432617, 0.0031936168670654297, 0.0031919479370117188, 0.003275156021118164, 0.003135204315185547, 0.0033495426177978516, 0.00321197509765625, 0.0031676292419433594, 0.0032160282135009766, 0.0031528472900390625, 0.0033829212188720703, 0.0034592151641845703, 0.0033996105194091797, 0.003421306610107422, 0.0034220218658447266, 0.0033969879150390625, 0.0034177303314208984, 0.0033721923828125, 0.027293682098388672, 0.003360271453857422, 0.003343820571899414, 0.003399372100830078, 0.0033960342407226562, 0.003347158432006836, 0.0032105445861816406, 0.0034394264221191406, 0.004059553146362305, 0.0034284591674804688, 0.006351947784423828, 0.0032858848571777344, 0.003225088119506836, 0.0033180713653564453, 0.0033757686614990234, 0.00335693359375, 0.0034246444702148438, 0.0033957958221435547, 0.0034096240997314453, 0.0034291744232177734, 0.0034346580505371094, 0.0034563541412353516, 0.003405332565307617, 0.00341796875, 0.003415822982788086, 0.0034453868865966797, 0.0034089088439941406, 0.0032575130462646484, 0.003422975540161133, 0.0034151077270507812, 0.0034186840057373047, 0.0034165382385253906, 0.0034170150756835938, 0.0034101009368896484, 0.003410816192626953, 0.028107404708862305, 0.003426790237426758, 0.003409862518310547, 0.004082441329956055, 0.0034177303314208984, 0.003442049026489258, 0.0034241676330566406, 0.0034203529357910156, 0.0034055709838867188, 0.0034372806549072266, 0.003418445587158203, 0.003396749496459961, 0.003395557403564453, 0.0034418106079101562, 0.003445148468017578, 0.0034208297729492188, 0.003388643264770508, 0.003438234329223633, 0.0033788681030273438, 0.0034058094024658203, 0.0033915042877197266, 0.003397703170776367, 0.003396272659301758, 0.0033783912658691406, 0.0034132003784179688]
[03/22 14:05:45] train_sl INFO: Total time taken by SELCON = 0.8177 
Hyperparms:	fraction:0.01	select every:35	num epochs:500	delta:0.01
[03/22 14:05:45] train_sl INFO: DotMap(setting='SL', dataset=DotMap(name='LawSchool_selcon', datadir='../data', feature='dss', type='pre-defined'), dataloader=DotMap(shuffle=True, batch_size=100, pin_memory=False), model=DotMap(architecture='RegressionNet', type='pre-defined', input_dim=10, numclasses=10), ckpt=DotMap(is_load=False, is_save=True, dir='results/', save_every=20), loss=DotMap(type='MeanSquaredLoss', use_sigmoid=False), optimizer=DotMap(type='adam', lr=0.01), scheduler=DotMap(type='StepLR', step_size=1, gamma=0.1), dss_args=DotMap(type='SELCON', fraction=0.01, select_every=35, kappa=0, delta=0.01, linear_layer=False, lam=1e-05, batch_sampler='sequential', selection_type='Supervised', collate_fn=None, model=RegressionNet(
  (linear): Linear(in_features=10, out_features=1, bias=True)
), lr=0.01, loss=MSELoss(), device='cuda', optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.01
    weight_decay: 0
), criterion=MSELoss(), num_classes=10, batch_size=100, num_epochs=200), train_args=DotMap(num_epochs=500, device='cuda', print_every=50, results_dir='results/', print_args=['val_loss', 'tst_loss', 'trn_loss', 'time'], return_args=[]), dss_arg=DotMap(batch_sampler=DotMap()), is_reg=DotMap())
[03/22 14:05:45] train_sl INFO: SELCON: starting pre compute
[03/22 14:06:54] train_sl INFO: SELCON: Finishing F phi
[03/22 14:06:57] train_sl INFO: SELCON: Finishing element wise F
[03/22 14:06:57] train_sl INFO: Model checkpoint saved at epoch: 20
[03/22 14:06:57] train_sl INFO: Epoch: 36, SELCON dataloader subset selection finished, takes 0.0222. 
[03/22 14:06:57] train_sl INFO: Model checkpoint saved at epoch: 40
[03/22 14:06:57] train_sl INFO: Epoch: 50 , Validation Loss: 0.22216201424598694 , Test Loss: 0.22646706849336623 , Training Loss: 0.20656549278646708 , Timing: 0.002988100051879883
[03/22 14:06:57] train_sl INFO: Model checkpoint saved at epoch: 60
[03/22 14:06:57] train_sl INFO: Epoch: 71, SELCON dataloader subset selection finished, takes 0.0245. 
[03/22 14:06:57] train_sl INFO: Model checkpoint saved at epoch: 80
[03/22 14:06:57] train_sl INFO: Epoch: 100 , Validation Loss: 0.07924883086234331 , Test Loss: 0.0843675471842289 , Training Loss: 0.0638947631411541 , Timing: 0.003408193588256836
[03/22 14:06:57] train_sl INFO: Model checkpoint saved at epoch: 100
[03/22 14:06:57] train_sl INFO: Epoch: 106, SELCON dataloader subset selection finished, takes 0.0325. 
[03/22 14:06:57] train_sl INFO: Model checkpoint saved at epoch: 120
[03/22 14:06:57] train_sl INFO: Model checkpoint saved at epoch: 140
[03/22 14:06:57] train_sl INFO: Epoch: 141, SELCON dataloader subset selection finished, takes 0.0217. 
[03/22 14:06:58] train_sl INFO: Epoch: 150 , Validation Loss: 0.03892257865518332 , Test Loss: 0.04258756227791309 , Training Loss: 0.0298190582722712 , Timing: 0.0029249191284179688
[03/22 14:06:58] train_sl INFO: Model checkpoint saved at epoch: 160
[03/22 14:06:58] train_sl INFO: Epoch: 176, SELCON dataloader subset selection finished, takes 0.0220. 
[03/22 14:06:58] train_sl INFO: Model checkpoint saved at epoch: 180
[03/22 14:06:58] train_sl INFO: Epoch: 200 , Validation Loss: 0.019484117301180957 , Test Loss: 0.021529421955347062 , Training Loss: 0.019356738395379998 , Timing: 0.0029916763305664062
[03/22 14:06:58] train_sl INFO: Model checkpoint saved at epoch: 200
[03/22 14:06:58] train_sl INFO: Epoch: 211, SELCON dataloader subset selection finished, takes 0.0215. 
[03/22 14:06:58] train_sl INFO: Model checkpoint saved at epoch: 220
[03/22 14:06:58] train_sl INFO: Model checkpoint saved at epoch: 240
[03/22 14:06:58] train_sl INFO: Epoch: 246, SELCON dataloader subset selection finished, takes 0.0218. 
[03/22 14:06:58] train_sl INFO: Epoch: 250 , Validation Loss: 0.023593092896044256 , Test Loss: 0.026261089835315943 , Training Loss: 0.02376488585007162 , Timing: 0.0029573440551757812
[03/22 14:06:58] train_sl INFO: Model checkpoint saved at epoch: 260
[03/22 14:06:58] train_sl INFO: Model checkpoint saved at epoch: 280
[03/22 14:06:58] train_sl INFO: Epoch: 281, SELCON dataloader subset selection finished, takes 0.0219. 
[03/22 14:06:58] train_sl INFO: Epoch: 300 , Validation Loss: 0.013587354589253664 , Test Loss: 0.014941041311249136 , Training Loss: 0.014341697553530909 , Timing: 0.0029413700103759766
[03/22 14:06:58] train_sl INFO: Model checkpoint saved at epoch: 300
[03/22 14:06:58] train_sl INFO: Epoch: 316, SELCON dataloader subset selection finished, takes 0.0215. 
[03/22 14:06:58] train_sl INFO: Model checkpoint saved at epoch: 320
[03/22 14:06:59] train_sl INFO: Model checkpoint saved at epoch: 340
[03/22 14:06:59] train_sl INFO: Epoch: 350 , Validation Loss: 0.024519015895202757 , Test Loss: 0.026569998171180487 , Training Loss: 0.02684539457102521 , Timing: 0.003004312515258789
[03/22 14:06:59] train_sl INFO: Epoch: 351, SELCON dataloader subset selection finished, takes 0.0219. 
[03/22 14:06:59] train_sl INFO: Model checkpoint saved at epoch: 360
[03/22 14:06:59] train_sl INFO: Model checkpoint saved at epoch: 380
[03/22 14:06:59] train_sl INFO: Epoch: 386, SELCON dataloader subset selection finished, takes 0.0219. 
[03/22 14:06:59] train_sl INFO: Epoch: 400 , Validation Loss: 0.023213107511401175 , Test Loss: 0.023696155101060868 , Training Loss: 0.023508841619611934 , Timing: 0.0051229000091552734
[03/22 14:06:59] train_sl INFO: Model checkpoint saved at epoch: 400
[03/22 14:06:59] train_sl INFO: Model checkpoint saved at epoch: 420
[03/22 14:06:59] train_sl INFO: Epoch: 421, SELCON dataloader subset selection finished, takes 0.0243. 
[03/22 14:06:59] train_sl INFO: Model checkpoint saved at epoch: 440
[03/22 14:06:59] train_sl INFO: Epoch: 450 , Validation Loss: 0.018412249535322188 , Test Loss: 0.018800111301243306 , Training Loss: 0.016985400315696515 , Timing: 0.0033714771270751953
[03/22 14:06:59] train_sl INFO: Epoch: 456, SELCON dataloader subset selection finished, takes 0.0242. 
[03/22 14:06:59] train_sl INFO: Model checkpoint saved at epoch: 460
[03/22 14:06:59] train_sl INFO: Model checkpoint saved at epoch: 480
[03/22 14:07:00] train_sl INFO: Epoch: 491, SELCON dataloader subset selection finished, takes 0.0242. 
[03/22 14:07:00] train_sl INFO: Epoch: 500 , Validation Loss: 0.017613207641988992 , Test Loss: 0.01918212682940066 , Training Loss: 0.01836627899095989 , Timing: 0.0033969879150390625
[03/22 14:07:00] train_sl INFO: Model checkpoint saved at epoch: 500
[03/22 14:07:00] train_sl INFO: SELCON Selection Run---------------------------------
[03/22 14:07:00] train_sl INFO: Final SubsetTrn: 0.035443
[03/22 14:07:00] train_sl INFO: Validation Loss: 0.02
[03/22 14:07:00] train_sl INFO: Test Data Loss: 0.019182
[03/22 14:07:00] train_sl INFO: ---------------------------------------------------------------------
[03/22 14:07:00] train_sl INFO: SELCON
[03/22 14:07:00] train_sl INFO: ---------------------------------------------------------------------
[03/22 14:07:00] train_sl INFO: [0.0038928985595703125, 0.0034236907958984375, 0.003215789794921875, 0.003211498260498047, 0.0032453536987304688, 0.003393888473510742, 0.003436565399169922, 0.003434896469116211, 0.0036919116973876953, 0.003423929214477539, 0.0034346580505371094, 0.0034465789794921875, 0.0034399032592773438, 0.0034456253051757812, 0.003283262252807617, 0.003433704376220703, 0.0034356117248535156, 0.003445863723754883, 0.0034360885620117188, 0.003460407257080078, 0.004170656204223633, 0.003258228302001953, 0.0035948753356933594, 0.00341796875, 0.0034313201904296875, 0.003449678421020508, 0.003431558609008789, 0.0034456253051757812, 0.0034203529357910156, 0.003433704376220703, 0.003445863723754883, 0.003454446792602539, 0.00341033935546875, 0.003407001495361328, 0.0033829212188720703, 0.025669097900390625, 0.003124237060546875, 0.0029952526092529297, 0.0030035972595214844, 0.0029981136322021484, 0.003232240676879883, 0.003006458282470703, 0.002992391586303711, 0.0029816627502441406, 0.002988100051879883, 0.003047466278076172, 0.0029921531677246094, 0.003000497817993164, 0.002995729446411133, 0.002988100051879883, 0.003357410430908203, 0.0030868053436279297, 0.0034439563751220703, 0.003431081771850586, 0.0034360885620117188, 0.003402233123779297, 0.003215312957763672, 0.0032024383544921875, 0.0033860206604003906, 0.0033004283905029297, 0.0033156871795654297, 0.0033168792724609375, 0.003317117691040039, 0.003258228302001953, 0.003294229507446289, 0.0033562183380126953, 0.003330230712890625, 0.0033380985260009766, 0.003326416015625, 0.003349781036376953, 0.02824687957763672, 0.003421306610107422, 0.0032088756561279297, 0.0032269954681396484, 0.003409147262573242, 0.0034570693969726562, 0.0034148693084716797, 0.003433704376220703, 0.0033915042877197266, 0.003393888473510742, 0.003240346908569336, 0.0033915042877197266, 0.0034329891204833984, 0.003405332565307617, 0.003418445587158203, 0.0034067630767822266, 0.0034258365631103516, 0.0033910274505615234, 0.003401517868041992, 0.003216266632080078, 0.003414154052734375, 0.0034177303314208984, 0.0033941268920898438, 0.003437042236328125, 0.0033986568450927734, 0.003396272659301758, 0.003214120864868164, 0.0037386417388916016, 0.00341796875, 0.003408193588256836, 0.003354787826538086, 0.003346681594848633, 0.003397703170776367, 0.003437519073486328, 0.006294727325439453, 0.03594636917114258, 0.003116607666015625, 0.0030412673950195312, 0.003023386001586914, 0.003026723861694336, 0.002996206283569336, 0.003024578094482422, 0.0030052661895751953, 0.003010988235473633, 0.0029959678649902344, 0.0030112266540527344, 0.003003358840942383, 0.003002166748046875, 0.0029609203338623047, 0.0029726028442382812, 0.003228425979614258, 0.002920866012573242, 0.0029327869415283203, 0.0029735565185546875, 0.0029082298278808594, 0.0029265880584716797, 0.0029191970825195312, 0.0029239654541015625, 0.002918720245361328, 0.002915620803833008, 0.002970457077026367, 0.0029239654541015625, 0.00290679931640625, 0.002917051315307617, 0.002983570098876953, 0.0029172897338867188, 0.0029785633087158203, 0.002968311309814453, 0.0029299259185791016, 0.00298309326171875, 0.025122642517089844, 0.003030538558959961, 0.0030100345611572266, 0.0029866695404052734, 0.003015279769897461, 0.0029060840606689453, 0.002953052520751953, 0.0029680728912353516, 0.002902507781982422, 0.0029249191284179688, 0.0032236576080322266, 0.0030183792114257812, 0.003003358840942383, 0.003009796142578125, 0.002995014190673828, 0.003002166748046875, 0.002991914749145508, 0.003004312515258789, 0.002992868423461914, 0.0029828548431396484, 0.0032258033752441406, 0.003004312515258789, 0.002985239028930664, 0.0030074119567871094, 0.0029883384704589844, 0.0029981136322021484, 0.0029816627502441406, 0.0029897689819335938, 0.0029931068420410156, 0.002995014190673828, 0.0029757022857666016, 0.0029838085174560547, 0.0029997825622558594, 0.002994060516357422, 0.0029850006103515625, 0.02540302276611328, 0.0030236244201660156, 0.0029964447021484375, 0.0030095577239990234, 0.003000020980834961, 0.0032396316528320312, 0.0030028820037841797, 0.0029892921447753906, 0.0030493736267089844, 0.0029952526092529297, 0.0029850006103515625, 0.002980947494506836, 0.003047466278076172, 0.0029904842376708984, 0.0029947757720947266, 0.0029888153076171875, 0.0029947757720947266, 0.003000020980834961, 0.0029938220977783203, 0.0029878616333007812, 0.003002643585205078, 0.0029811859130859375, 0.002954244613647461, 0.002928495407104492, 0.0029916763305664062, 0.003194093704223633, 0.002933502197265625, 0.0029158592224121094, 0.0029611587524414062, 0.002907276153564453, 0.0028924942016601562, 0.0029697418212890625, 0.002926349639892578, 0.002996206283569336, 0.002949953079223633, 0.02492356300354004, 0.0029449462890625, 0.0029664039611816406, 0.002948284149169922, 0.0029900074005126953, 0.0029265880584716797, 0.002900838851928711, 0.0029449462890625, 0.002949237823486328, 0.0029077529907226562, 0.0031480789184570312, 0.002912759780883789, 0.0029449462890625, 0.0029144287109375, 0.002898693084716797, 0.002943277359008789, 0.0029115676879882812, 0.002916097640991211, 0.003008127212524414, 0.0029027462005615234, 0.0029408931732177734, 0.0029158592224121094, 0.0029554367065429688, 0.0029904842376708984, 0.0029289722442626953, 0.002904176712036133, 0.002888202667236328, 0.0029456615447998047, 0.0029997825622558594, 0.0029251575469970703, 0.0031747817993164062, 0.0029973983764648438, 0.0029616355895996094, 0.002949237823486328, 0.002980470657348633, 0.0249786376953125, 0.002910137176513672, 0.002923250198364258, 0.0030622482299804688, 0.0029573440551757812, 0.0032033920288085938, 0.002921581268310547, 0.00290679931640625, 0.0029077529907226562, 0.0029096603393554688, 0.0029091835021972656, 0.002959012985229492, 0.0029058456420898438, 0.002908468246459961, 0.0029191970825195312, 0.003237485885620117, 0.003049612045288086, 0.0029571056365966797, 0.002962350845336914, 0.0029768943786621094, 0.0029811859130859375, 0.003002166748046875, 0.002997875213623047, 0.0029370784759521484, 0.0029342174530029297, 0.0029659271240234375, 0.0030307769775390625, 0.002935171127319336, 0.0029494762420654297, 0.002970457077026367, 0.003015756607055664, 0.0029573440551757812, 0.0029287338256835938, 0.002968311309814453, 0.0029764175415039062, 0.025252103805541992, 0.0030002593994140625, 0.0030028820037841797, 0.002952098846435547, 0.0029401779174804688, 0.0030171871185302734, 0.002985239028930664, 0.0029184818267822266, 0.0029044151306152344, 0.0028972625732421875, 0.002943754196166992, 0.0029664039611816406, 0.0029222965240478516, 0.0029811859130859375, 0.002967357635498047, 0.0029458999633789062, 0.0029294490814208984, 0.002970457077026367, 0.00290679931640625, 0.0029413700103759766, 0.0032181739807128906, 0.0029671192169189453, 0.002923727035522461, 0.0029692649841308594, 0.0029070377349853516, 0.0030236244201660156, 0.00299835205078125, 0.0029845237731933594, 0.003016948699951172, 0.002992391586303711, 0.0029952526092529297, 0.0029783248901367188, 0.0029621124267578125, 0.002947568893432617, 0.002914905548095703, 0.024905681610107422, 0.002969026565551758, 0.0029985904693603516, 0.0030014514923095703, 0.0029506683349609375, 0.003188610076904297, 0.002927064895629883, 0.0029592514038085938, 0.002947568893432617, 0.002901792526245117, 0.0029273033142089844, 0.0029544830322265625, 0.002906322479248047, 0.002934694290161133, 0.0029685497283935547, 0.003016948699951172, 0.0029900074005126953, 0.0029859542846679688, 0.002989053726196289, 0.0029821395874023438, 0.002990245819091797, 0.002995729446411133, 0.0029900074005126953, 0.0029795169830322266, 0.0030145645141601562, 0.003233671188354492, 0.003000974655151367, 0.0029900074005126953, 0.0029745101928710938, 0.002993345260620117, 0.002994537353515625, 0.0029985904693603516, 0.0029914379119873047, 0.0030059814453125, 0.003004312515258789, 0.025449275970458984, 0.003009796142578125, 0.0029516220092773438, 0.002933025360107422, 0.0029654502868652344, 0.0029439926147460938, 0.002956390380859375, 0.002920389175415039, 0.0030062198638916016, 0.003005504608154297, 0.0032074451446533203, 0.002918720245361328, 0.0030066967010498047, 0.0029249191284179688, 0.002901315689086914, 0.003011465072631836, 0.002953052520751953, 0.0039033889770507812, 0.003002643585205078, 0.0030210018157958984, 0.003009796142578125, 0.003024578094482422, 0.0029273033142089844, 0.002964496612548828, 0.0029888153076171875, 0.002927541732788086, 0.002957582473754883, 0.0030014514923095703, 0.002979278564453125, 0.0029282569885253906, 0.0031969547271728516, 0.0029633045196533203, 0.0029871463775634766, 0.002960681915283203, 0.0029282569885253906, 0.025320053100585938, 0.0029761791229248047, 0.0029745101928710938, 0.003231048583984375, 0.0030183792114257812, 0.0032837390899658203, 0.0030088424682617188, 0.003011465072631836, 0.0030107498168945312, 0.005097627639770508, 0.00870060920715332, 0.007544755935668945, 0.00641632080078125, 0.0062634944915771484, 0.0051229000091552734, 0.0035216808319091797, 0.0034027099609375, 0.003351449966430664, 0.003422260284423828, 0.003422260284423828, 0.003644227981567383, 0.0034017562866210938, 0.00334930419921875, 0.0033974647521972656, 0.0034935474395751953, 0.0033838748931884766, 0.003340482711791992, 0.0034203529357910156, 0.003413677215576172, 0.0034363269805908203, 0.003584623336791992, 0.0033986568450927734, 0.003242015838623047, 0.003305673599243164, 0.0034837722778320312, 0.02802872657775879, 0.003247499465942383, 0.0034062862396240234, 0.0033893585205078125, 0.003377676010131836, 0.0031981468200683594, 0.0031888484954833984, 0.0032041072845458984, 0.0034079551696777344, 0.0034151077270507812, 0.003406524658203125, 0.0034089088439941406, 0.00337982177734375, 0.0034606456756591797, 0.0033855438232421875, 0.0033850669860839844, 0.003407001495361328, 0.0034084320068359375, 0.0034101009368896484, 0.0034127235412597656, 0.003443002700805664, 0.0033800601959228516, 0.003379344940185547, 0.0034019947052001953, 0.0034208297729492188, 0.003389596939086914, 0.0036754608154296875, 0.0034296512603759766, 0.003408193588256836, 0.0033714771270751953, 0.003427267074584961, 0.00341796875, 0.0033724308013916016, 0.003367185592651367, 0.0033729076385498047, 0.027904748916625977, 0.003389596939086914, 0.003419160842895508, 0.0034019947052001953, 0.003421783447265625, 0.00342559814453125, 0.003432035446166992, 0.0033690929412841797, 0.0033757686614990234, 0.003374814987182617, 0.0034058094024658203, 0.003413677215576172, 0.0033931732177734375, 0.0033860206604003906, 0.003412008285522461, 0.0033919811248779297, 0.003383159637451172, 0.003714323043823242, 0.003386974334716797, 0.004122734069824219, 0.0033767223358154297, 0.0033452510833740234, 0.00344085693359375, 0.0033702850341796875, 0.0033876895904541016, 0.003408193588256836, 0.0033636093139648438, 0.0034198760986328125, 0.0033905506134033203, 0.003437519073486328, 0.0034160614013671875, 0.0033829212188720703, 0.003370523452758789, 0.00342559814453125, 0.004033565521240234, 0.02785491943359375, 0.003407716751098633, 0.0034189224243164062, 0.003409862518310547, 0.0034127235412597656, 0.0034134387969970703, 0.0034241676330566406, 0.003412961959838867, 0.0034041404724121094, 0.0033969879150390625]
[03/22 14:07:00] train_sl INFO: Total time taken by SELCON = 1.9292 
Hyperparms:	fraction:0.01	select every:35	num epochs:500	delta:0.04
[03/22 14:07:00] train_sl INFO: DotMap(setting='SL', dataset=DotMap(name='LawSchool_selcon', datadir='../data', feature='dss', type='pre-defined'), dataloader=DotMap(shuffle=True, batch_size=100, pin_memory=False), model=DotMap(architecture='RegressionNet', type='pre-defined', input_dim=10, numclasses=10), ckpt=DotMap(is_load=False, is_save=True, dir='results/', save_every=20), loss=DotMap(type='MeanSquaredLoss', use_sigmoid=False), optimizer=DotMap(type='adam', lr=0.01), scheduler=DotMap(type='StepLR', step_size=1, gamma=0.1), dss_args=DotMap(type='SELCON', fraction=0.01, select_every=35, kappa=0, delta=0.04, linear_layer=False, lam=1e-05, batch_sampler='sequential', selection_type='Supervised', collate_fn=None, model=RegressionNet(
  (linear): Linear(in_features=10, out_features=1, bias=True)
), lr=0.01, loss=MSELoss(), device='cuda', optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.01
    weight_decay: 0
), criterion=MSELoss(), num_classes=10, batch_size=100, num_epochs=500), train_args=DotMap(num_epochs=500, device='cuda', print_every=50, results_dir='results/', print_args=['val_loss', 'tst_loss', 'trn_loss', 'time'], return_args=[]), dss_arg=DotMap(batch_sampler=DotMap()), is_reg=DotMap())
[03/22 14:07:00] train_sl INFO: SELCON: starting pre compute
[03/22 14:07:06] train_sl INFO: SELCON: Finishing F phi
[03/22 14:07:09] train_sl INFO: SELCON: Finishing element wise F
[03/22 14:07:09] train_sl INFO: Model checkpoint saved at epoch: 20
[03/22 14:07:09] train_sl INFO: Epoch: 36, SELCON dataloader subset selection finished, takes 0.0243. 
[03/22 14:07:09] train_sl INFO: Model checkpoint saved at epoch: 40
[03/22 14:07:09] train_sl INFO: Epoch: 50 , Validation Loss: 0.2886866830289364 , Test Loss: 0.2925582565367222 , Training Loss: 0.24548324999900964 , Timing: 0.0031862258911132812
[03/22 14:07:09] train_sl INFO: Model checkpoint saved at epoch: 60
[03/22 14:07:09] train_sl INFO: Epoch: 71, SELCON dataloader subset selection finished, takes 0.0239. 
[03/22 14:07:09] train_sl INFO: Model checkpoint saved at epoch: 80
[03/22 14:07:09] train_sl INFO: Epoch: 100 , Validation Loss: 0.020756501331925394 , Test Loss: 0.021905697137117385 , Training Loss: 0.01746219640615611 , Timing: 0.0033779144287109375
[03/22 14:07:09] train_sl INFO: Model checkpoint saved at epoch: 100
[03/22 14:07:09] train_sl INFO: Epoch: 106, SELCON dataloader subset selection finished, takes 0.0237. 
[03/22 14:07:09] train_sl INFO: Model checkpoint saved at epoch: 120
[03/22 14:07:09] train_sl INFO: Model checkpoint saved at epoch: 140
[03/22 14:07:09] train_sl INFO: Epoch: 141, SELCON dataloader subset selection finished, takes 0.0238. 
[03/22 14:07:10] train_sl INFO: Epoch: 150 , Validation Loss: 0.014398176595568657 , Test Loss: 0.014694199664518237 , Training Loss: 0.013618421028905477 , Timing: 0.003173351287841797
[03/22 14:07:10] train_sl INFO: Model checkpoint saved at epoch: 160
[03/22 14:07:10] train_sl INFO: Epoch: 176, SELCON dataloader subset selection finished, takes 0.0240. 
[03/22 14:07:10] train_sl INFO: Model checkpoint saved at epoch: 180
[03/22 14:07:10] train_sl INFO: Epoch: 200 , Validation Loss: 0.01471641450189054 , Test Loss: 0.014495649188756943 , Training Loss: 0.013199058330008904 , Timing: 0.006285190582275391
[03/22 14:07:10] train_sl INFO: Model checkpoint saved at epoch: 200
[03/22 14:07:10] train_sl INFO: Epoch: 211, SELCON dataloader subset selection finished, takes 0.0215. 
[03/22 14:07:10] train_sl INFO: Model checkpoint saved at epoch: 220
[03/22 14:07:10] train_sl INFO: Model checkpoint saved at epoch: 240
[03/22 14:07:10] train_sl INFO: Epoch: 246, SELCON dataloader subset selection finished, takes 0.0214. 
[03/22 14:07:10] train_sl INFO: Epoch: 250 , Validation Loss: 0.01526023275218904 , Test Loss: 0.014942726632580162 , Training Loss: 0.013654900611772273 , Timing: 0.0034346580505371094
[03/22 14:07:10] train_sl INFO: Model checkpoint saved at epoch: 260
[03/22 14:07:10] train_sl INFO: Model checkpoint saved at epoch: 280
[03/22 14:07:10] train_sl INFO: Epoch: 281, SELCON dataloader subset selection finished, takes 0.0239. 
[03/22 14:07:11] train_sl INFO: Epoch: 300 , Validation Loss: 0.013845029519870877 , Test Loss: 0.013776902854442597 , Training Loss: 0.013664358640268732 , Timing: 0.003142118453979492
[03/22 14:07:11] train_sl INFO: Model checkpoint saved at epoch: 300
[03/22 14:07:11] train_sl INFO: Epoch: 316, SELCON dataloader subset selection finished, takes 0.0239. 
[03/22 14:07:11] train_sl INFO: Model checkpoint saved at epoch: 320
[03/22 14:07:11] train_sl INFO: Model checkpoint saved at epoch: 340
[03/22 14:07:11] train_sl INFO: Epoch: 350 , Validation Loss: 0.020903201913461088 , Test Loss: 0.02027568775229156 , Training Loss: 0.017525033470099934 , Timing: 0.0033910274505615234
[03/22 14:07:11] train_sl INFO: Epoch: 351, SELCON dataloader subset selection finished, takes 0.0246. 
[03/22 14:07:11] train_sl INFO: Model checkpoint saved at epoch: 360
[03/22 14:07:11] train_sl INFO: Model checkpoint saved at epoch: 380
[03/22 14:07:11] train_sl INFO: Epoch: 386, SELCON dataloader subset selection finished, takes 0.0242. 
[03/22 14:07:11] train_sl INFO: Epoch: 400 , Validation Loss: 0.012794883036985994 , Test Loss: 0.012655164953321219 , Training Loss: 0.01193608607326706 , Timing: 0.0031881332397460938
[03/22 14:07:11] train_sl INFO: Model checkpoint saved at epoch: 400
[03/22 14:07:11] train_sl INFO: Model checkpoint saved at epoch: 420
[03/22 14:07:11] train_sl INFO: Epoch: 421, SELCON dataloader subset selection finished, takes 0.0240. 
[03/22 14:07:11] train_sl INFO: Model checkpoint saved at epoch: 440
[03/22 14:07:12] train_sl INFO: Epoch: 450 , Validation Loss: 0.013708288595080376 , Test Loss: 0.013399466592818498 , Training Loss: 0.012157983048102604 , Timing: 0.0034317970275878906
[03/22 14:07:12] train_sl INFO: Epoch: 456, SELCON dataloader subset selection finished, takes 0.0240. 
[03/22 14:07:12] train_sl INFO: Model checkpoint saved at epoch: 460
[03/22 14:07:12] train_sl INFO: Model checkpoint saved at epoch: 480
[03/22 14:07:12] train_sl INFO: Epoch: 491, SELCON dataloader subset selection finished, takes 0.0247. 
[03/22 14:07:12] train_sl INFO: Epoch: 500 , Validation Loss: 0.012637326307594777 , Test Loss: 0.012422546977177262 , Training Loss: 0.011826362080263117 , Timing: 0.003361940383911133
[03/22 14:07:12] train_sl INFO: Model checkpoint saved at epoch: 500
[03/22 14:07:12] train_sl INFO: SELCON Selection Run---------------------------------
[03/22 14:07:12] train_sl INFO: Final SubsetTrn: 0.013946
[03/22 14:07:12] train_sl INFO: Validation Loss: 0.01
[03/22 14:07:12] train_sl INFO: Test Data Loss: 0.012423
[03/22 14:07:12] train_sl INFO: ---------------------------------------------------------------------
[03/22 14:07:12] train_sl INFO: SELCON
[03/22 14:07:12] train_sl INFO: ---------------------------------------------------------------------
[03/22 14:07:12] train_sl INFO: [0.0039196014404296875, 0.0036573410034179688, 0.0037310123443603516, 0.0036177635192871094, 0.003842592239379883, 0.0038177967071533203, 0.0038139820098876953, 0.0038356781005859375, 0.0038416385650634766, 0.003827810287475586, 0.003803253173828125, 0.003841876983642578, 0.003846406936645508, 0.003839254379272461, 0.0038194656372070312, 0.0038254261016845703, 0.003656148910522461, 0.0036716461181640625, 0.0036792755126953125, 0.003667116165161133, 0.004101276397705078, 0.003668546676635742, 0.0036602020263671875, 0.0036554336547851562, 0.0036590099334716797, 0.0038111209869384766, 0.003807544708251953, 0.003818988800048828, 0.0038275718688964844, 0.004288673400878906, 0.003824949264526367, 0.0038175582885742188, 0.00382232666015625, 0.0038111209869384766, 0.00380706787109375, 0.027982473373413086, 0.003397226333618164, 0.0033872127532958984, 0.0034286975860595703, 0.004029273986816406, 0.003394603729248047, 0.003714323043823242, 0.0033881664276123047, 0.0033609867095947266, 0.0032460689544677734, 0.003404378890991211, 0.0033943653106689453, 0.0032494068145751953, 0.0031669139862060547, 0.0031862258911132812, 0.003490924835205078, 0.0033905506134033203, 0.0034198760986328125, 0.003380298614501953, 0.003376007080078125, 0.0031986236572265625, 0.003398418426513672, 0.003423452377319336, 0.0034148693084716797, 0.0034034252166748047, 0.0034019947052001953, 0.0033490657806396484, 0.0034151077270507812, 0.003177165985107422, 0.0033502578735351562, 0.003401041030883789, 0.0033767223358154297, 0.003373861312866211, 0.0034165382385253906, 0.0034003257751464844, 0.027464866638183594, 0.0033686161041259766, 0.003349781036376953, 0.003408193588256836, 0.003384828567504883, 0.0034248828887939453, 0.003398418426513672, 0.003345012664794922, 0.003358602523803711, 0.0034027099609375, 0.0034127235412597656, 0.003374338150024414, 0.0034096240997314453, 0.003384828567504883, 0.0033915042877197266, 0.0034024715423583984, 0.0033550262451171875, 0.0033512115478515625, 0.0033521652221679688, 0.0033524036407470703, 0.0034580230712890625, 0.003413677215576172, 0.0034155845642089844, 0.0034584999084472656, 0.003390789031982422, 0.0033996105194091797, 0.0034084320068359375, 0.0033829212188720703, 0.003381967544555664, 0.0033779144287109375, 0.0034494400024414062, 0.003376007080078125, 0.003337860107421875, 0.0033736228942871094, 0.0034284591674804688, 0.028017282485961914, 0.003426074981689453, 0.003434419631958008, 0.0033783912658691406, 0.003435373306274414, 0.003366231918334961, 0.0034227371215820312, 0.003392457962036133, 0.003342151641845703, 0.0033559799194335938, 0.0033559799194335938, 0.003451824188232422, 0.0033354759216308594, 0.0033903121948242188, 0.0033948421478271484, 0.0033965110778808594, 0.0033605098724365234, 0.003385782241821289, 0.003393411636352539, 0.003365039825439453, 0.0034101009368896484, 0.003205537796020508, 0.003401041030883789, 0.003371000289916992, 0.0033600330352783203, 0.003344297409057617, 0.0033855438232421875, 0.0036673545837402344, 0.003335714340209961, 0.0033686161041259766, 0.0034189224243164062, 0.0033876895904541016, 0.0034236907958984375, 0.0033774375915527344, 0.003172159194946289, 0.02747631072998047, 0.003339052200317383, 0.003381490707397461, 0.0033376216888427734, 0.003423452377319336, 0.0033910274505615234, 0.003389596939086914, 0.0033898353576660156, 0.003203868865966797, 0.003173351287841797, 0.0034170150756835938, 0.003393411636352539, 0.0034096240997314453, 0.0032427310943603516, 0.0034029483795166016, 0.0034012794494628906, 0.0033991336822509766, 0.003391265869140625, 0.003394603729248047, 0.003408670425415039, 0.0033845901489257812, 0.003396749496459961, 0.0034143924713134766, 0.003414154052734375, 0.003408670425415039, 0.003407001495361328, 0.003395557403564453, 0.0033986568450927734, 0.0034224987030029297, 0.0034170150756835938, 0.003402233123779297, 0.0034041404724121094, 0.0034177303314208984, 0.0034253597259521484, 0.003384828567504883, 0.027699708938598633, 0.003398895263671875, 0.003405332565307617, 0.003415822982788086, 0.0033941268920898438, 0.0034332275390625, 0.0033876895904541016, 0.0033757686614990234, 0.003371715545654297, 0.0034291744232177734, 0.004010915756225586, 0.003428220748901367, 0.003365039825439453, 0.0033538341522216797, 0.003359556198120117, 0.003358602523803711, 0.0033562183380126953, 0.0033960342407226562, 0.003401517868041992, 0.0033600330352783203, 0.003400564193725586, 0.003352642059326172, 0.005724430084228516, 0.0074841976165771484, 0.006285190582275391, 0.0031900405883789062, 0.002948284149169922, 0.003026247024536133, 0.0029535293579101562, 0.0029256343841552734, 0.002972126007080078, 0.0029871463775634766, 0.0029096603393554688, 0.0029740333557128906, 0.002981424331665039, 0.02490711212158203, 0.0029833316802978516, 0.002922534942626953, 0.0029382705688476562, 0.0029344558715820312, 0.0029103755950927734, 0.0029120445251464844, 0.0029609203338623047, 0.0029592514038085938, 0.002967357635498047, 0.0031850337982177734, 0.0029582977294921875, 0.002935647964477539, 0.0030078887939453125, 0.0030286312103271484, 0.002938985824584961, 0.002925872802734375, 0.00299835205078125, 0.0029146671295166016, 0.002940654754638672, 0.0029473304748535156, 0.002937793731689453, 0.002975940704345703, 0.0029985904693603516, 0.0028994083404541016, 0.0029213428497314453, 0.0029125213623046875, 0.0029447078704833984, 0.002938985824584961, 0.0029137134552001953, 0.003220081329345703, 0.0030083656311035156, 0.0029582977294921875, 0.0030007362365722656, 0.002920866012573242, 0.025033950805664062, 0.003707408905029297, 0.003330230712890625, 0.003394603729248047, 0.0034346580505371094, 0.003484487533569336, 0.0034227371215820312, 0.003355741500854492, 0.0033850669860839844, 0.003350973129272461, 0.003346681594848633, 0.003340005874633789, 0.0033495426177978516, 0.0034160614013671875, 0.003353118896484375, 0.0034215450286865234, 0.003346681594848633, 0.003367900848388672, 0.0033719539642333984, 0.003366708755493164, 0.003420591354370117, 0.0033533573150634766, 0.003431081771850586, 0.003385782241821289, 0.0033707618713378906, 0.0033943653106689453, 0.003386259078979492, 0.0033812522888183594, 0.003377199172973633, 0.0034050941467285156, 0.0033876895904541016, 0.0033829212188720703, 0.0033721923828125, 0.0033905506134033203, 0.0033750534057617188, 0.02759861946105957, 0.0037631988525390625, 0.0033485889434814453, 0.0033593177795410156, 0.003391742706298828, 0.0033986568450927734, 0.0033483505249023438, 0.003376483917236328, 0.003375530242919922, 0.0034246444702148438, 0.003414154052734375, 0.0033998489379882812, 0.003385305404663086, 0.003444671630859375, 0.003371715545654297, 0.0033740997314453125, 0.003434419631958008, 0.0031821727752685547, 0.003141164779663086, 0.003142118453979492, 0.003439664840698242, 0.003407716751098633, 0.0034132003784179688, 0.0034110546112060547, 0.003409147262573242, 0.003394603729248047, 0.0034203529357910156, 0.003397226333618164, 0.003417491912841797, 0.003425121307373047, 0.0034041404724121094, 0.0034058094024658203, 0.003457784652709961, 0.0033931732177734375, 0.0034008026123046875, 0.027576446533203125, 0.0034537315368652344, 0.0034208297729492188, 0.003409862518310547, 0.003415822982788086, 0.003411531448364258, 0.003406047821044922, 0.0034034252166748047, 0.0034058094024658203, 0.003420114517211914, 0.0033986568450927734, 0.0034248828887939453, 0.003410816192626953, 0.003407001495361328, 0.0033931732177734375, 0.0034019947052001953, 0.0034089088439941406, 0.0034208297729492188, 0.003420591354370117, 0.0034148693084716797, 0.003416299819946289, 0.0033957958221435547, 0.0033965110778808594, 0.003403186798095703, 0.0034036636352539062, 0.0034096240997314453, 0.0034253597259521484, 0.0033922195434570312, 0.0033974647521972656, 0.0034024715423583984, 0.0032052993774414062, 0.003358602523803711, 0.0033867359161376953, 0.003414154052734375, 0.0033910274505615234, 0.02853679656982422, 0.0034437179565429688, 0.003398895263671875, 0.0031890869140625, 0.0031800270080566406, 0.003173351287841797, 0.0032570362091064453, 0.003363370895385742, 0.003362894058227539, 0.0032033920288085938, 0.003228425979614258, 0.0031888484954833984, 0.0033941268920898438, 0.003390073776245117, 0.003374338150024414, 0.0034368038177490234, 0.003413677215576172, 0.003259897232055664, 0.003401041030883789, 0.0033965110778808594, 0.0034132003784179688, 0.0034203529357910156, 0.0034117698669433594, 0.0033550262451171875, 0.0033364295959472656, 0.004024505615234375, 0.003343343734741211, 0.003384828567504883, 0.0034089088439941406, 0.0033860206604003906, 0.0034284591674804688, 0.003427267074584961, 0.0034003257751464844, 0.0034062862396240234, 0.0033905506134033203, 0.027805566787719727, 0.0034203529357910156, 0.0034303665161132812, 0.003419637680053711, 0.0034661293029785156, 0.003419637680053711, 0.003541707992553711, 0.003405332565307617, 0.003438711166381836, 0.003252267837524414, 0.0034363269805908203, 0.00321197509765625, 0.003206968307495117, 0.0031595230102539062, 0.0031881332397460938, 0.0038285255432128906, 0.003440380096435547, 0.003421306610107422, 0.003414630889892578, 0.003414154052734375, 0.0034177303314208984, 0.0033860206604003906, 0.0034003257751464844, 0.0034134387969970703, 0.003407716751098633, 0.003415822982788086, 0.0034317970275878906, 0.0034339427947998047, 0.003392934799194336, 0.0034003257751464844, 0.003373384475708008, 0.0033614635467529297, 0.0033779144287109375, 0.0033986568450927734, 0.00334930419921875, 0.027676820755004883, 0.0033600330352783203, 0.0033469200134277344, 0.003359079360961914, 0.0037119388580322266, 0.0033555030822753906, 0.0033893585205078125, 0.003351926803588867, 0.0033559799194335938, 0.0033426284790039062, 0.00337982177734375, 0.0033371448516845703, 0.003355264663696289, 0.003401041030883789, 0.003419160842895508, 0.0034151077270507812, 0.003397226333618164, 0.0033783912658691406, 0.004127025604248047, 0.0033807754516601562, 0.003367185592651367, 0.0033473968505859375, 0.003403902053833008, 0.0037293434143066406, 0.003410816192626953, 0.0033855438232421875, 0.0034074783325195312, 0.0034246444702148438, 0.0033762454986572266, 0.0034317970275878906, 0.0034401416778564453, 0.003415822982788086, 0.0034170150756835938, 0.0034258365631103516, 0.0034155845642089844, 0.02759528160095215, 0.0034241676330566406, 0.0034084320068359375, 0.0034449100494384766, 0.0034029483795166016, 0.003412008285522461, 0.0034019947052001953, 0.0034096240997314453, 0.003397703170776367, 0.003412961959838867, 0.003420591354370117, 0.0033867359161376953, 0.003368377685546875, 0.0033385753631591797, 0.0033731460571289062, 0.0034024715423583984, 0.003392934799194336, 0.003305196762084961, 0.003414630889892578, 0.00342559814453125, 0.003420591354370117, 0.003247499465942383, 0.0034384727478027344, 0.0034530162811279297, 0.003426790237426758, 0.003283262252807617, 0.0032570362091064453, 0.003215789794921875, 0.0032694339752197266, 0.0032148361206054688, 0.0032918453216552734, 0.003213644027709961, 0.0034346580505371094, 0.012763023376464844, 0.004288911819458008, 0.028307437896728516, 0.003278970718383789, 0.003204345703125, 0.003313779830932617, 0.003176450729370117, 0.0036067962646484375, 0.00333404541015625, 0.003452301025390625, 0.003379344940185547, 0.003361940383911133]
[03/22 14:07:12] train_sl INFO: Total time taken by SELCON = 2.0469 
Hyperparms:	fraction:0.01	select every:35	num epochs:500	delta:0.1
[03/22 14:07:12] train_sl INFO: DotMap(setting='SL', dataset=DotMap(name='LawSchool_selcon', datadir='../data', feature='dss', type='pre-defined'), dataloader=DotMap(shuffle=True, batch_size=100, pin_memory=False), model=DotMap(architecture='RegressionNet', type='pre-defined', input_dim=10, numclasses=10), ckpt=DotMap(is_load=False, is_save=True, dir='results/', save_every=20), loss=DotMap(type='MeanSquaredLoss', use_sigmoid=False), optimizer=DotMap(type='adam', lr=0.01), scheduler=DotMap(type='StepLR', step_size=1, gamma=0.1), dss_args=DotMap(type='SELCON', fraction=0.01, select_every=35, kappa=0, delta=0.1, linear_layer=False, lam=1e-05, batch_sampler='sequential', selection_type='Supervised', collate_fn=None, model=RegressionNet(
  (linear): Linear(in_features=10, out_features=1, bias=True)
), lr=0.01, loss=MSELoss(), device='cuda', optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.01
    weight_decay: 0
), criterion=MSELoss(), num_classes=10, batch_size=100, num_epochs=500), train_args=DotMap(num_epochs=500, device='cuda', print_every=50, results_dir='results/', print_args=['val_loss', 'tst_loss', 'trn_loss', 'time'], return_args=[]), dss_arg=DotMap(batch_sampler=DotMap()), is_reg=DotMap())
[03/22 14:07:12] train_sl INFO: SELCON: starting pre compute
[03/22 14:07:12] train_sl INFO: SELCON: Finishing F phi
[03/22 14:07:15] train_sl INFO: SELCON: Finishing element wise F
[03/22 14:07:15] train_sl INFO: Model checkpoint saved at epoch: 20
[03/22 14:07:15] train_sl INFO: Epoch: 36, SELCON dataloader subset selection finished, takes 0.0242. 
[03/22 14:07:15] train_sl INFO: Model checkpoint saved at epoch: 40
[03/22 14:07:15] train_sl INFO: Epoch: 50 , Validation Loss: 7.506666195392609 , Test Loss: 7.483156859874725 , Training Loss: 4.639689380159745 , Timing: 0.0054242610931396484
[03/22 14:07:15] train_sl INFO: Model checkpoint saved at epoch: 60
[03/22 14:07:15] train_sl INFO: Epoch: 71, SELCON dataloader subset selection finished, takes 0.0217. 
[03/22 14:07:15] train_sl INFO: Model checkpoint saved at epoch: 80
[03/22 14:07:15] train_sl INFO: Epoch: 100 , Validation Loss: 8.237860536575317 , Test Loss: 8.19261293411255 , Training Loss: 5.0461956193813915 , Timing: 0.0031571388244628906
[03/22 14:07:15] train_sl INFO: Model checkpoint saved at epoch: 100
[03/22 14:07:15] train_sl INFO: Epoch: 106, SELCON dataloader subset selection finished, takes 0.0217. 
[03/22 14:07:15] train_sl INFO: Model checkpoint saved at epoch: 120
[03/22 14:07:15] train_sl INFO: Model checkpoint saved at epoch: 140
[03/22 14:07:15] train_sl INFO: Epoch: 141, SELCON dataloader subset selection finished, takes 0.0213. 
[03/22 14:07:16] train_sl INFO: Epoch: 150 , Validation Loss: 6.787944173812866 , Test Loss: 6.76200532913208 , Training Loss: 4.0404571283322115 , Timing: 0.0029027462005615234
[03/22 14:07:16] train_sl INFO: Model checkpoint saved at epoch: 160
[03/22 14:07:16] train_sl INFO: Epoch: 176, SELCON dataloader subset selection finished, takes 0.0214. 
[03/22 14:07:16] train_sl INFO: Model checkpoint saved at epoch: 180
[03/22 14:07:16] train_sl INFO: Epoch: 200 , Validation Loss: 5.775739085674286 , Test Loss: 5.764533346891403 , Training Loss: 3.383424232785518 , Timing: 0.0029141902923583984
[03/22 14:07:16] train_sl INFO: Model checkpoint saved at epoch: 200
[03/22 14:07:16] train_sl INFO: Epoch: 211, SELCON dataloader subset selection finished, takes 0.0215. 
[03/22 14:07:16] train_sl INFO: Model checkpoint saved at epoch: 220
[03/22 14:07:16] train_sl INFO: Model checkpoint saved at epoch: 240
[03/22 14:07:16] train_sl INFO: Epoch: 246, SELCON dataloader subset selection finished, takes 0.0217. 
[03/22 14:07:16] train_sl INFO: Epoch: 250 , Validation Loss: 4.82334001660347 , Test Loss: 4.817980718612671 , Training Loss: 2.7344844604914007 , Timing: 0.0029134750366210938
[03/22 14:07:16] train_sl INFO: Model checkpoint saved at epoch: 260
[03/22 14:07:16] train_sl INFO: Model checkpoint saved at epoch: 280
[03/22 14:07:16] train_sl INFO: Epoch: 281, SELCON dataloader subset selection finished, takes 0.0239. 
[03/22 14:07:17] train_sl INFO: Epoch: 300 , Validation Loss: 3.998022109270096 , Test Loss: 3.9986867785453795 , Training Loss: 2.2040021241857457 , Timing: 0.0035860538482666016
[03/22 14:07:17] train_sl INFO: Model checkpoint saved at epoch: 300
[03/22 14:07:17] train_sl INFO: Epoch: 316, SELCON dataloader subset selection finished, takes 0.0245. 
[03/22 14:07:17] train_sl INFO: Model checkpoint saved at epoch: 320
[03/22 14:07:17] train_sl INFO: Model checkpoint saved at epoch: 340
[03/22 14:07:17] train_sl INFO: Epoch: 350 , Validation Loss: 3.4149847477674484 , Test Loss: 3.417185905575752 , Training Loss: 1.8398662604964697 , Timing: 0.0032758712768554688
[03/22 14:07:17] train_sl INFO: Epoch: 351, SELCON dataloader subset selection finished, takes 0.0230. 
[03/22 14:07:17] train_sl INFO: Model checkpoint saved at epoch: 360
[03/22 14:07:17] train_sl INFO: Model checkpoint saved at epoch: 380
[03/22 14:07:17] train_sl INFO: Epoch: 386, SELCON dataloader subset selection finished, takes 0.0260. 
[03/22 14:07:17] train_sl INFO: Epoch: 400 , Validation Loss: 3.158989343047142 , Test Loss: 3.162241521477699 , Training Loss: 1.7033840784659753 , Timing: 0.00858759880065918
[03/22 14:07:17] train_sl INFO: Model checkpoint saved at epoch: 400
[03/22 14:07:17] train_sl INFO: Model checkpoint saved at epoch: 420
[03/22 14:07:17] train_sl INFO: Epoch: 421, SELCON dataloader subset selection finished, takes 0.0280. 
[03/22 14:07:17] train_sl INFO: Model checkpoint saved at epoch: 440
[03/22 14:07:18] train_sl INFO: Epoch: 450 , Validation Loss: 2.5931643307209016 , Test Loss: 2.594738706946373 , Training Loss: 1.3366285181389406 , Timing: 0.003467082977294922
[03/22 14:07:18] train_sl INFO: Epoch: 456, SELCON dataloader subset selection finished, takes 0.0248. 
[03/22 14:07:18] train_sl INFO: Model checkpoint saved at epoch: 460
[03/22 14:07:18] train_sl INFO: Model checkpoint saved at epoch: 480
[03/22 14:07:18] train_sl INFO: Epoch: 491, SELCON dataloader subset selection finished, takes 0.0247. 
[03/22 14:07:18] train_sl INFO: Epoch: 500 , Validation Loss: 2.2856685131788255 , Test Loss: 2.2896815478801726 , Training Loss: 1.1737554339835277 , Timing: 0.0032088756561279297
[03/22 14:07:18] train_sl INFO: Model checkpoint saved at epoch: 500
[03/22 14:07:18] train_sl INFO: SELCON Selection Run---------------------------------
[03/22 14:07:18] train_sl INFO: Final SubsetTrn: 0.050450
[03/22 14:07:18] train_sl INFO: Validation Loss: 2.29
[03/22 14:07:18] train_sl INFO: Test Data Loss: 2.289682
[03/22 14:07:18] train_sl INFO: ---------------------------------------------------------------------
[03/22 14:07:18] train_sl INFO: SELCON
[03/22 14:07:18] train_sl INFO: ---------------------------------------------------------------------
[03/22 14:07:18] train_sl INFO: [0.003981113433837891, 0.003712177276611328, 0.0038352012634277344, 0.003797292709350586, 0.0038118362426757812, 0.0037975311279296875, 0.0037832260131835938, 0.0037953853607177734, 0.003793954849243164, 0.003815889358520508, 0.0038285255432128906, 0.0037834644317626953, 0.0037832260131835938, 0.0037734508514404297, 0.0038306713104248047, 0.0037953853607177734, 0.003688335418701172, 0.0043985843658447266, 0.0038118362426757812, 0.0037851333618164062, 0.0038242340087890625, 0.003792285919189453, 0.0037794113159179688, 0.0036945343017578125, 0.0038406848907470703, 0.003797769546508789, 0.003793478012084961, 0.0037183761596679688, 0.0036230087280273438, 0.003787994384765625, 0.0038080215454101562, 0.003794431686401367, 0.0038444995880126953, 0.003706216812133789, 0.003804445266723633, 0.027892589569091797, 0.0032563209533691406, 0.0034074783325195312, 0.0033943653106689453, 0.003374338150024414, 0.0036072731018066406, 0.0034189224243164062, 0.003421306610107422, 0.003228902816772461, 0.005551338195800781, 0.00869894027709961, 0.007977485656738281, 0.007108211517333984, 0.005795001983642578, 0.0054242610931396484, 0.003228425979614258, 0.0029850006103515625, 0.002999544143676758, 0.0029947757720947266, 0.003009319305419922, 0.0029871463775634766, 0.002987384796142578, 0.002960681915283203, 0.002911806106567383, 0.0029077529907226562, 0.003132343292236328, 0.0029850006103515625, 0.0029382705688476562, 0.00293731689453125, 0.0029413700103759766, 0.002899646759033203, 0.003009796142578125, 0.002933502197265625, 0.0029144287109375, 0.0029571056365966797, 0.024821043014526367, 0.002912759780883789, 0.002930879592895508, 0.0029549598693847656, 0.003080606460571289, 0.0030219554901123047, 0.0029494762420654297, 0.002919912338256836, 0.0029816627502441406, 0.002949953079223633, 0.0032281875610351562, 0.0029494762420654297, 0.002959728240966797, 0.0029408931732177734, 0.002961874008178711, 0.00289154052734375, 0.0029954910278320312, 0.0029878616333007812, 0.0029859542846679688, 0.0029058456420898438, 0.0029020309448242188, 0.002892017364501953, 0.002923250198364258, 0.0029480457305908203, 0.002980470657348633, 0.0029463768005371094, 0.002939462661743164, 0.0029120445251464844, 0.0029458999633789062, 0.0031571388244628906, 0.003322601318359375, 0.0029032230377197266, 0.002943277359008789, 0.002992868423461914, 0.002982616424560547, 0.02500295639038086, 0.003045320510864258, 0.002923727035522461, 0.0029675960540771484, 0.002952098846435547, 0.002916097640991211, 0.002952098846435547, 0.0029752254486083984, 0.002954244613647461, 0.0029294490814208984, 0.0029349327087402344, 0.0029730796813964844, 0.0030493736267089844, 0.0029401779174804688, 0.0029363632202148438, 0.00292205810546875, 0.002971172332763672, 0.0030155181884765625, 0.0029218196868896484, 0.0029633045196533203, 0.002953052520751953, 0.0029935836791992188, 0.002961397171020508, 0.0029566287994384766, 0.0029692649841308594, 0.002961874008178711, 0.0029234886169433594, 0.002975940704345703, 0.002953052520751953, 0.0029790401458740234, 0.0029306411743164062, 0.0029592514038085938, 0.0029604434967041016, 0.003008127212524414, 0.002901792526245117, 0.024720191955566406, 0.0029098987579345703, 0.0028901100158691406, 0.003061056137084961, 0.0029840469360351562, 0.002912282943725586, 0.0028994083404541016, 0.00290679931640625, 0.0029144287109375, 0.0029027462005615234, 0.0031588077545166016, 0.0029144287109375, 0.0030024051666259766, 0.002987384796142578, 0.002942800521850586, 0.0029594898223876953, 0.002985715866088867, 0.002992391586303711, 0.0029358863830566406, 0.003023862838745117, 0.003198385238647461, 0.002985715866088867, 0.0029706954956054688, 0.0029349327087402344, 0.002998828887939453, 0.002925395965576172, 0.002975940704345703, 0.0030519962310791016, 0.002948284149169922, 0.002885103225708008, 0.0029680728912353516, 0.0029611587524414062, 0.0030167102813720703, 0.002955198287963867, 0.0029485225677490234, 0.024808168411254883, 0.002980470657348633, 0.0029554367065429688, 0.0029723644256591797, 0.002980947494506836, 0.0031795501708984375, 0.0029697418212890625, 0.0031554698944091797, 0.0029222965240478516, 0.003050088882446289, 0.003008127212524414, 0.002950429916381836, 0.002899646759033203, 0.002891063690185547, 0.002956867218017578, 0.0029020309448242188, 0.0028994083404541016, 0.002963542938232422, 0.002919912338256836, 0.0029087066650390625, 0.002941131591796875, 0.002977132797241211, 0.0029010772705078125, 0.003014087677001953, 0.0029141902923583984, 0.003162384033203125, 0.0028998851776123047, 0.002888917922973633, 0.00296783447265625, 0.002907991409301758, 0.0028922557830810547, 0.0029709339141845703, 0.002923727035522461, 0.002971172332763672, 0.0029134750366210938, 0.024870872497558594, 0.002897024154663086, 0.0029048919677734375, 0.0029952526092529297, 0.002943754196166992, 0.0029294490814208984, 0.0029010772705078125, 0.0028967857360839844, 0.0029251575469970703, 0.0029273033142089844, 0.0031828880310058594, 0.0029578208923339844, 0.0029904842376708984, 0.0029027462005615234, 0.002890348434448242, 0.0029680728912353516, 0.0029168128967285156, 0.0029425621032714844, 0.002937793731689453, 0.0029103755950927734, 0.0029451847076416016, 0.0029342174530029297, 0.0030608177185058594, 0.0030517578125, 0.0029709339141845703, 0.0029370784759521484, 0.0029740333557128906, 0.0030188560485839844, 0.0029458999633789062, 0.002924203872680664, 0.0032165050506591797, 0.003010272979736328, 0.0029647350311279297, 0.0029528141021728516, 0.002913236618041992, 0.024883747100830078, 0.0029103755950927734, 0.002957582473754883, 0.002910614013671875, 0.0029134750366210938, 0.003477811813354492, 0.0034101009368896484, 0.0034313201904296875, 0.0033745765686035156, 0.0033578872680664062, 0.0033807754516601562, 0.0033898353576660156, 0.0033931732177734375, 0.003400564193725586, 0.0034058094024658203, 0.0034139156341552734, 0.0032205581665039062, 0.0033931732177734375, 0.0034189224243164062, 0.0034084320068359375, 0.0031805038452148438, 0.003186464309692383, 0.0031843185424804688, 0.0031752586364746094, 0.003180980682373047, 0.003164529800415039, 0.0034072399139404297, 0.0034198760986328125, 0.0032651424407958984, 0.0034122467041015625, 0.0034105777740478516, 0.003417491912841797, 0.0033974647521972656, 0.0034058094024658203, 0.003399372100830078, 0.027562379837036133, 0.003403902053833008, 0.003411531448364258, 0.0034475326538085938, 0.0034117698669433594, 0.0034036636352539062, 0.003182649612426758, 0.003180980682373047, 0.0034089088439941406, 0.0034139156341552734, 0.0032100677490234375, 0.0034143924713134766, 0.0033974647521972656, 0.003402233123779297, 0.0034003257751464844, 0.0032525062561035156, 0.0037097930908203125, 0.003595590591430664, 0.0042572021484375, 0.0035860538482666016, 0.005471706390380859, 0.0037012100219726562, 0.0033164024353027344, 0.0032715797424316406, 0.0032570362091064453, 0.0033648014068603516, 0.003698587417602539, 0.003404378890991211, 0.0043942928314208984, 0.0034177303314208984, 0.0034856796264648438, 0.003689289093017578, 0.003778696060180664, 0.0034821033477783203, 0.0034132003784179688, 0.02817392349243164, 0.0034074783325195312, 0.0033752918243408203, 0.003385782241821289, 0.0033855438232421875, 0.0034329891204833984, 0.0034284591674804688, 0.0034728050231933594, 0.0034232139587402344, 0.0034279823303222656, 0.0034494400024414062, 0.0034406185150146484, 0.0034492015838623047, 0.0034208297729492188, 0.0034322738647460938, 0.0034723281860351562, 0.003401041030883789, 0.003464937210083008, 0.0034961700439453125, 0.0033881664276123047, 0.0034940242767333984, 0.003457784652709961, 0.003218412399291992, 0.0033211708068847656, 0.0032100677490234375, 0.003328084945678711, 0.0035359859466552734, 0.003235340118408203, 0.0052754878997802734, 0.0030388832092285156, 0.0030050277709960938, 0.002997159957885742, 0.003013134002685547, 0.0030167102813720703, 0.0032758712768554688, 0.02752542495727539, 0.004597663879394531, 0.004958152770996094, 0.0046198368072509766, 0.0034792423248291016, 0.003542661666870117, 0.0034780502319335938, 0.003546476364135742, 0.0032787322998046875, 0.0032248497009277344, 0.003275156021118164, 0.003256559371948242, 0.003300905227661133, 0.0033006668090820312, 0.0036003589630126953, 0.0034987926483154297, 0.009576797485351562, 0.009504556655883789, 0.005967617034912109, 0.005049467086791992, 0.004430294036865234, 0.004044294357299805, 0.003557443618774414, 0.0035469532012939453, 0.0033359527587890625, 0.0030715465545654297, 0.0032205581665039062, 0.0033419132232666016, 0.003390789031982422, 0.0032606124877929688, 0.004096508026123047, 0.0033500194549560547, 0.0033702850341796875, 0.003299713134765625, 0.0032434463500976562, 0.02999114990234375, 0.003262758255004883, 0.0033020973205566406, 0.00337982177734375, 0.0032329559326171875, 0.0032160282135009766, 0.0032339096069335938, 0.0034825801849365234, 0.003312349319458008, 0.0036122798919677734, 0.0033483505249023438, 0.00413060188293457, 0.003365755081176758, 0.005253791809082031, 0.00858759880065918, 0.0038907527923583984, 0.003294229507446289, 0.003278970718383789, 0.00327301025390625, 0.0034847259521484375, 0.0034532546997070312, 0.0032274723052978516, 0.0034363269805908203, 0.0034270286560058594, 0.004283905029296875, 0.003299236297607422, 0.003258943557739258, 0.003446340560913086, 0.003253459930419922, 0.003231525421142578, 0.0032410621643066406, 0.003215312957763672, 0.003240346908569336, 0.003336668014526367, 0.003551483154296875, 0.03198957443237305, 0.0033512115478515625, 0.0033049583435058594, 0.0032644271850585938, 0.0032770633697509766, 0.00325775146484375, 0.003314971923828125, 0.0034956932067871094, 0.003396749496459961, 0.0035982131958007812, 0.003281116485595703, 0.0034360885620117188, 0.00331878662109375, 0.0032608509063720703, 0.003489255905151367, 0.003265857696533203, 0.0032448768615722656, 0.003262758255004883, 0.0032427310943603516, 0.0032579898834228516, 0.0032753944396972656, 0.0033063888549804688, 0.0039560794830322266, 0.0032203197479248047, 0.003229856491088867, 0.003232240676879883, 0.0032324790954589844, 0.0032901763916015625, 0.0034737586975097656, 0.003467082977294922, 0.0036842823028564453, 0.0034482479095458984, 0.0034308433532714844, 0.003433704376220703, 0.0034432411193847656, 0.028865814208984375, 0.0034520626068115234, 0.0034470558166503906, 0.0034437179565429688, 0.003442049026489258, 0.0034568309783935547, 0.003242969512939453, 0.003431081771850586, 0.0033338069915771484, 0.0034475326538085938, 0.003325223922729492, 0.0033609867095947266, 0.0034372806549072266, 0.003264188766479492, 0.0032358169555664062, 0.003221273422241211, 0.0034852027893066406, 0.003435850143432617, 0.0034410953521728516, 0.003412961959838867, 0.0034279823303222656, 0.0034232139587402344, 0.0034568309783935547, 0.0034444332122802734, 0.003316164016723633, 0.003417491912841797, 0.0034787654876708984, 0.0034542083740234375, 0.0032575130462646484, 0.0032308101654052734, 0.00539088249206543, 0.0034427642822265625, 0.0034258365631103516, 0.003476858139038086, 0.0034842491149902344, 0.028522253036499023, 0.0032448768615722656, 0.003234386444091797, 0.0034356117248535156, 0.0034241676330566406, 0.0034470558166503906, 0.003425121307373047, 0.003212451934814453, 0.0032389163970947266, 0.0032088756561279297]
[03/22 14:07:18] train_sl INFO: Total time taken by SELCON = 2.0150 
Hyperparms:	fraction:0.03	select every:35	num epochs:50	delta:0.01
[03/22 14:07:18] train_sl INFO: DotMap(setting='SL', dataset=DotMap(name='LawSchool_selcon', datadir='../data', feature='dss', type='pre-defined'), dataloader=DotMap(shuffle=True, batch_size=100, pin_memory=False), model=DotMap(architecture='RegressionNet', type='pre-defined', input_dim=10, numclasses=10), ckpt=DotMap(is_load=False, is_save=True, dir='results/', save_every=20), loss=DotMap(type='MeanSquaredLoss', use_sigmoid=False), optimizer=DotMap(type='adam', lr=0.01), scheduler=DotMap(type='StepLR', step_size=1, gamma=0.1), dss_args=DotMap(type='SELCON', fraction=0.03, select_every=35, kappa=0, delta=0.01, linear_layer=False, lam=1e-05, batch_sampler='sequential', selection_type='Supervised', collate_fn=None, model=RegressionNet(
  (linear): Linear(in_features=10, out_features=1, bias=True)
), lr=0.01, loss=MSELoss(), device='cuda', optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.01
    weight_decay: 0
), criterion=MSELoss(), num_classes=10, batch_size=100, num_epochs=500), train_args=DotMap(num_epochs=50, device='cuda', print_every=50, results_dir='results/', print_args=['val_loss', 'tst_loss', 'trn_loss', 'time'], return_args=[]), dss_arg=DotMap(batch_sampler=DotMap()), is_reg=DotMap())
[03/22 14:07:18] train_sl INFO: SELCON: starting pre compute
[03/22 14:08:23] train_sl INFO: SELCON: Finishing F phi
[03/22 14:08:25] train_sl INFO: SELCON: Finishing element wise F
[03/22 14:08:26] train_sl INFO: Model checkpoint saved at epoch: 20
Hyperparms:	fraction:0.03	select every:35	num epochs:50	delta:0.04
[03/22 14:08:26] train_sl INFO: DotMap(setting='SL', dataset=DotMap(name='LawSchool_selcon', datadir='../data', feature='dss', type='pre-defined'), dataloader=DotMap(shuffle=True, batch_size=100, pin_memory=False), model=DotMap(architecture='RegressionNet', type='pre-defined', input_dim=10, numclasses=10), ckpt=DotMap(is_load=False, is_save=True, dir='results/', save_every=20), loss=DotMap(type='MeanSquaredLoss', use_sigmoid=False), optimizer=DotMap(type='adam', lr=0.01), scheduler=DotMap(type='StepLR', step_size=1, gamma=0.1), dss_args=DotMap(type='SELCON', fraction=0.03, select_every=35, kappa=0, delta=0.04, linear_layer=False, lam=1e-05, batch_sampler='sequential', selection_type='Supervised', collate_fn=None, model=RegressionNet(
  (linear): Linear(in_features=10, out_features=1, bias=True)
), lr=0.01, loss=MSELoss(), device='cuda', optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.01
    weight_decay: 0
), criterion=MSELoss(), num_classes=10, batch_size=100, num_epochs=50), train_args=DotMap(num_epochs=50, device='cuda', print_every=50, results_dir='results/', print_args=['val_loss', 'tst_loss', 'trn_loss', 'time'], return_args=[]), dss_arg=DotMap(batch_sampler=DotMap()), is_reg=DotMap())
[03/22 14:08:26] train_sl INFO: SELCON: starting pre compute
[03/22 14:08:32] train_sl INFO: SELCON: Finishing F phi
[03/22 14:08:34] train_sl INFO: SELCON: Finishing element wise F
[03/22 14:08:34] train_sl INFO: Model checkpoint saved at epoch: 20
Hyperparms:	fraction:0.03	select every:35	num epochs:50	delta:0.1
[03/22 14:08:35] train_sl INFO: DotMap(setting='SL', dataset=DotMap(name='LawSchool_selcon', datadir='../data', feature='dss', type='pre-defined'), dataloader=DotMap(shuffle=True, batch_size=100, pin_memory=False), model=DotMap(architecture='RegressionNet', type='pre-defined', input_dim=10, numclasses=10), ckpt=DotMap(is_load=False, is_save=True, dir='results/', save_every=20), loss=DotMap(type='MeanSquaredLoss', use_sigmoid=False), optimizer=DotMap(type='adam', lr=0.01), scheduler=DotMap(type='StepLR', step_size=1, gamma=0.1), dss_args=DotMap(type='SELCON', fraction=0.03, select_every=35, kappa=0, delta=0.1, linear_layer=False, lam=1e-05, batch_sampler='sequential', selection_type='Supervised', collate_fn=None, model=RegressionNet(
  (linear): Linear(in_features=10, out_features=1, bias=True)
), lr=0.01, loss=MSELoss(), device='cuda', optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.01
    weight_decay: 0
), criterion=MSELoss(), num_classes=10, batch_size=100, num_epochs=50), train_args=DotMap(num_epochs=50, device='cuda', print_every=50, results_dir='results/', print_args=['val_loss', 'tst_loss', 'trn_loss', 'time'], return_args=[]), dss_arg=DotMap(batch_sampler=DotMap()), is_reg=DotMap())
[03/22 14:08:35] train_sl INFO: SELCON: starting pre compute
[03/22 14:08:49] train_sl INFO: SELCON: Finishing F phi
[03/22 14:08:51] train_sl INFO: SELCON: Finishing element wise F
[03/22 14:08:52] train_sl INFO: Model checkpoint saved at epoch: 20
Hyperparms:	fraction:0.03	select every:35	num epochs:100	delta:0.01
[03/22 14:08:52] train_sl INFO: DotMap(setting='SL', dataset=DotMap(name='LawSchool_selcon', datadir='../data', feature='dss', type='pre-defined'), dataloader=DotMap(shuffle=True, batch_size=100, pin_memory=False), model=DotMap(architecture='RegressionNet', type='pre-defined', input_dim=10, numclasses=10), ckpt=DotMap(is_load=False, is_save=True, dir='results/', save_every=20), loss=DotMap(type='MeanSquaredLoss', use_sigmoid=False), optimizer=DotMap(type='adam', lr=0.01), scheduler=DotMap(type='StepLR', step_size=1, gamma=0.1), dss_args=DotMap(type='SELCON', fraction=0.03, select_every=35, kappa=0, delta=0.01, linear_layer=False, lam=1e-05, batch_sampler='sequential', selection_type='Supervised', collate_fn=None, model=RegressionNet(
  (linear): Linear(in_features=10, out_features=1, bias=True)
), lr=0.01, loss=MSELoss(), device='cuda', optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.01
    weight_decay: 0
), criterion=MSELoss(), num_classes=10, batch_size=100, num_epochs=50), train_args=DotMap(num_epochs=100, device='cuda', print_every=50, results_dir='results/', print_args=['val_loss', 'tst_loss', 'trn_loss', 'time'], return_args=[]), dss_arg=DotMap(batch_sampler=DotMap()), is_reg=DotMap())
[03/22 14:08:52] train_sl INFO: SELCON: starting pre compute
[03/22 14:08:52] train_sl INFO: SELCON: Finishing F phi
[03/22 14:08:54] train_sl INFO: SELCON: Finishing element wise F
[03/22 14:08:54] train_sl INFO: Model checkpoint saved at epoch: 20
Hyperparms:	fraction:0.03	select every:35	num epochs:100	delta:0.04
[03/22 14:08:55] train_sl INFO: DotMap(setting='SL', dataset=DotMap(name='LawSchool_selcon', datadir='../data', feature='dss', type='pre-defined'), dataloader=DotMap(shuffle=True, batch_size=100, pin_memory=False), model=DotMap(architecture='RegressionNet', type='pre-defined', input_dim=10, numclasses=10), ckpt=DotMap(is_load=False, is_save=True, dir='results/', save_every=20), loss=DotMap(type='MeanSquaredLoss', use_sigmoid=False), optimizer=DotMap(type='adam', lr=0.01), scheduler=DotMap(type='StepLR', step_size=1, gamma=0.1), dss_args=DotMap(type='SELCON', fraction=0.03, select_every=35, kappa=0, delta=0.04, linear_layer=False, lam=1e-05, batch_sampler='sequential', selection_type='Supervised', collate_fn=None, model=RegressionNet(
  (linear): Linear(in_features=10, out_features=1, bias=True)
), lr=0.01, loss=MSELoss(), device='cuda', optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.01
    weight_decay: 0
), criterion=MSELoss(), num_classes=10, batch_size=100, num_epochs=100), train_args=DotMap(num_epochs=100, device='cuda', print_every=50, results_dir='results/', print_args=['val_loss', 'tst_loss', 'trn_loss', 'time'], return_args=[]), dss_arg=DotMap(batch_sampler=DotMap()), is_reg=DotMap())
[03/22 14:08:55] train_sl INFO: SELCON: starting pre compute
[03/22 14:08:55] train_sl INFO: SELCON: Finishing F phi
[03/22 14:08:57] train_sl INFO: SELCON: Finishing element wise F
[03/22 14:08:57] train_sl INFO: Model checkpoint saved at epoch: 20
Hyperparms:	fraction:0.03	select every:35	num epochs:100	delta:0.1
[03/22 14:08:57] train_sl INFO: DotMap(setting='SL', dataset=DotMap(name='LawSchool_selcon', datadir='../data', feature='dss', type='pre-defined'), dataloader=DotMap(shuffle=True, batch_size=100, pin_memory=False), model=DotMap(architecture='RegressionNet', type='pre-defined', input_dim=10, numclasses=10), ckpt=DotMap(is_load=False, is_save=True, dir='results/', save_every=20), loss=DotMap(type='MeanSquaredLoss', use_sigmoid=False), optimizer=DotMap(type='adam', lr=0.01), scheduler=DotMap(type='StepLR', step_size=1, gamma=0.1), dss_args=DotMap(type='SELCON', fraction=0.03, select_every=35, kappa=0, delta=0.1, linear_layer=False, lam=1e-05, batch_sampler='sequential', selection_type='Supervised', collate_fn=None, model=RegressionNet(
  (linear): Linear(in_features=10, out_features=1, bias=True)
), lr=0.01, loss=MSELoss(), device='cuda', optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.01
    weight_decay: 0
), criterion=MSELoss(), num_classes=10, batch_size=100, num_epochs=100), train_args=DotMap(num_epochs=100, device='cuda', print_every=50, results_dir='results/', print_args=['val_loss', 'tst_loss', 'trn_loss', 'time'], return_args=[]), dss_arg=DotMap(batch_sampler=DotMap()), is_reg=DotMap())
[03/22 14:08:57] train_sl INFO: SELCON: starting pre compute
[03/22 14:08:58] train_sl INFO: SELCON: Finishing F phi
[03/22 14:09:01] train_sl INFO: SELCON: Finishing element wise F
[03/22 14:09:01] train_sl INFO: Model checkpoint saved at epoch: 20
Hyperparms:	fraction:0.03	select every:35	num epochs:200	delta:0.01
[03/22 14:09:01] train_sl INFO: DotMap(setting='SL', dataset=DotMap(name='LawSchool_selcon', datadir='../data', feature='dss', type='pre-defined'), dataloader=DotMap(shuffle=True, batch_size=100, pin_memory=False), model=DotMap(architecture='RegressionNet', type='pre-defined', input_dim=10, numclasses=10), ckpt=DotMap(is_load=False, is_save=True, dir='results/', save_every=20), loss=DotMap(type='MeanSquaredLoss', use_sigmoid=False), optimizer=DotMap(type='adam', lr=0.01), scheduler=DotMap(type='StepLR', step_size=1, gamma=0.1), dss_args=DotMap(type='SELCON', fraction=0.03, select_every=35, kappa=0, delta=0.01, linear_layer=False, lam=1e-05, batch_sampler='sequential', selection_type='Supervised', collate_fn=None, model=RegressionNet(
  (linear): Linear(in_features=10, out_features=1, bias=True)
), lr=0.01, loss=MSELoss(), device='cuda', optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.01
    weight_decay: 0
), criterion=MSELoss(), num_classes=10, batch_size=100, num_epochs=100), train_args=DotMap(num_epochs=200, device='cuda', print_every=50, results_dir='results/', print_args=['val_loss', 'tst_loss', 'trn_loss', 'time'], return_args=[]), dss_arg=DotMap(batch_sampler=DotMap()), is_reg=DotMap())
[03/22 14:09:01] train_sl INFO: SELCON: starting pre compute
[03/22 14:10:22] train_sl INFO: SELCON: Finishing F phi
[03/22 14:10:25] train_sl INFO: SELCON: Finishing element wise F
[03/22 14:10:25] train_sl INFO: Model checkpoint saved at epoch: 20
Hyperparms:	fraction:0.03	select every:35	num epochs:200	delta:0.04
[03/22 14:10:25] train_sl INFO: DotMap(setting='SL', dataset=DotMap(name='LawSchool_selcon', datadir='../data', feature='dss', type='pre-defined'), dataloader=DotMap(shuffle=True, batch_size=100, pin_memory=False), model=DotMap(architecture='RegressionNet', type='pre-defined', input_dim=10, numclasses=10), ckpt=DotMap(is_load=False, is_save=True, dir='results/', save_every=20), loss=DotMap(type='MeanSquaredLoss', use_sigmoid=False), optimizer=DotMap(type='adam', lr=0.01), scheduler=DotMap(type='StepLR', step_size=1, gamma=0.1), dss_args=DotMap(type='SELCON', fraction=0.03, select_every=35, kappa=0, delta=0.04, linear_layer=False, lam=1e-05, batch_sampler='sequential', selection_type='Supervised', collate_fn=None, model=RegressionNet(
  (linear): Linear(in_features=10, out_features=1, bias=True)
), lr=0.01, loss=MSELoss(), device='cuda', optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.01
    weight_decay: 0
), criterion=MSELoss(), num_classes=10, batch_size=100, num_epochs=200), train_args=DotMap(num_epochs=200, device='cuda', print_every=50, results_dir='results/', print_args=['val_loss', 'tst_loss', 'trn_loss', 'time'], return_args=[]), dss_arg=DotMap(batch_sampler=DotMap()), is_reg=DotMap())
[03/22 14:10:25] train_sl INFO: SELCON: starting pre compute
[03/22 14:10:25] train_sl INFO: SELCON: Finishing F phi
[03/22 14:10:28] train_sl INFO: SELCON: Finishing element wise F
[03/22 14:10:28] train_sl INFO: Model checkpoint saved at epoch: 20
Hyperparms:	fraction:0.03	select every:35	num epochs:200	delta:0.1
[03/22 14:10:28] train_sl INFO: DotMap(setting='SL', dataset=DotMap(name='LawSchool_selcon', datadir='../data', feature='dss', type='pre-defined'), dataloader=DotMap(shuffle=True, batch_size=100, pin_memory=False), model=DotMap(architecture='RegressionNet', type='pre-defined', input_dim=10, numclasses=10), ckpt=DotMap(is_load=False, is_save=True, dir='results/', save_every=20), loss=DotMap(type='MeanSquaredLoss', use_sigmoid=False), optimizer=DotMap(type='adam', lr=0.01), scheduler=DotMap(type='StepLR', step_size=1, gamma=0.1), dss_args=DotMap(type='SELCON', fraction=0.03, select_every=35, kappa=0, delta=0.1, linear_layer=False, lam=1e-05, batch_sampler='sequential', selection_type='Supervised', collate_fn=None, model=RegressionNet(
  (linear): Linear(in_features=10, out_features=1, bias=True)
), lr=0.01, loss=MSELoss(), device='cuda', optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.01
    weight_decay: 0
), criterion=MSELoss(), num_classes=10, batch_size=100, num_epochs=200), train_args=DotMap(num_epochs=200, device='cuda', print_every=50, results_dir='results/', print_args=['val_loss', 'tst_loss', 'trn_loss', 'time'], return_args=[]), dss_arg=DotMap(batch_sampler=DotMap()), is_reg=DotMap())
[03/22 14:10:28] train_sl INFO: SELCON: starting pre compute
[03/22 14:10:37] train_sl INFO: SELCON: Finishing F phi
[03/22 14:10:40] train_sl INFO: SELCON: Finishing element wise F
[03/22 14:10:40] train_sl INFO: Model checkpoint saved at epoch: 20
Hyperparms:	fraction:0.03	select every:35	num epochs:500	delta:0.01
[03/22 14:10:40] train_sl INFO: DotMap(setting='SL', dataset=DotMap(name='LawSchool_selcon', datadir='../data', feature='dss', type='pre-defined'), dataloader=DotMap(shuffle=True, batch_size=100, pin_memory=False), model=DotMap(architecture='RegressionNet', type='pre-defined', input_dim=10, numclasses=10), ckpt=DotMap(is_load=False, is_save=True, dir='results/', save_every=20), loss=DotMap(type='MeanSquaredLoss', use_sigmoid=False), optimizer=DotMap(type='adam', lr=0.01), scheduler=DotMap(type='StepLR', step_size=1, gamma=0.1), dss_args=DotMap(type='SELCON', fraction=0.03, select_every=35, kappa=0, delta=0.01, linear_layer=False, lam=1e-05, batch_sampler='sequential', selection_type='Supervised', collate_fn=None, model=RegressionNet(
  (linear): Linear(in_features=10, out_features=1, bias=True)
), lr=0.01, loss=MSELoss(), device='cuda', optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.01
    weight_decay: 0
), criterion=MSELoss(), num_classes=10, batch_size=100, num_epochs=200), train_args=DotMap(num_epochs=500, device='cuda', print_every=50, results_dir='results/', print_args=['val_loss', 'tst_loss', 'trn_loss', 'time'], return_args=[]), dss_arg=DotMap(batch_sampler=DotMap()), is_reg=DotMap())
[03/22 14:10:40] train_sl INFO: SELCON: starting pre compute
Hyperparms:	fraction:0.03	select every:35	num epochs:500	delta:0.04
[03/22 14:11:50] train_sl INFO: DotMap(setting='SL', dataset=DotMap(name='LawSchool_selcon', datadir='../data', feature='dss', type='pre-defined'), dataloader=DotMap(shuffle=True, batch_size=100, pin_memory=False), model=DotMap(architecture='RegressionNet', type='pre-defined', input_dim=10, numclasses=10), ckpt=DotMap(is_load=False, is_save=True, dir='results/', save_every=20), loss=DotMap(type='MeanSquaredLoss', use_sigmoid=False), optimizer=DotMap(type='adam', lr=0.01), scheduler=DotMap(type='StepLR', step_size=1, gamma=0.1), dss_args=DotMap(type='SELCON', fraction=0.03, select_every=35, kappa=0, delta=0.04, linear_layer=False, lam=1e-05, batch_sampler='sequential', selection_type='Supervised', collate_fn=None, model=RegressionNet(
  (linear): Linear(in_features=10, out_features=1, bias=True)
), lr=0.01, loss=MSELoss(), device='cuda', optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.01
    weight_decay: 0
), criterion=MSELoss(), num_classes=10, batch_size=100, num_epochs=500), train_args=DotMap(num_epochs=500, device='cuda', print_every=50, results_dir='results/', print_args=['val_loss', 'tst_loss', 'trn_loss', 'time'], return_args=[]), dss_arg=DotMap(batch_sampler=DotMap()), is_reg=DotMap())
[03/22 14:11:50] train_sl INFO: SELCON: starting pre compute
[03/22 14:11:50] train_sl INFO: SELCON: Finishing F phi
[03/22 14:11:53] train_sl INFO: SELCON: Finishing element wise F
[03/22 14:11:53] train_sl INFO: Model checkpoint saved at epoch: 20
Hyperparms:	fraction:0.03	select every:35	num epochs:500	delta:0.1
[03/22 14:11:53] train_sl INFO: DotMap(setting='SL', dataset=DotMap(name='LawSchool_selcon', datadir='../data', feature='dss', type='pre-defined'), dataloader=DotMap(shuffle=True, batch_size=100, pin_memory=False), model=DotMap(architecture='RegressionNet', type='pre-defined', input_dim=10, numclasses=10), ckpt=DotMap(is_load=False, is_save=True, dir='results/', save_every=20), loss=DotMap(type='MeanSquaredLoss', use_sigmoid=False), optimizer=DotMap(type='adam', lr=0.01), scheduler=DotMap(type='StepLR', step_size=1, gamma=0.1), dss_args=DotMap(type='SELCON', fraction=0.03, select_every=35, kappa=0, delta=0.1, linear_layer=False, lam=1e-05, batch_sampler='sequential', selection_type='Supervised', collate_fn=None, model=RegressionNet(
  (linear): Linear(in_features=10, out_features=1, bias=True)
), lr=0.01, loss=MSELoss(), device='cuda', optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.01
    weight_decay: 0
), criterion=MSELoss(), num_classes=10, batch_size=100, num_epochs=500), train_args=DotMap(num_epochs=500, device='cuda', print_every=50, results_dir='results/', print_args=['val_loss', 'tst_loss', 'trn_loss', 'time'], return_args=[]), dss_arg=DotMap(batch_sampler=DotMap()), is_reg=DotMap())
[03/22 14:11:53] train_sl INFO: SELCON: starting pre compute
[03/22 14:11:58] train_sl INFO: SELCON: Finishing F phi
[03/22 14:12:00] train_sl INFO: SELCON: Finishing element wise F
[03/22 14:12:01] train_sl INFO: Model checkpoint saved at epoch: 20
Hyperparms:	fraction:0.05	select every:35	num epochs:50	delta:0.01
[03/22 14:12:01] train_sl INFO: DotMap(setting='SL', dataset=DotMap(name='LawSchool_selcon', datadir='../data', feature='dss', type='pre-defined'), dataloader=DotMap(shuffle=True, batch_size=100, pin_memory=False), model=DotMap(architecture='RegressionNet', type='pre-defined', input_dim=10, numclasses=10), ckpt=DotMap(is_load=False, is_save=True, dir='results/', save_every=20), loss=DotMap(type='MeanSquaredLoss', use_sigmoid=False), optimizer=DotMap(type='adam', lr=0.01), scheduler=DotMap(type='StepLR', step_size=1, gamma=0.1), dss_args=DotMap(type='SELCON', fraction=0.05, select_every=35, kappa=0, delta=0.01, linear_layer=False, lam=1e-05, batch_sampler='sequential', selection_type='Supervised', collate_fn=None, model=RegressionNet(
  (linear): Linear(in_features=10, out_features=1, bias=True)
), lr=0.01, loss=MSELoss(), device='cuda', optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.01
    weight_decay: 0
), criterion=MSELoss(), num_classes=10, batch_size=100, num_epochs=500), train_args=DotMap(num_epochs=50, device='cuda', print_every=50, results_dir='results/', print_args=['val_loss', 'tst_loss', 'trn_loss', 'time'], return_args=[]), dss_arg=DotMap(batch_sampler=DotMap()), is_reg=DotMap())
[03/22 14:12:01] train_sl INFO: SELCON: starting pre compute
[03/22 14:12:01] train_sl INFO: SELCON: Finishing F phi
[03/22 14:12:03] train_sl INFO: SELCON: Finishing element wise F
[03/22 14:12:03] train_sl INFO: Model checkpoint saved at epoch: 20
Hyperparms:	fraction:0.05	select every:35	num epochs:50	delta:0.04
[03/22 14:12:04] train_sl INFO: DotMap(setting='SL', dataset=DotMap(name='LawSchool_selcon', datadir='../data', feature='dss', type='pre-defined'), dataloader=DotMap(shuffle=True, batch_size=100, pin_memory=False), model=DotMap(architecture='RegressionNet', type='pre-defined', input_dim=10, numclasses=10), ckpt=DotMap(is_load=False, is_save=True, dir='results/', save_every=20), loss=DotMap(type='MeanSquaredLoss', use_sigmoid=False), optimizer=DotMap(type='adam', lr=0.01), scheduler=DotMap(type='StepLR', step_size=1, gamma=0.1), dss_args=DotMap(type='SELCON', fraction=0.05, select_every=35, kappa=0, delta=0.04, linear_layer=False, lam=1e-05, batch_sampler='sequential', selection_type='Supervised', collate_fn=None, model=RegressionNet(
  (linear): Linear(in_features=10, out_features=1, bias=True)
), lr=0.01, loss=MSELoss(), device='cuda', optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.01
    weight_decay: 0
), criterion=MSELoss(), num_classes=10, batch_size=100, num_epochs=50), train_args=DotMap(num_epochs=50, device='cuda', print_every=50, results_dir='results/', print_args=['val_loss', 'tst_loss', 'trn_loss', 'time'], return_args=[]), dss_arg=DotMap(batch_sampler=DotMap()), is_reg=DotMap())
[03/22 14:12:04] train_sl INFO: SELCON: starting pre compute
[03/22 14:12:07] train_sl INFO: SELCON: Finishing F phi
[03/22 14:12:10] train_sl INFO: SELCON: Finishing element wise F
[03/22 14:12:10] train_sl INFO: Model checkpoint saved at epoch: 20
Hyperparms:	fraction:0.05	select every:35	num epochs:50	delta:0.1
[03/22 14:12:10] train_sl INFO: DotMap(setting='SL', dataset=DotMap(name='LawSchool_selcon', datadir='../data', feature='dss', type='pre-defined'), dataloader=DotMap(shuffle=True, batch_size=100, pin_memory=False), model=DotMap(architecture='RegressionNet', type='pre-defined', input_dim=10, numclasses=10), ckpt=DotMap(is_load=False, is_save=True, dir='results/', save_every=20), loss=DotMap(type='MeanSquaredLoss', use_sigmoid=False), optimizer=DotMap(type='adam', lr=0.01), scheduler=DotMap(type='StepLR', step_size=1, gamma=0.1), dss_args=DotMap(type='SELCON', fraction=0.05, select_every=35, kappa=0, delta=0.1, linear_layer=False, lam=1e-05, batch_sampler='sequential', selection_type='Supervised', collate_fn=None, model=RegressionNet(
  (linear): Linear(in_features=10, out_features=1, bias=True)
), lr=0.01, loss=MSELoss(), device='cuda', optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.01
    weight_decay: 0
), criterion=MSELoss(), num_classes=10, batch_size=100, num_epochs=50), train_args=DotMap(num_epochs=50, device='cuda', print_every=50, results_dir='results/', print_args=['val_loss', 'tst_loss', 'trn_loss', 'time'], return_args=[]), dss_arg=DotMap(batch_sampler=DotMap()), is_reg=DotMap())
[03/22 14:12:10] train_sl INFO: SELCON: starting pre compute
[03/22 14:12:13] train_sl INFO: SELCON: Finishing F phi
[03/22 14:12:15] train_sl INFO: SELCON: Finishing element wise F
[03/22 14:12:16] train_sl INFO: Model checkpoint saved at epoch: 20
Hyperparms:	fraction:0.05	select every:35	num epochs:100	delta:0.01
[03/22 14:12:16] train_sl INFO: DotMap(setting='SL', dataset=DotMap(name='LawSchool_selcon', datadir='../data', feature='dss', type='pre-defined'), dataloader=DotMap(shuffle=True, batch_size=100, pin_memory=False), model=DotMap(architecture='RegressionNet', type='pre-defined', input_dim=10, numclasses=10), ckpt=DotMap(is_load=False, is_save=True, dir='results/', save_every=20), loss=DotMap(type='MeanSquaredLoss', use_sigmoid=False), optimizer=DotMap(type='adam', lr=0.01), scheduler=DotMap(type='StepLR', step_size=1, gamma=0.1), dss_args=DotMap(type='SELCON', fraction=0.05, select_every=35, kappa=0, delta=0.01, linear_layer=False, lam=1e-05, batch_sampler='sequential', selection_type='Supervised', collate_fn=None, model=RegressionNet(
  (linear): Linear(in_features=10, out_features=1, bias=True)
), lr=0.01, loss=MSELoss(), device='cuda', optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.01
    weight_decay: 0
), criterion=MSELoss(), num_classes=10, batch_size=100, num_epochs=50), train_args=DotMap(num_epochs=100, device='cuda', print_every=50, results_dir='results/', print_args=['val_loss', 'tst_loss', 'trn_loss', 'time'], return_args=[]), dss_arg=DotMap(batch_sampler=DotMap()), is_reg=DotMap())
[03/22 14:12:16] train_sl INFO: SELCON: starting pre compute
[03/22 14:13:02] train_sl INFO: SELCON: Finishing F phi
[03/22 14:13:04] train_sl INFO: SELCON: Finishing element wise F
[03/22 14:13:04] train_sl INFO: Model checkpoint saved at epoch: 20
Hyperparms:	fraction:0.05	select every:35	num epochs:100	delta:0.04
[03/22 14:13:05] train_sl INFO: DotMap(setting='SL', dataset=DotMap(name='LawSchool_selcon', datadir='../data', feature='dss', type='pre-defined'), dataloader=DotMap(shuffle=True, batch_size=100, pin_memory=False), model=DotMap(architecture='RegressionNet', type='pre-defined', input_dim=10, numclasses=10), ckpt=DotMap(is_load=False, is_save=True, dir='results/', save_every=20), loss=DotMap(type='MeanSquaredLoss', use_sigmoid=False), optimizer=DotMap(type='adam', lr=0.01), scheduler=DotMap(type='StepLR', step_size=1, gamma=0.1), dss_args=DotMap(type='SELCON', fraction=0.05, select_every=35, kappa=0, delta=0.04, linear_layer=False, lam=1e-05, batch_sampler='sequential', selection_type='Supervised', collate_fn=None, model=RegressionNet(
  (linear): Linear(in_features=10, out_features=1, bias=True)
), lr=0.01, loss=MSELoss(), device='cuda', optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.01
    weight_decay: 0
), criterion=MSELoss(), num_classes=10, batch_size=100, num_epochs=100), train_args=DotMap(num_epochs=100, device='cuda', print_every=50, results_dir='results/', print_args=['val_loss', 'tst_loss', 'trn_loss', 'time'], return_args=[]), dss_arg=DotMap(batch_sampler=DotMap()), is_reg=DotMap())
[03/22 14:13:05] train_sl INFO: SELCON: starting pre compute
[03/22 14:13:10] train_sl INFO: SELCON: Finishing F phi
[03/22 14:13:12] train_sl INFO: SELCON: Finishing element wise F
[03/22 14:13:12] train_sl INFO: Model checkpoint saved at epoch: 20
Hyperparms:	fraction:0.05	select every:35	num epochs:100	delta:0.1
[03/22 14:13:13] train_sl INFO: DotMap(setting='SL', dataset=DotMap(name='LawSchool_selcon', datadir='../data', feature='dss', type='pre-defined'), dataloader=DotMap(shuffle=True, batch_size=100, pin_memory=False), model=DotMap(architecture='RegressionNet', type='pre-defined', input_dim=10, numclasses=10), ckpt=DotMap(is_load=False, is_save=True, dir='results/', save_every=20), loss=DotMap(type='MeanSquaredLoss', use_sigmoid=False), optimizer=DotMap(type='adam', lr=0.01), scheduler=DotMap(type='StepLR', step_size=1, gamma=0.1), dss_args=DotMap(type='SELCON', fraction=0.05, select_every=35, kappa=0, delta=0.1, linear_layer=False, lam=1e-05, batch_sampler='sequential', selection_type='Supervised', collate_fn=None, model=RegressionNet(
  (linear): Linear(in_features=10, out_features=1, bias=True)
), lr=0.01, loss=MSELoss(), device='cuda', optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.01
    weight_decay: 0
), criterion=MSELoss(), num_classes=10, batch_size=100, num_epochs=100), train_args=DotMap(num_epochs=100, device='cuda', print_every=50, results_dir='results/', print_args=['val_loss', 'tst_loss', 'trn_loss', 'time'], return_args=[]), dss_arg=DotMap(batch_sampler=DotMap()), is_reg=DotMap())
[03/22 14:13:13] train_sl INFO: SELCON: starting pre compute
[03/22 14:13:15] train_sl INFO: SELCON: Finishing F phi
[03/22 14:13:18] train_sl INFO: SELCON: Finishing element wise F
[03/22 14:13:18] train_sl INFO: Model checkpoint saved at epoch: 20
Hyperparms:	fraction:0.05	select every:35	num epochs:200	delta:0.01
[03/22 14:13:18] train_sl INFO: DotMap(setting='SL', dataset=DotMap(name='LawSchool_selcon', datadir='../data', feature='dss', type='pre-defined'), dataloader=DotMap(shuffle=True, batch_size=100, pin_memory=False), model=DotMap(architecture='RegressionNet', type='pre-defined', input_dim=10, numclasses=10), ckpt=DotMap(is_load=False, is_save=True, dir='results/', save_every=20), loss=DotMap(type='MeanSquaredLoss', use_sigmoid=False), optimizer=DotMap(type='adam', lr=0.01), scheduler=DotMap(type='StepLR', step_size=1, gamma=0.1), dss_args=DotMap(type='SELCON', fraction=0.05, select_every=35, kappa=0, delta=0.01, linear_layer=False, lam=1e-05, batch_sampler='sequential', selection_type='Supervised', collate_fn=None, model=RegressionNet(
  (linear): Linear(in_features=10, out_features=1, bias=True)
), lr=0.01, loss=MSELoss(), device='cuda', optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.01
    weight_decay: 0
), criterion=MSELoss(), num_classes=10, batch_size=100, num_epochs=100), train_args=DotMap(num_epochs=200, device='cuda', print_every=50, results_dir='results/', print_args=['val_loss', 'tst_loss', 'trn_loss', 'time'], return_args=[]), dss_arg=DotMap(batch_sampler=DotMap()), is_reg=DotMap())
[03/22 14:13:18] train_sl INFO: SELCON: starting pre compute
Hyperparms:	fraction:0.05	select every:35	num epochs:200	delta:0.04
[03/22 14:13:49] train_sl INFO: DotMap(setting='SL', dataset=DotMap(name='LawSchool_selcon', datadir='../data', feature='dss', type='pre-defined'), dataloader=DotMap(shuffle=True, batch_size=100, pin_memory=False), model=DotMap(architecture='RegressionNet', type='pre-defined', input_dim=10, numclasses=10), ckpt=DotMap(is_load=False, is_save=True, dir='results/', save_every=20), loss=DotMap(type='MeanSquaredLoss', use_sigmoid=False), optimizer=DotMap(type='adam', lr=0.01), scheduler=DotMap(type='StepLR', step_size=1, gamma=0.1), dss_args=DotMap(type='SELCON', fraction=0.05, select_every=35, kappa=0, delta=0.04, linear_layer=False, lam=1e-05, batch_sampler='sequential', selection_type='Supervised', collate_fn=None, model=RegressionNet(
  (linear): Linear(in_features=10, out_features=1, bias=True)
), lr=0.01, loss=MSELoss(), device='cuda', optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.01
    weight_decay: 0
), criterion=MSELoss(), num_classes=10, batch_size=100, num_epochs=200), train_args=DotMap(num_epochs=200, device='cuda', print_every=50, results_dir='results/', print_args=['val_loss', 'tst_loss', 'trn_loss', 'time'], return_args=[]), dss_arg=DotMap(batch_sampler=DotMap()), is_reg=DotMap())
[03/22 14:13:49] train_sl INFO: SELCON: starting pre compute
Hyperparms:	fraction:0.05	select every:35	num epochs:200	delta:0.1
[03/22 14:13:49] train_sl INFO: DotMap(setting='SL', dataset=DotMap(name='LawSchool_selcon', datadir='../data', feature='dss', type='pre-defined'), dataloader=DotMap(shuffle=True, batch_size=100, pin_memory=False), model=DotMap(architecture='RegressionNet', type='pre-defined', input_dim=10, numclasses=10), ckpt=DotMap(is_load=False, is_save=True, dir='results/', save_every=20), loss=DotMap(type='MeanSquaredLoss', use_sigmoid=False), optimizer=DotMap(type='adam', lr=0.01), scheduler=DotMap(type='StepLR', step_size=1, gamma=0.1), dss_args=DotMap(type='SELCON', fraction=0.05, select_every=35, kappa=0, delta=0.1, linear_layer=False, lam=1e-05, batch_sampler='sequential', selection_type='Supervised', collate_fn=None, model=RegressionNet(
  (linear): Linear(in_features=10, out_features=1, bias=True)
), lr=0.01, loss=MSELoss(), device='cuda', optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.01
    weight_decay: 0
), criterion=MSELoss(), num_classes=10, batch_size=100, num_epochs=200), train_args=DotMap(num_epochs=200, device='cuda', print_every=50, results_dir='results/', print_args=['val_loss', 'tst_loss', 'trn_loss', 'time'], return_args=[]), dss_arg=DotMap(batch_sampler=DotMap()), is_reg=DotMap())
[03/22 14:13:49] train_sl INFO: SELCON: starting pre compute
Hyperparms:	fraction:0.05	select every:35	num epochs:500	delta:0.01
[03/22 14:13:49] train_sl INFO: DotMap(setting='SL', dataset=DotMap(name='LawSchool_selcon', datadir='../data', feature='dss', type='pre-defined'), dataloader=DotMap(shuffle=True, batch_size=100, pin_memory=False), model=DotMap(architecture='RegressionNet', type='pre-defined', input_dim=10, numclasses=10), ckpt=DotMap(is_load=False, is_save=True, dir='results/', save_every=20), loss=DotMap(type='MeanSquaredLoss', use_sigmoid=False), optimizer=DotMap(type='adam', lr=0.01), scheduler=DotMap(type='StepLR', step_size=1, gamma=0.1), dss_args=DotMap(type='SELCON', fraction=0.05, select_every=35, kappa=0, delta=0.01, linear_layer=False, lam=1e-05, batch_sampler='sequential', selection_type='Supervised', collate_fn=None, model=RegressionNet(
  (linear): Linear(in_features=10, out_features=1, bias=True)
), lr=0.01, loss=MSELoss(), device='cuda', optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.01
    weight_decay: 0
), criterion=MSELoss(), num_classes=10, batch_size=100, num_epochs=200), train_args=DotMap(num_epochs=500, device='cuda', print_every=50, results_dir='results/', print_args=['val_loss', 'tst_loss', 'trn_loss', 'time'], return_args=[]), dss_arg=DotMap(batch_sampler=DotMap()), is_reg=DotMap())
[03/22 14:13:49] train_sl INFO: SELCON: starting pre compute
[03/22 14:13:49] train_sl INFO: SELCON: Finishing F phi
Hyperparms:	fraction:0.05	select every:35	num epochs:500	delta:0.04
[03/22 14:13:50] train_sl INFO: DotMap(setting='SL', dataset=DotMap(name='LawSchool_selcon', datadir='../data', feature='dss', type='pre-defined'), dataloader=DotMap(shuffle=True, batch_size=100, pin_memory=False), model=DotMap(architecture='RegressionNet', type='pre-defined', input_dim=10, numclasses=10), ckpt=DotMap(is_load=False, is_save=True, dir='results/', save_every=20), loss=DotMap(type='MeanSquaredLoss', use_sigmoid=False), optimizer=DotMap(type='adam', lr=0.01), scheduler=DotMap(type='StepLR', step_size=1, gamma=0.1), dss_args=DotMap(type='SELCON', fraction=0.05, select_every=35, kappa=0, delta=0.04, linear_layer=False, lam=1e-05, batch_sampler='sequential', selection_type='Supervised', collate_fn=None, model=RegressionNet(
  (linear): Linear(in_features=10, out_features=1, bias=True)
), lr=0.01, loss=MSELoss(), device='cuda', optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.01
    weight_decay: 0
), criterion=MSELoss(), num_classes=10, batch_size=100, num_epochs=500), train_args=DotMap(num_epochs=500, device='cuda', print_every=50, results_dir='results/', print_args=['val_loss', 'tst_loss', 'trn_loss', 'time'], return_args=[]), dss_arg=DotMap(batch_sampler=DotMap()), is_reg=DotMap())
[03/22 14:13:50] train_sl INFO: SELCON: starting pre compute
[03/22 14:13:51] train_sl INFO: SELCON: Finishing F phi
[03/22 14:13:54] train_sl INFO: SELCON: Finishing element wise F
[03/22 14:13:54] train_sl INFO: Model checkpoint saved at epoch: 20
Hyperparms:	fraction:0.05	select every:35	num epochs:500	delta:0.1
[03/22 14:13:54] train_sl INFO: DotMap(setting='SL', dataset=DotMap(name='LawSchool_selcon', datadir='../data', feature='dss', type='pre-defined'), dataloader=DotMap(shuffle=True, batch_size=100, pin_memory=False), model=DotMap(architecture='RegressionNet', type='pre-defined', input_dim=10, numclasses=10), ckpt=DotMap(is_load=False, is_save=True, dir='results/', save_every=20), loss=DotMap(type='MeanSquaredLoss', use_sigmoid=False), optimizer=DotMap(type='adam', lr=0.01), scheduler=DotMap(type='StepLR', step_size=1, gamma=0.1), dss_args=DotMap(type='SELCON', fraction=0.05, select_every=35, kappa=0, delta=0.1, linear_layer=False, lam=1e-05, batch_sampler='sequential', selection_type='Supervised', collate_fn=None, model=RegressionNet(
  (linear): Linear(in_features=10, out_features=1, bias=True)
), lr=0.01, loss=MSELoss(), device='cuda', optimizer=Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.01
    weight_decay: 0
), criterion=MSELoss(), num_classes=10, batch_size=100, num_epochs=500), train_args=DotMap(num_epochs=500, device='cuda', print_every=50, results_dir='results/', print_args=['val_loss', 'tst_loss', 'trn_loss', 'time'], return_args=[]), dss_arg=DotMap(batch_sampler=DotMap()), is_reg=DotMap())
[03/22 14:13:54] train_sl INFO: SELCON: starting pre compute
[03/22 14:13:54] train_sl INFO: SELCON: Finishing F phi
[03/22 14:13:57] train_sl INFO: SELCON: Finishing element wise F
[03/22 14:13:57] train_sl INFO: Model checkpoint saved at epoch: 20
