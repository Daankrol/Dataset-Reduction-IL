#cifar10. lr = 0.01, 
#- non pretrained all methods, craig, gradmatch, glister, random, full
#- warm up v non warm up -> gradmatch, craig
#- gradmatch,craig: 30x10, 20x5, 
## cifar10 all methods
# python3 run_experiment.py --config ./exp_configs/cifar10/config_gradmatch.py --select_every 10 --fraction 0.2 # ok
# python3 run_experiment.py --config ./exp_configs/cifar10/config_gradmatchpb.py --select_every 10 --fraction 0.2 # ok
# python3 run_experiment.py --config ./exp_configs/cifar10/config_glister.py --select_every 10 --fraction 0.2 # ok
# python3 run_experiment.py --config ./exp_configs/cifar10/config_craig.py --select_every 10 --fraction 0.2 # ok
# python3 run_experiment.py --config ./exp_configs/cifar10/config_craigpb.py --select_every 10 --fraction 0.2 # ok
# python3 run_experiment.py --config ./exp_configs/cifar10/config_random.py --select_every 10 --fraction 0.2 # ok
# python3 run_experiment.py --config ./exp_configs/cifar10/config_full.py # ok
## cifar 10 warm up vs non warm up
# python3 run_experiment.py --config ./exp_configs/cifar10/config_craig-warm.py --select_every 10 --fraction 0.2 --kappa 0.05 # ok
# python3 run_experiment.py --config ./exp_configs/cifar10/config_gradmatch-warm.py --select_every 10 --fraction 0.2 --kappa 0.05 # ok
# python3 run_experiment.py --config ./exp_configs/cifar10/config_craig-warm.py --select_every 10 --fraction 0.2 --kappa 0.015 # ok
# python3 run_experiment.py --config ./exp_configs/cifar10/config_gradmatch-warm.py --select_every 10 --fraction 0.2 --kappa 0.015 # ok

#cub200. lr=0.005 for pretrained, lr=0.001 for non-pretrained. Internet says 0.0001 for pretrained. (te laag)  (https://notebooks.githubusercontent.com/view/ipynb?browser=chrome&color_mode=auto&commit=2022ec832137b512324d324129d083f63f6a2188&device=unknown&enc_url=68747470733a2f2f7261772e67697468756275736572636f6e74656e742e636f6d2f736c69706e6974736b6179612f63616c746563682d62697264732d616476616e6365642d636c617373696669636174696f6e2f323032326563383332313337623531323332346433323431323964303833663633663661323138382f6e6f7465626f6f6b2e6970796e62&logged_in=false&nwo=slipnitskaya%2Fcaltech-birds-advanced-classification&path=notebook.ipynb&platform=android&repository_id=306156391&repository_type=Repository&version=98)
#- non pretrained all methods, 
#- pretrained finetune all methods
#- warm up v non warm up -> gradmatch (finetune and non-pretrained)
#- gradmatch pretrain, non finetune 
# python3 run_experiment.py --config ./exp_configs/cub200/config_gradmatch.py --select_every 10 --fraction 0.2 --lr 0.001 # ok
# python3 run_experiment.py --config ./exp_configs/cub200/config_gradmatch.py --select_every 10 --fraction 0.2 --lr 0.005 # ok
# python3 run_experiment.py --config ./exp_configs/cub200/config_gradmatch.py --select_every 10 --fraction 0.2 --lr 0.01 # ok
# python3 run_experiment.py --config ./exp_configs/cub200/config_gradmatchpb.py --select_every 10 --fraction 0.2 --lr 0.001 # ok
# python3 run_experiment.py --config ./exp_configs/cub200/config_glister.py --select_every 10 --fraction 0.2 --lr 0.001 # ok
# python3 run_experiment.py --config ./exp_configs/cub200/config_craig.py --select_every 10 --fraction 0.2 --lr 0.001 # ok
# python3 run_experiment.py --config ./exp_configs/cub200/config_craigpb.py --select_every 10 --fraction 0.2 --lr 0.001 # ok
# python3 run_experiment.py --config ./exp_configs/cub200/config_random.py --select_every 10 --fraction 0.2 --lr 0.001 # ok
# python3 run_experiment.py --config ./exp_configs/cub200/config_full.py --lr 0.001 # ok
## pretrained finetune
# python3 run_experiment.py --config ./exp_configs/cub200/config_gradmatch.py --select_every 10 --fraction 0.2 --lr 0.005 --pretrained --finetune # ok
# python3 run_experiment.py --config ./exp_configs/cub200/config_gradmatchpb.py --select_every 10 --fraction 0.2 --lr 0.005 --pretrained --finetune # ok
# python3 run_experiment.py --config ./exp_configs/cub200/config_glister.py --select_every 10 --fraction 0.2 --lr 0.005 --pretrained --finetune # ok
# python3 run_experiment.py --config ./exp_configs/cub200/config_craig.py --select_every 10 --fraction 0.2 --lr 0.005 --pretrained --finetune # ok
# python3 run_experiment.py --config ./exp_configs/cub200/config_craigpb.py --select_every 10 --fraction 0.2 --lr 0.005 --pretrained --finetune # ok
# python3 run_experiment.py --config ./exp_configs/cub200/config_random.py --select_every 10 --fraction 0.2 --lr 0.005 --pretrained --finetune # ok
# python3 run_experiment.py --config ./exp_configs/cub200/config_full.py --lr 0.005 --pretrained --finetune # ok
## warmup 
# python3 run_experiment.py --config ./exp_configs/cub200/config_gradmatch-warm.py --kappa 0.05 --select_every 10 --fraction 0.2 --lr 0.001 # ok
# python3 run_experiment.py --config ./exp_configs/cub200/config_gradmatch-warm.py --kappa 0.05 --select_every 10 --fraction 0.2 --lr 0.01 # ok
# python3 run_experiment.py --config ./exp_configs/cub200/config_gradmatch-warm.py --kappa 0.05 --select_every 10 --fraction 0.2 --lr 0.005 --pretrained --finetune # ok
## non fine tune
# python3 run_experiment.py --config ./exp_configs/cub200/config_gradmatch.py --select_every 10 --fraction 0.2 --pretrained --lr 0.001 # ok
# python3 run_experiment.py --config ./exp_configs/cub200/config_gradmatch.py --select_every 10 --fraction 0.2 --pretrained --lr 0.005 # ok
# python3 run_experiment.py --config ./exp_configs/cub200/config_gradmatch.py --select_every 10 --fraction 0.2 --pretrained --lr 0.0005 # ok


#papilion 
## pretrained finetune 
# python3 run_experiment.py --config ./exp_configs/papilion/config_gradmatch.py --select_every 10 --fraction 0.2 --lr 0.005 --pretrained --finetune # ok
# python3 run_experiment.py --config ./exp_configs/papilion/config_gradmatchpb.py --select_every 10 --fraction 0.2 --lr 0.005 --pretrained --finetune # ok
# python3 run_experiment.py --config ./exp_configs/papilion/config_glister.py --select_every 10 --fraction 0.2 --lr 0.005 --pretrained --finetune # ok
# python3 run_experiment.py --config ./exp_configs/papilion/config_craig.py --select_every 10 --fraction 0.2 --lr 0.005 --pretrained --finetune # ok
# python3 run_experiment.py --config ./exp_configs/papilion/config_craigpb.py --select_every 10 --fraction 0.2 --lr 0.005 --pretrained --finetune # ok
# python3 run_experiment.py --config ./exp_configs/papilion/config_random.py --select_every 10 --fraction 0.2 --lr 0.005 --pretrained --finetune # ok
# python3 run_experiment.py --config ./exp_configs/papilion/config_full.py --lr 0.005 --pretrained --finetune # ok
## warmup 
# python3 run_experiment.py --config ./exp_configs/papilion/config_gradmatch-warm.py --kappa 0.05 --select_every 10 --fraction 0.2 --lr 0.001 # ok
# python3 run_experiment.py --config ./exp_configs/papilion/config_gradmatch-warm.py --kappa 0.05 --select_every 10 --fraction 0.2 --lr 0.01 # ok
# python3 run_experiment.py --config ./exp_configs/papilion/config_gradmatch-warm.py --kappa 0.05 --select_every 10 --fraction 0.2 --lr 0.005 --pretrained --finetune # ok

#inaturalist: paper from iNaturalist gives lr of 0.0045 for inception,resnet,mobilenet pretrained on imagenet
#- all methods, pretrained finetuning  (2 different learning rates.) Or exp with 3 learning rates
#- Gradmatch and Craig, 20% every 10, 30% every 10, 10% every 5
#- warm up v non warm up -> gradmatch
# python3 run_experiment.py --config ./exp_configs/inaturalist/config_gradmatch.py --select_every 10 --fraction 0.2 --lr 0.0045 --pretrained --finetune # ok
# python3 run_experiment.py --config ./exp_configs/inaturalist/config_gradmatchpb.py --select_every 10 --fraction 0.2 --lr 0.0045 --pretrained --finetune # ok
# python3 run_experiment.py --config ./exp_configs/inaturalist/config_glister.py --select_every 10 --fraction 0.2 --lr 0.0045 --pretrained --finetune # ok
# python3 run_experiment.py --config ./exp_configs/inaturalist/config_craig.py --select_every 10 --fraction 0.2 --lr 0.0045 --pretrained --finetune # ok
# python3 run_experiment.py --config ./exp_configs/inaturalist/config_craigpb.py --select_every 10 --fraction 0.2 --lr 0.0045 --pretrained --finetune # ok
# python3 run_experiment.py --config ./exp_configs/inaturalist/config_random.py --select_every 10 --fraction 0.2 --lr 0.0045 --pretrained --finetune # ok
# python3 run_experiment.py --config ./exp_configs/inaturalist/config_full.py --lr 0.0045 --pretrained --finetune # ok
## different fractions
# python3 run_experiment.py --config ./exp_configs/inaturalist/config_gradmatch.py --select_every 10 --fraction 0.3 --lr 0.0045 --pretrained --finetune # ok
# python3 run_experiment.py --config ./exp_configs/inaturalist/config_craig.py --select_every 10 --fraction 0.3 --lr 0.0045 --pretrained --finetune # ok
# python3 run_experiment.py --config ./exp_configs/inaturalist/config_gradmatch.py --select_every 5 --fraction 0.1 --lr 0.0045 --pretrained --finetune # ok
# python3 run_experiment.py --config ./exp_configs/inaturalist/config_craig.py --select_every 5 --fraction 0.1 --lr 0.0045 --pretrained --finetune # ok
## warm up
# python3 run_experiment.py --config ./exp_configs/inaturalist/config_gradmatch-warm.py --select_every 10 --fraction 0.2 --lr 0.0045 --pretrained --finetune # ok
# python3 run_experiment.py --config ./exp_configs/inaturalist/config_craig-warm.py --select_every 10 --fraction 0.2 --lr 0.0045 --pretrained --finetune # ok


### 23-8
# Experiments with larger subset size subset sizes.
# efficientNet, cub, FT, PT,
# # 30% every 10. 30% every 5, 20% every 5, 40% every 10
# python3 run_experiment.py --config ./exp_configs/cub200/config_gradmatchpb.py --select_every 10 --fraction 0.3 --lr 0.005 --pretrained --finetune
# python3 run_experiment.py --config ./exp_configs/cub200/config_gradmatchpb.py --select_every 5 --fraction 0.3 --lr 0.005 --pretrained --finetune
# python3 run_experiment.py --config ./exp_configs/cub200/config_gradmatchpb.py --select_every 5 --fraction 0.2 --lr 0.005 --pretrained --finetune
# python3 run_experiment.py --config ./exp_configs/cub200/config_gradmatchpb.py --select_every 10 --fraction 0.4 --lr 0.005 --pretrained --finetune
#
# # Exps with more epochs, cub, gradmatch, PT, FT, change scheduler
# python3 run_experiment.py --config ./exp_configs/cub200/config_gradmatchpb_long.py --select_every 10 --fraction 0.2 --lr 0.005 --pretrained --finetune
# python3 run_experiment.py --config ./exp_configs/cub200/config_gradmatchpb_long.py --select_every 10 --fraction 0.4 --lr 0.005 --pretrained --finetune
#
# # experiments with Facility-location, graph-cut
# #cifar10 ResNet. no PT. no FT
# python3 run_experiment.py --config ./exp_configs/cifar10/config_submodular.py --select_every 10 --fraction 0.2 --submod_function facility-location
# python3 run_experiment.py --config ./exp_configs/cifar10/config_submodular.py --select_every 10 --fraction 0.2 --submod_function graph-cut
# python3 run_experiment.py --config ./exp_configs/cifar10/config_submodular.py --select_every 10 --fraction 0.2 --submod_function sum-redundancy
# python3 run_experiment.py --config ./exp_configs/cifar10/config_submodular.py --select_every 10 --fraction 0.2 --submod_function saturated-coverage
# #cubs efficientNet PT, FT
# python3 run_experiment.py --config ./exp_configs/cub200/config_submodular.py --select_every 10 --fraction 0.2 --submod_function facility-location --pretrained --finetune
# python3 run_experiment.py --config ./exp_configs/cub200/config_submodular.py --select_every 10 --fraction 0.2 --submod_function graph-cut --pretrained --finetune
# python3 run_experiment.py --config ./exp_configs/cub200/config_submodular.py --select_every 10 --fraction 0.2 --submod_function sum-redundancy --pretrained --finetune
# python3 run_experiment.py --config ./exp_configs/cub200/config_submodular.py --select_every 10 --fraction 0.2 --submod_function saturated-coverage --pretrained --finetune
# #pap
# python3 run_experiment.py --config ./exp_configs/cub200/config_submodular.py --select_every 10 --fraction 0.2 --submod_function facility-location --pretrained --finetune
# python3 run_experiment.py --config ./exp_configs/cub200/config_submodular.py --select_every 10 --fraction 0.2 --submod_function graph-cut --pretrained --finetune
# python3 run_experiment.py --config ./exp_configs/cub200/config_submodular.py --select_every 10 --fraction 0.2 --submod_function sum-redundancy --pretrained --finetune
# python3 run_experiment.py --config ./exp_configs/cub200/config_submodular.py --select_every 10 --fraction 0.2 --submod_function saturated-coverage --pretrained --finetune
#
# # experiments with selection every 2, 4, 6, epochs. One dataset, one method.
# python3 run_experiment.py --config ./exp_configs/cifar10/config_gradmatchpb.py --select_every 2 --fraction 0.2
# python3 run_experiment.py --config ./exp_configs/cifar10/config_gradmatchpb.py --select_every 4 --fraction 0.2
# python3 run_experiment.py --config ./exp_configs/cifar10/config_gradmatchpb.py --select_every 6 --fraction 0.2
#
# # INaturalist experiments with 50% of the original dataset size.
# python3 run_experiment.py --config ./exp_configs/inaturalist/config_gradmatchpb.py --select_every 10 --fraction 0.2 --lr 0.0045 --pretrained --finetune
#
# # Gradmatch lam=1, PT,FT, cub
# python3 run_experiment.py --config ./exp_configs/cub200/config_gradmatchpb.py --select_every 10 --fraction 0.2 --lr 0.005 --pretrained --finetune --lam 1


## INaturalist test with 50% data
# python3 run_experiment.py --config ./exp_configs/inaturalist/config_gradmatchpb.py --select_every 10 --fraction 0.2 --lr 0.0045 --pretrained --finetune # ok


## submodular methods
# cubs, non-PT (lookup lr), lr 0.01 and 0.1
# python3 run_experiment.py --config ./exp_configs/cub200/config_submodular.py --lr 0.01 --select_every 10 --fraction 0.2 --submod_function facility-location # ok
# python3 run_experiment.py --config ./exp_configs/cub200/config_submodular.py --lr 0.01 --select_every 10 --fraction 0.2 --submod_function sum-redundancy # ok
# python3 run_experiment.py --config ./exp_configs/cub200/config_submodular.py --lr 0.01 --select_every 10 --fraction 0.2 --submod_function graph-cut # ok
# python3 run_experiment.py --config ./exp_configs/cub200/config_submodular.py --lr 0.01 --select_every 10 --fraction 0.2 --submod_function saturated-coverage # ok

# python3 run_experiment.py --config ./exp_configs/cub200/config_submodular.py --lr 0.1 --select_every 10 --fraction 0.2 --submod_function facility-location # ok
# python3 run_experiment.py --config ./exp_configs/cub200/config_submodular.py --lr 0.1 --select_every 10 --fraction 0.2 --submod_function sum-redundancy # ok
# python3 run_experiment.py --config ./exp_configs/cub200/config_submodular.py --lr 0.1 --select_every 10 --fraction 0.2 --submod_function graph-cut # ok
# python3 run_experiment.py --config ./exp_configs/cub200/config_submodular.py --lr 0.1 --select_every 10 --fraction 0.2 --submod_function saturated-coverage # ok

# papilion, look up fraction and rates
# python3 run_experiment.py --config ./exp_configs/papilion/config_submodular.py --select_every 10 --fraction 0.2 --submod_function facility-location --pretrained --finetune # ok
# python3 run_experiment.py --config ./exp_configs/papilion/config_submodular.py --select_every 10 --fraction 0.2 --submod_function sum-redundancy --pretrained --finetune # ok
# python3 run_experiment.py --config ./exp_configs/papilion/config_submodular.py --select_every 10 --fraction 0.2 --submod_function graph-cut --pretrained --finetune # ok
# python3 run_experiment.py --config ./exp_configs/papilion/config_submodular.py --select_every 10 --fraction 0.2 --submod_function saturated-coverage --pretrained --finetune # ok

## non-adaptive
# multiple fractions: 0.3, 0.5, all datasets, most methods. Also random
# use PerClass (balancing) and PerBatch

# python3 run_experiment.py --config ./exp_configs/cifar10/config_random.py --fraction 0.3 --nonadaptive # ok
# python3 run_experiment.py --config ./exp_configs/cifar10/config_gradmatch.py --fraction 0.3 --nonadaptive # ok
# python3 run_experiment.py --config ./exp_configs/cifar10/config_gradmatchpb.py --fraction 0.3 --nonadaptive # ok
# python3 run_experiment.py --config ./exp_configs/cifar10/config_glister.py --fraction 0.3 --nonadaptive # ok
#python3 run_experiment.py --config ./exp_configs/cifar10/config_submodular.py --fraction 0.3 --nonadaptive --submod_function facility-location --selection_type PerClass
# python3 run_experiment.py --config ./exp_configs/cifar10/config_submodular.py --fraction 0.3 --nonadaptive --submod_function facility-location --selection_type PerBatch # ok

# python3 run_experiment.py --config ./exp_configs/cifar10/config_random.py --fraction 0.5 --nonadaptive # ok
# python3 run_experiment.py --config ./exp_configs/cifar10/config_gradmatch.py --fraction 0.5 --nonadaptive # ok
# python3 run_experiment.py --config ./exp_configs/cifar10/config_gradmatchpb.py --fraction 0.5 --nonadaptive # ok
# python3 run_experiment.py --config ./exp_configs/cifar10/config_glister.py --fraction 0.5 --nonadaptive # ok
#python3 run_experiment.py --config ./exp_configs/cifar10/config_submodular.py --fraction 0.5 --nonadaptive --submod_function facility-location --selection_type PerClass
# python3 run_experiment.py --config ./exp_configs/cifar10/config_submodular.py --fraction 0.5 --nonadaptive --submod_function facility-location --selection_type PerBatch # ok

## cubs, 0.4, PT-FT
# python3 run_experiment.py --config ./exp_configs/cub200/config_random.py --fraction 0.4 --nonadaptive --pretrained --finetune # ok
# python3 run_experiment.py --config ./exp_configs/cub200/config_gradmatch.py --fraction 0.4 --nonadaptive --pretrained --finetune # ok
# python3 run_experiment.py --config ./exp_configs/cub200/config_gradmatchpb.py --fraction 0.4 --nonadaptive --pretrained --finetune # ok
# python3 run_experiment.py --config ./exp_configs/cub200/config_glister.py --fraction 0.4 --nonadaptive --pretrained --finetune # ok
#python3 run_experiment.py --config ./exp_configs/cub200/config_submodular.py --fraction 0.4 --nonadaptive --submod_function facility-location --selection_type PerClass --pretrained --finetune
# python3 run_experiment.py --config ./exp_configs/cub200/config_submodular.py --fraction 0.4 --nonadaptive --submod_function facility-location --selection_type PerBatch --pretrained --finetune # ok

## non-pretrained with warm up with larger subset sizes. 
# python3 run_experiment.py --config ./exp_configs/cifar10/config_gradmatchpb.py --fraction 0.2 --select_every 20 # ok
# python3 run_experiment.py --config ./exp_configs/cifar10/config_gradmatchpb.py --fraction 0.2 --select_every 30 # ok
# python3 run_experiment.py --config ./exp_configs/cifar10/config_gradmatchpb.py --fraction 0.3 --select_every 20 # ok
# python3 run_experiment.py --config ./exp_configs/cifar10/config_gradmatchpb.py --fraction 0.3 --select_every 30 # ok
# python3 run_experiment.py --config ./exp_configs/cifar10/config_gradmatchpb.py --fraction 0.4 --select_every 20 # ok
# python3 run_experiment.py --config ./exp_configs/cifar10/config_gradmatchpb.py --fraction 0.4 --select_every 30 # ok

## 31-8 warm up slide 11, full with 0.01 LR as this was not done yet 
# python3 run_experiment.py --config ./exp_configs/cub200/config_full.py --lr 0.01 # ok
# python3 run_experiment.py --config ./exp_configs/cub200/config_gradmatch.py --lr 0.01 # ok
# python3 run_experiment.py --config ./exp_configs/cub200/config_random.py --lr 0.01 --select_every 1 # ok

## 1-9 - TL experiments without a scheduler
##         - Cub200, PT, FT, 20x10,  Random, full, gradmatch
# python3 run_experiment.py --config ./exp_configs/cub200/config_full.py --disable_scheduler --lr 0.005 --pretrained --finetune # ok
# python3 run_experiment.py --config ./exp_configs/cub200/config_gradmatchpb.py --lr 0.005 --disable_scheduler --select_every 10 --fraction 0.2 --pretrained --finetune # ok
# python3 run_experiment.py --config ./exp_configs/cub200/config_random.py --lr 0.005 --disable_scheduler --select_every 1 --fraction 0.2 --pretrained --finetune # ok
# python3 run_experiment.py --config ./exp_configs/cub200/config_random.py --lr 0.005 --disable_scheduler --select_every 10 --fraction 0.2 --pretrained --finetune # ok

## data dependent scheduler
# python3 run_experiment.py --config ./exp_configs/cub200/config_full.py --lr 0.005 --pretrained --finetune # ok
# python3 run_experiment.py --config ./exp_configs/cub200/config_gradmatchpb.py --lr 0.005 --data_dependent_scheduler --select_every 10 --fraction 0.2 --pretrained --finetune # ok
# python3 run_experiment.py --config ./exp_configs/cub200/config_random.py --lr 0.005 --data_dependent_scheduler --select_every 1 --fraction 0.2 --pretrained --finetune # ok
# python3 run_experiment.py --config ./exp_configs/cub200/config_random.py --lr 0.005 --data_dependent_scheduler --select_every 10 --fraction 0.2 --pretrained --finetune # ok
 
## 6-9 not all methods of slide 11 (warm up) ran with lr=0.01 and not all have new metrics.
# GLISTER, CRAIG, Gradmatch-warm k 0.05, graph-cut, facility-location, 
# python3 run_experiment.py --config ./exp_configs/cub200/config_glister.py --lr 0.01 --select_every 10 --fraction 0.2 # ok
# python3 run_experiment.py --config ./exp_configs/cub200/config_random.py --lr 0.01 --select_every 10 --fraction 0.2 # ok
# python3 run_experiment.py --config ./exp_configs/cub200/config_gradmatchpb.py --lr 0.01 --select_every 10 --fraction 0.2 # ok
# python3 run_experiment.py --config ./exp_configs/cub200/config_craig.py --lr 0.01 --select_every 10 --fraction 0.2 # ok
# python3 run_experiment.py --config ./exp_configs/cub200/config_craigpb.py --lr 0.01 --select_every 10 --fraction 0.2 # ok
# python3 run_experiment.py --config ./exp_configs/cub200/config_.py --lr 0.01 --select_every 10 --fraction 0.2 # ok
# python3 run_experiment.py --config ./exp_configs/cub200/config_submodular.py --lr 0.01 --fraction 0.2 --select_every 10 --submod_function facility-location --selection_type PerBatch # ok
# python3 run_experiment.py --config ./exp_configs/cub200/config_submodular.py --lr 0.01 --fraction 0.2 --select_every 10 --submod_function facility-location --selection_type PerClass # ok
# python3 run_experiment.py --config ./exp_configs/cub200/config_submodular.py --lr 0.01 --fraction 0.2 --select_every 10 --submod_function graph-cut --selection_type PerBatch # ok
# python3 run_experiment.py --config ./exp_configs/cub200/config_submodular.py --lr 0.01 --fraction 0.2 --select_every 10 --submod_function graph-cut --selection_type PerClass # ok

# try to outperform Full on Cub200 non-PT with GradmatchPB at multiple fractions and also do warm-up once with rate 20x10, try Gradmatch-pb with higher LR. 
# python3 run_experiment.py --config ./exp_configs/cub200/config_gradmatchpb.py --lr 0.01 --select_every 10 --fraction 0.2 --kappa 0.05 # ok
# python3 run_experiment.py --config ./exp_configs/cub200/config_gradmatchpb.py --lr 0.01 --select_every 10 --fraction 0.3 --kappa 0.05 # ok
# python3 run_experiment.py --config ./exp_configs/cub200/config_gradmatchpb.py --lr 0.01 --select_every 10 --fraction 0.2 --kappa 0.05 --data_dependent_scheduler # ok
# python3 run_experiment.py --config ./exp_configs/cub200/config_gradmatchpb.py --lr 0.01 --select_every 10 --fraction 0.3 --kappa 0.05 --data_dependent_scheduler # ok

## do LR search on Pap with Full training. (look at wandb)
# python3 run_experiment.py --config ./exp_configs/papilion/config_gradmatchpb.py --lr 0.01 --select_every 10 --fraction 0.2 # ok
# python3 run_experiment.py --config ./exp_configs/papilion/config_gradmatchpb.py --lr 0.05 --select_every 10 --fraction 0.2 # ok
# python3 run_experiment.py --config ./exp_configs/papilion/config_gradmatchpb.py --lr 0.01 --select_every 10 --fraction 0.2 # ok
# python3 run_experiment.py --config ./exp_configs/papilion/config_gradmatchpb.py --lr 0.005 --select_every 10 --fraction 0.2 # ok
# python3 run_experiment.py --config ./exp_configs/papilion/config_gradmatchpb.py --lr 0.001 --select_every 10 --fraction 0.2 # ok

## EXP settings based on GradMatch paper
# GradmatchPB-Warm, kappa=0.5, 350 epochs, TMax = 350, 30% every 20
# python3 run_experiment.py --config ./exp_configs/cub200/config_gradmatchpb_long.py --lr 0.01 --epochs 350 --kappa 0.5 --fraction 0.3 --select_every 20 # ok
