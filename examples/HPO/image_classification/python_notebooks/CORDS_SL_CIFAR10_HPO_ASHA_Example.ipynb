{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "CORDS_SL_CIFAR10_HPO_ASHA_Example.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Subset selection for Hyperparameter tuning\n",
        "\n",
        "In this tutorial, we will look at an example showing how to integrate GradMatchPB-Warm subset selection strategy in typical hyperparameter tuning loop\n",
        "for configuration model training on CIFAR100 dataset with TPE as hyper-parameter search algorithm and ASHA as hyper-parameter scheduler."
      ],
      "metadata": {
        "id": "kFDegt9Xh5XS"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nRBrJb8I_vUv"
      },
      "source": [
        "### Cloning CORDS repository"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x35Mfc-RnKkX",
        "outputId": "3b84a4ba-697e-4fc4-f51f-3c28cb229b30"
      },
      "source": [
        "!git clone https://github.com/decile-team/cords.git\n",
        "%cd cords/\n",
        "%ls"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'cords'...\n",
            "remote: Enumerating objects: 3996, done.\u001b[K\n",
            "remote: Counting objects: 100% (2618/2618), done.\u001b[K\n",
            "remote: Compressing objects: 100% (1189/1189), done.\u001b[K\n",
            "remote: Total 3996 (delta 1706), reused 2243 (delta 1390), pack-reused 1378\u001b[K\n",
            "Receiving objects: 100% (3996/3996), 54.64 MiB | 17.93 MiB/s, done.\n",
            "Resolving deltas: 100% (2443/2443), done.\n",
            "/content/cords\n",
            "\u001b[0m\u001b[01;34mbenchmarks\u001b[0m/  \u001b[01;34mdocs\u001b[0m/        README.md      \u001b[01;34mtests\u001b[0m/        train_ssl.py\n",
            "\u001b[01;34mconfigs\u001b[0m/     \u001b[01;34mexamples\u001b[0m/    \u001b[01;34mrequirements\u001b[0m/  train_hpo.py\n",
            "\u001b[01;34mcords\u001b[0m/       LICENSE.txt  setup.py       train_sl.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gAA3K0cVnyd9"
      },
      "source": [
        "### Install prerequisite libraries of CORDS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6CXZ4L1ynmcp",
        "outputId": "3c88304d-0764-42f4-ee2a-5a6e4d5c4de5"
      },
      "source": [
        "!pip install dotmap\n",
        "!pip install apricot-select\n",
        "!pip install ray[default]\n",
        "!pip install ray[tune]\n",
        "!pip install datasets"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting dotmap\n",
            "  Downloading dotmap-1.3.26-py3-none-any.whl (11 kB)\n",
            "Installing collected packages: dotmap\n",
            "Successfully installed dotmap-1.3.26\n",
            "Collecting apricot-select\n",
            "  Downloading apricot-select-0.6.1.tar.gz (28 kB)\n",
            "Requirement already satisfied: numpy>=1.14.2 in /usr/local/lib/python3.7/dist-packages (from apricot-select) (1.21.5)\n",
            "Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from apricot-select) (1.4.1)\n",
            "Requirement already satisfied: numba>=0.43.0 in /usr/local/lib/python3.7/dist-packages (from apricot-select) (0.51.2)\n",
            "Requirement already satisfied: tqdm>=4.24.0 in /usr/local/lib/python3.7/dist-packages (from apricot-select) (4.62.3)\n",
            "Collecting nose\n",
            "  Downloading nose-1.3.7-py3-none-any.whl (154 kB)\n",
            "\u001b[K     |████████████████████████████████| 154 kB 7.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from numba>=0.43.0->apricot-select) (57.4.0)\n",
            "Requirement already satisfied: llvmlite<0.35,>=0.34.0.dev0 in /usr/local/lib/python3.7/dist-packages (from numba>=0.43.0->apricot-select) (0.34.0)\n",
            "Building wheels for collected packages: apricot-select\n",
            "  Building wheel for apricot-select (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for apricot-select: filename=apricot_select-0.6.1-py3-none-any.whl size=48786 sha256=1b4eb91dc07c416e6b5c124c69520332e5671ceec59601fa554b165ee465c127\n",
            "  Stored in directory: /root/.cache/pip/wheels/1d/b0/5d/41bab30f23d17864700963dad70bbeda159a409e94f0778f2f\n",
            "Successfully built apricot-select\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gzyCsbnJn3_L"
      },
      "source": [
        "### Import necessary libraries"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "from cords.utils.config_utils import load_config_data\n",
        "from ray.tune.suggest.hyperopt import HyperOptSearch\n",
        "from ray.tune.suggest.bayesopt import BayesOptSearch\n",
        "from ray.tune.suggest.skopt import SkOptSearch\n",
        "from ray.tune.suggest.dragonfly import DragonflySearch\n",
        "from ray.tune.suggest.ax import AxSearch\n",
        "from ray.tune.suggest.bohb import TuneBOHB\n",
        "from ray.tune.suggest.nevergrad import NevergradSearch\n",
        "from ray.tune.suggest.optuna import OptunaSearch\n",
        "from ray.tune.suggest.zoopt import ZOOptSearch\n",
        "from ray.tune.suggest.sigopt import SigOptSearch\n",
        "from ray.tune.suggest.hebo import HEBOSearch\n",
        "from ray.tune.schedulers import AsyncHyperBandScheduler\n",
        "from ray.tune.schedulers import HyperBandScheduler\n",
        "from ray.tune.schedulers.hb_bohb import HyperBandForBOHB\n",
        "from ray import tune"
      ],
      "metadata": {
        "id": "eTCjdwOJNKAV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xdgARYmvNxwM"
      },
      "source": [
        "### Loading hyperparameter configuration file with predefined arguments:\n",
        "\n",
        "We have a set of predefined configuration files added to CORDS for HPO under cords/configs/HPO/ which can be used directly by loading them as a dotmap object. \n",
        "\n",
        "An example of predefined configuration for Hyper-parameter tuning on CIFAR100 dataset with ASHA as scheduler and TPE as search algorithm can be found below:\n",
        "\n",
        "```Python\n",
        "from ray import tune\n",
        "\n",
        "config = dict(setting= \"hyperparamtuning\",\n",
        "\n",
        "# parameter for subset selection\n",
        "# all settings for subset selection will be fetched from here\n",
        "subset_config = \"configs/SL/config_gradmatchpb-warm_cifar100.py\",\n",
        "\n",
        "# parameters for hyper-parameter tuning\n",
        "# search space for hyper-parameter tuning\n",
        "space = dict(\n",
        "        learning_rate=tune.uniform(0.001, 0.01), \n",
        "        learning_rate1=tune.uniform(0.001, 0.01),\n",
        "        learning_rate2=tune.uniform(0.001, 0.01),\n",
        "        learning_rate3=tune.uniform(0.001, 0.01),\n",
        "        scheduler= tune.choice(['cosine_annealing', 'linear_decay']),\n",
        "        nesterov= tune.choice([True, False]),\n",
        "        gamma= tune.uniform(0.05, 0.5),    \n",
        "        ),\n",
        "\n",
        "# tuning algorithm \n",
        "search_algo = \"TPE\",\n",
        "\n",
        "# number of hyper-parameter set to try\n",
        "num_evals = 27,\n",
        "\n",
        "# metric to be optimized, for 'mean_loss' metric mode should be 'min'\n",
        "metric = \"mean_accuracy\",\n",
        "mode = \"max\",\n",
        "\n",
        "# scheduler to be used (i.e ASHAScheduler)\n",
        "# scheduler terminates trials that perform poorly\n",
        "# learn more here: https://docs.ray.io/en/releases-0.7.1/tune-schedulers.html\n",
        "scheduler = 'hyperband',\n",
        "\n",
        "# where to store logs\n",
        "log_dir = \"RayLogs/\",\n",
        "\n",
        "# resume hyper-parameter tuning from previous log\n",
        "# specify 'name' (i.e main_2021-03-09_18-33-56) below\n",
        "resume = False,\n",
        "\n",
        "# only required if you want to resume from previous checkpoint\n",
        "# it can also be specified if you don't want to resume\n",
        "name = None,\n",
        "\n",
        "# specify resources to be used per trial\n",
        "# i.e {'gpu':1, 'cpu':2}\n",
        "resources = {'gpu':1},\n",
        "\n",
        "# if True, trains model on Full dataset with the best parameter selected.\n",
        "final_train = True\n",
        ")\n",
        "```\n",
        "\n",
        "Please find a detailed documentation explaining the available configuration parameters in the following readthedocs [page]()\n",
        "\n",
        "***Loading the predefined configuration file directly using the load_config_data function in CORDS***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vozeGsg3CenF"
      },
      "outputs": [],
      "source": [
        "from cords.utils.config_utils import load_config_data\n",
        "param_tuning_cfg = load_config_data('/content/cords/configs/HPO/config_hyper-param_tuning_cifar100.py')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Modifying Default SL training loop to include search space parameters as configurable parameters\n",
        "In this example, the search space parameters are layer-wise learning rates, step-size decay, learning rate scheduler, and nesterov momentum boolean indicator. We modify the default pytorch training loop given in train_sl.py to include the hyper-parameter search space parameters in the following manner:\n",
        "\n",
        "```python\n",
        "def optimizer_with_scheduler(self, model):\n",
        "        if self.cfg.optimizer.type == 'sgd':\n",
        "            optimizer = optim.SGD( [\n",
        "                                    {\"params\": model.linear.parameters(), \"lr\": self.cfg.optimizer.lr1},\n",
        "                                    {\"params\": model.layer4.parameters(), \"lr\": self.cfg.optimizer.lr2},\n",
        "                                    {\"params\": model.layer3.parameters(), \"lr\": self.cfg.optimizer.lr2},\n",
        "                                    {\"params\": model.layer2.parameters(), \"lr\": self.cfg.optimizer.lr2},\n",
        "                                    {\"params\": model.layer1.parameters(), \"lr\": self.cfg.optimizer.lr2},\n",
        "                                    {\"params\": model.conv1.parameters(), \"lr\": self.cfg.optimizer.lr3},\n",
        "                                    ],\n",
        "                                    lr=self.cfg.optimizer.lr,\n",
        "                                  momentum=self.cfg.optimizer.momentum,\n",
        "                                  weight_decay=self.cfg.optimizer.weight_decay,\n",
        "                                  nesterov=self.cfg.optimizer.nesterov)\n",
        "        elif self.cfg.optimizer.type == \"adam\":\n",
        "            optimizer = optim.Adam(model.parameters(), lr=self.cfg.optimizer.lr)\n",
        "        elif self.cfg.optimizer.type == \"rmsprop\":\n",
        "            optimizer = optim.RMSprop(model.parameters(), lr=self.cfg.optimizer.lr)\n",
        "\n",
        "        if self.cfg.scheduler.type == 'cosine_annealing':\n",
        "            scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer,\n",
        "                                                                   T_max=self.cfg.scheduler.T_max)\n",
        "        elif self.cfg.scheduler.type == 'linear_decay':\n",
        "            scheduler = torch.optim.lr_scheduler.StepLR(optimizer, \n",
        "                                                        step_size=self.cfg.scheduler.stepsize, \n",
        "                                                        gamma=self.cfg.scheduler.gamma)\n",
        "        else:\n",
        "            scheduler = None\n",
        "        return optimizer, scheduler\n",
        "```\n",
        "\n",
        "We include the modified default train_sl loop in examples\\HPO\\image_classification\\python_code\\vision_train_sl.py file."
      ],
      "metadata": {
        "id": "-OQLGEIkc0Wm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.append('/content/cords/examples/HPO/image_classification/python_code/vision_train_sl')\n",
        "from vision_train_sl import TrainClassifier"
      ],
      "metadata": {
        "id": "VKrh9ScbdkzO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Instantiating GradMatchPB-Warm arguments required for train_sl.py by loading the corresponding configuration file."
      ],
      "metadata": {
        "id": "htuwbUF1n_vE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_cfg_file = '/content/cords/configs/SL/config_gradmatchpb-warm_cifar100.py'\n",
        "train_cfg = load_config_data(train_cfg_file)\n",
        "\n",
        "\"\"\"\n",
        "Note that we have to do following changes to standard training configuration files\n",
        "to get them working for Hyper-parameter tuning.\n",
        "\"\"\"\n",
        "train_cfg.report_tune = True\n",
        "train_cfg.train_args.print_every = 1\n",
        "\n",
        "#Instantiating the train classifier class with the loaded train_cfg\n",
        "train_class = TrainClassifier(train_cfg)"
      ],
      "metadata": {
        "id": "mjHq6D-7p-nq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Get Hyper-parameter search algorithm \n",
        "\n",
        "In this example, we will be using Tree-structured parzen estimator(TPE) as the hyper-parameter search algorithm. In the hyper-parameter tuning configuration file, the search algorithm option is given as cfg.search_algo."
      ],
      "metadata": {
        "id": "UHH8xGywg6x5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "method = param_tuning_cfg.search_algo\n",
        "#Search space\n",
        "space = param_tuning_cfg.space\n",
        "#Evaluation metric for configuration evaluation\n",
        "metric = param_tuning_cfg.metric\n",
        "#maximum or minimum mode\n",
        "mode = param_tuning_cfg.mode\n",
        "\n",
        "\"\"\"\n",
        "Shows all hyper-parameter search algorithm that work with CORDS. We use ray-tune library for hyper-parameter tuning\n",
        "in CORDS. Hence, all search algorithms given in raytune can be used with CORDS as well.\n",
        "\"\"\"\n",
        "# HyperOptSearch \n",
        "if method == \"hyperopt\" or method == \"TPE\":\n",
        "    search = HyperOptSearch(space, metric, mode)\n",
        "# BayesOptSearch\n",
        "elif method == \"bayesopt\" or method == \"BO\":\n",
        "    search = BayesOptSearch(space, metric = metric, mode = mode)\n",
        "# SkoptSearch\n",
        "elif method == \"skopt\" or method == \"SKBO\":\n",
        "    search = SkOptSearch(space, metric = metric, mode = mode)\n",
        "# DragonflySearch\n",
        "elif method == \"dragonfly\" or method == \"SBO\":\n",
        "    search = DragonflySearch(space, metric = metric, mode = mode)\n",
        "# AxSearch\n",
        "elif method == \"ax\" or method == \"BBO\":\n",
        "    search = AxSearch(space, metric = metric, mode = mode)\n",
        "# TuneBOHB\n",
        "elif method == \"tunebohb\" or method == \"BOHB\":\n",
        "    search = TuneBOHB(space, metric = metric, mode = mode)\n",
        "# NevergradSearch\n",
        "elif method == \"nevergrad\" or method == \"GFO\":\n",
        "    search = NevergradSearch(space, metric = metric, mode = mode)\n",
        "# OptunaSearch\n",
        "elif method == \"optuna\" or method == \"OSA\":\n",
        "    search = OptunaSearch(space, metric = metric, mode = mode)\n",
        "# ZOOptSearch\n",
        "elif method == \"zoopt\" or method == \"ZOO\":\n",
        "    search = ZOOptSearch(space, metric = metric, mode = mode)\n",
        "# SigOptSearch\n",
        "elif method == \"sigopt\":\n",
        "    search = SigOptSearch(space, metric = metric, mode = mode)\n",
        "# HEBOSearch\n",
        "elif method == \"hebo\" or method == \"HEBO\":\n",
        "    search = HEBOSearch(space, metric = metric, mode = mode)\n",
        "else:\n",
        "    search = None\n"
      ],
      "metadata": {
        "id": "SoKFg3_ChrMt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Get Hyper-parameter scheduler\n",
        "\n",
        "In this example, we will be using ASHA as the hyper-parameter scheduler algorithm. In the hyper-parameter tuning configuration file, the scheduler option is given as cfg.scheduler."
      ],
      "metadata": {
        "id": "QC2pBy4RngM_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "method = param_tuning_cfg.scheduler\n",
        "\n",
        "if method == \"ASHA\" or method == \"asha\":\n",
        "    scheduler = AsyncHyperBandScheduler(metric = metric, mode = mode, \n",
        "                                        max_t = train_cfg.train_args.num_epochs)\n",
        "elif method == \"hyperband\" or method == \"HB\":\n",
        "    scheduler = HyperBandScheduler(metric = metric, mode = mode, \n",
        "                max_t = train_cfg.train_args.num_epochs)\n",
        "elif method == \"BOHB\":\n",
        "    scheduler = HyperBandForBOHB(metric = metric, mode = mode)\n",
        "else:\n",
        "    scheduler = None\n",
        "\n",
        "scheduler = scheduler"
      ],
      "metadata": {
        "id": "GxBRqdlWnfaT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Utility function that updates the training configuration parameters with new parameters suggested by search algorithm"
      ],
      "metadata": {
        "id": "iiWcnKDns1MC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def update_parameters(config, new_config):\n",
        "    # a generic function to update parameters\n",
        "    if 'learning_rate' in new_config:\n",
        "        config.optimizer.lr = new_config['learning_rate']\n",
        "    if 'learning_rate1' in new_config:\n",
        "        config.optimizer.lr1 = new_config['learning_rate1']\n",
        "    if 'learning_rate2' in new_config:\n",
        "        config.optimizer.lr2 = new_config['learning_rate2']\n",
        "    if 'learning_rate3' in new_config:\n",
        "        config.optimizer.lr3 = new_config['learning_rate3']\n",
        "    if 'optimizer' in new_config:\n",
        "        config.optimizer.type = new_config['optimizer']\n",
        "    if 'nesterov' in new_config:\n",
        "        config.optimizer.nesterov = new_config['nesterov']\n",
        "    if 'scheduler' in new_config:\n",
        "        config.scheduler.type = new_config['scheduler']\n",
        "    if 'gamma' in new_config:\n",
        "        config.scheduler.gamma = new_config['gamma']\n",
        "    if 'epochs' in new_config:\n",
        "        config.train_args.num_epochs = new_config['epochs']\n",
        "    if 'trn_batch_size' in new_config:\n",
        "        config.dataloader.batch_size = new_config['trn_batch_size']\n",
        "    if 'hidden_size' in new_config:\n",
        "        config.model.hidden_size = new_config['hidden_size']\n",
        "    if 'num_layers' in new_config:\n",
        "        config.model.num_layers = new_config['num_layers']\n",
        "    return config\n"
      ],
      "metadata": {
        "id": "8f6fJf3Dsyqu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Utility function that takes in the search configuration parameters suggested by hyper-parameter search algorithm, update the training configuration file accordingly, and train the model with the new configuration. "
      ],
      "metadata": {
        "id": "pHjupGiitRF_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def param_tune(config):\n",
        "    #update parameters in config dict\n",
        "    new_config = update_parameters(train_cfg, config)\n",
        "    train_cfg = new_config\n",
        "    # turn on reporting to ray every time\n",
        "    train_cfg.report_tune = True\n",
        "    train_class.train()\n"
      ],
      "metadata": {
        "id": "tcyfqFKrtQhP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Start Hyper-parameter tuning"
      ],
      "metadata": {
        "id": "mGf7aLfguRk1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "analysis = tune.run(\n",
        "          param_tune,\n",
        "          num_samples=param_tuning_cfg.num_evals,\n",
        "          search_alg=search,\n",
        "          scheduler=scheduler,\n",
        "          resources_per_trial=param_tuning_cfg.cfg.resources,\n",
        "          local_dir=param_tuning_cfg.log_dir+train_cfg.dss_args.type+'/',\n",
        "          log_to_file=True,\n",
        "          name=param_tuning_cfg.name,\n",
        "          resume=param_tuning_cfg.resume)"
      ],
      "metadata": {
        "id": "Jf8EOTO5uREQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Get best hyper-parameter configuration"
      ],
      "metadata": {
        "id": "pGae8uBLu588"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "best_config = analysis.get_best_config(metric=param_tuning_cfg.metric, mode=param_tuning_cfg.mode)\n",
        "print(\"Best Config: \", best_config)"
      ],
      "metadata": {
        "id": "kLFc7plxu9fl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}